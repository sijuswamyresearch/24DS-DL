<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Basics of Artificial Neural Network – 24DS-DL</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./benchmarkDL.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./intro.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Basics of Artificial Neural Network</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">24DS-DL</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Basics of Artificial Neural Network</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./benchmarkDL.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Benchmarking Deep Learning with Libraries</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./microprojectsMLP.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Micro Projects using ANN</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-to-artificial-neural-networks-anns" id="toc-introduction-to-artificial-neural-networks-anns" class="nav-link active" data-scroll-target="#introduction-to-artificial-neural-networks-anns"><span class="header-section-number">1.1</span> Introduction to Artificial Neural Networks (ANNs)</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">1.1.1</span> Introduction</a></li>
  <li><a href="#historical-development-and-evolution" id="toc-historical-development-and-evolution" class="nav-link" data-scroll-target="#historical-development-and-evolution"><span class="header-section-number">1.1.2</span> Historical Development and Evolution</a></li>
  <li><a href="#modern-applications-of-artificial-neural-networks" id="toc-modern-applications-of-artificial-neural-networks" class="nav-link" data-scroll-target="#modern-applications-of-artificial-neural-networks"><span class="header-section-number">1.1.3</span> Modern Applications of Artificial Neural Networks</a></li>
  <li><a href="#challenges-and-future-directions" id="toc-challenges-and-future-directions" class="nav-link" data-scroll-target="#challenges-and-future-directions"><span class="header-section-number">1.1.4</span> Challenges and Future Directions</a></li>
  </ul></li>
  <li><a href="#perceptron" id="toc-perceptron" class="nav-link" data-scroll-target="#perceptron"><span class="header-section-number">1.2</span> Perceptron</a>
  <ul class="collapse">
  <li><a href="#perceptron-model" id="toc-perceptron-model" class="nav-link" data-scroll-target="#perceptron-model"><span class="header-section-number">1.2.1</span> Perceptron Model</a></li>
  <li><a href="#perceptron-learning-rule" id="toc-perceptron-learning-rule" class="nav-link" data-scroll-target="#perceptron-learning-rule"><span class="header-section-number">1.2.2</span> Perceptron Learning Rule</a></li>
  <li><a href="#perceptron-algorithm" id="toc-perceptron-algorithm" class="nav-link" data-scroll-target="#perceptron-algorithm"><span class="header-section-number">1.2.3</span> Perceptron Algorithm</a></li>
  <li><a href="#perceptron-theorem-and-margin" id="toc-perceptron-theorem-and-margin" class="nav-link" data-scroll-target="#perceptron-theorem-and-margin"><span class="header-section-number">1.2.4</span> Perceptron Theorem and Margin</a></li>
  <li><a href="#implications-of-the-theorem" id="toc-implications-of-the-theorem" class="nav-link" data-scroll-target="#implications-of-the-theorem"><span class="header-section-number">1.2.5</span> Implications of the Theorem</a></li>
  <li><a href="#characterizing-data-sets-for-fast-convergence" id="toc-characterizing-data-sets-for-fast-convergence" class="nav-link" data-scroll-target="#characterizing-data-sets-for-fast-convergence"><span class="header-section-number">1.2.6</span> Characterizing Data Sets for Fast Convergence</a></li>
  <li><a href="#example-of-a-dataset-with-large-margin" id="toc-example-of-a-dataset-with-large-margin" class="nav-link" data-scroll-target="#example-of-a-dataset-with-large-margin"><span class="header-section-number">1.2.7</span> Example of a Dataset with Large Margin</a></li>
  <li><a href="#simulating-or-gate-using-perceptron" id="toc-simulating-or-gate-using-perceptron" class="nav-link" data-scroll-target="#simulating-or-gate-using-perceptron"><span class="header-section-number">1.2.8</span> Simulating <code>OR</code> gate using Perceptron</a></li>
  <li><a href="#simulating-and-gate-using-a-perceptron" id="toc-simulating-and-gate-using-a-perceptron" class="nav-link" data-scroll-target="#simulating-and-gate-using-a-perceptron"><span class="header-section-number">1.2.9</span> Simulating <code>AND</code> gate using a perceptron</a></li>
  <li><a href="#simulating-all-logic-gates-using-the-perceptron" id="toc-simulating-all-logic-gates-using-the-perceptron" class="nav-link" data-scroll-target="#simulating-all-logic-gates-using-the-perceptron"><span class="header-section-number">1.2.10</span> Simulating all logic gates using the Perceptron</a></li>
  <li><a href="#introduction-to-the-sigmoid-activation-function" id="toc-introduction-to-the-sigmoid-activation-function" class="nav-link" data-scroll-target="#introduction-to-the-sigmoid-activation-function"><span class="header-section-number">1.2.11</span> Introduction to the Sigmoid Activation Function</a></li>
  <li><a href="#historical-context" id="toc-historical-context" class="nav-link" data-scroll-target="#historical-context"><span class="header-section-number">1.2.12</span> Historical Context</a></li>
  <li><a href="#relevance-to-modern-neural-networks" id="toc-relevance-to-modern-neural-networks" class="nav-link" data-scroll-target="#relevance-to-modern-neural-networks"><span class="header-section-number">1.2.13</span> Relevance to Modern Neural Networks</a></li>
  <li><a href="#applications" id="toc-applications" class="nav-link" data-scroll-target="#applications"><span class="header-section-number">1.2.14</span> Applications</a></li>
  <li><a href="#sigmoid-neuron" id="toc-sigmoid-neuron" class="nav-link" data-scroll-target="#sigmoid-neuron"><span class="header-section-number">1.2.15</span> Sigmoid Neuron</a></li>
  </ul></li>
  <li><a href="#important-theorems-and-results" id="toc-important-theorems-and-results" class="nav-link" data-scroll-target="#important-theorems-and-results"><span class="header-section-number">1.3</span> Important Theorems and Results</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples"><span class="header-section-number">1.4</span> Examples</a>
  <ul class="collapse">
  <li><a href="#simple-example-perceptron-for-and-gate-with-native-python-code" id="toc-simple-example-perceptron-for-and-gate-with-native-python-code" class="nav-link" data-scroll-target="#simple-example-perceptron-for-and-gate-with-native-python-code"><span class="header-section-number">1.4.1</span> Simple Example: Perceptron for AND Gate with native <code>Python</code> code</a></li>
  </ul></li>
  <li><a href="#the-perceptron-and-the-xor-problem" id="toc-the-perceptron-and-the-xor-problem" class="nav-link" data-scroll-target="#the-perceptron-and-the-xor-problem"><span class="header-section-number">1.5</span> The Perceptron and the XOR Problem</a>
  <ul class="collapse">
  <li><a href="#why-the-perceptron-fails-at-xor" id="toc-why-the-perceptron-fails-at-xor" class="nav-link" data-scroll-target="#why-the-perceptron-fails-at-xor"><span class="header-section-number">1.5.1</span> Why the Perceptron Fails at XOR</a></li>
  </ul></li>
  <li><a href="#multi-layer-perceptron-mlp" id="toc-multi-layer-perceptron-mlp" class="nav-link" data-scroll-target="#multi-layer-perceptron-mlp"><span class="header-section-number">1.6</span> Multi-Layer Perceptron (MLP)</a></li>
  <li><a href="#the-need-for-an-appropriate-learning-algorithm-in-multi-layer-networks" id="toc-the-need-for-an-appropriate-learning-algorithm-in-multi-layer-networks" class="nav-link" data-scroll-target="#the-need-for-an-appropriate-learning-algorithm-in-multi-layer-networks"><span class="header-section-number">1.7</span> The Need for an Appropriate Learning Algorithm in Multi-Layer Networks</a>
  <ul class="collapse">
  <li><a href="#challenge-of-multi-layer-networks" id="toc-challenge-of-multi-layer-networks" class="nav-link" data-scroll-target="#challenge-of-multi-layer-networks"><span class="header-section-number">1.7.1</span> Challenge of Multi-Layer Networks</a></li>
  <li><a href="#why-a-learning-algorithm-is-needed" id="toc-why-a-learning-algorithm-is-needed" class="nav-link" data-scroll-target="#why-a-learning-algorithm-is-needed"><span class="header-section-number">1.7.2</span> Why a Learning Algorithm is Needed?</a></li>
  <li><a href="#backpropagation-the-core-learning-algorithm" id="toc-backpropagation-the-core-learning-algorithm" class="nav-link" data-scroll-target="#backpropagation-the-core-learning-algorithm"><span class="header-section-number">1.7.3</span> Backpropagation: The Core Learning Algorithm</a></li>
  </ul></li>
  <li><a href="#mathematical-background-of-backpropagation" id="toc-mathematical-background-of-backpropagation" class="nav-link" data-scroll-target="#mathematical-background-of-backpropagation"><span class="header-section-number">1.8</span> Mathematical Background of Backpropagation</a>
  <ul class="collapse">
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function"><span class="header-section-number">1.8.1</span> Loss Function</a></li>
  <li><a href="#forward-pass" id="toc-forward-pass" class="nav-link" data-scroll-target="#forward-pass"><span class="header-section-number">1.8.2</span> Forward Pass</a></li>
  <li><a href="#backward-pass-computing-gradients" id="toc-backward-pass-computing-gradients" class="nav-link" data-scroll-target="#backward-pass-computing-gradients"><span class="header-section-number">1.8.3</span> Backward Pass: Computing Gradients</a></li>
  <li><a href="#weight-update-rule-gradient-descent" id="toc-weight-update-rule-gradient-descent" class="nav-link" data-scroll-target="#weight-update-rule-gradient-descent"><span class="header-section-number">1.8.4</span> Weight Update Rule (Gradient Descent)</a></li>
  <li><a href="#why-mlp-can-solve-xor" id="toc-why-mlp-can-solve-xor" class="nav-link" data-scroll-target="#why-mlp-can-solve-xor"><span class="header-section-number">1.8.5</span> Why MLP Can Solve XOR?</a></li>
  </ul></li>
  <li><a href="#implementing-xor-with-mlp" id="toc-implementing-xor-with-mlp" class="nav-link" data-scroll-target="#implementing-xor-with-mlp"><span class="header-section-number">1.9</span> Implementing XOR with MLP</a></li>
  <li><a href="#performance-metrics-a-critical-component-of-prediction-problems" id="toc-performance-metrics-a-critical-component-of-prediction-problems" class="nav-link" data-scroll-target="#performance-metrics-a-critical-component-of-prediction-problems"><span class="header-section-number">1.10</span> Performance Metrics: A Critical Component of Prediction Problems</a>
  <ul class="collapse">
  <li><a href="#confusion-matrix" id="toc-confusion-matrix" class="nav-link" data-scroll-target="#confusion-matrix"><span class="header-section-number">1.10.1</span> Confusion Matrix</a></li>
  <li><a href="#accuracy" id="toc-accuracy" class="nav-link" data-scroll-target="#accuracy"><span class="header-section-number">1.10.2</span> Accuracy</a></li>
  <li><a href="#precision" id="toc-precision" class="nav-link" data-scroll-target="#precision"><span class="header-section-number">1.10.3</span> Precision</a></li>
  <li><a href="#recall-sensitivity" id="toc-recall-sensitivity" class="nav-link" data-scroll-target="#recall-sensitivity"><span class="header-section-number">1.10.4</span> Recall (Sensitivity)</a></li>
  <li><a href="#specificity" id="toc-specificity" class="nav-link" data-scroll-target="#specificity"><span class="header-section-number">1.10.5</span> Specificity</a></li>
  <li><a href="#f1-score" id="toc-f1-score" class="nav-link" data-scroll-target="#f1-score"><span class="header-section-number">1.10.6</span> F1 Score</a></li>
  <li><a href="#relation-to-hypothesis-testing" id="toc-relation-to-hypothesis-testing" class="nav-link" data-scroll-target="#relation-to-hypothesis-testing"><span class="header-section-number">1.10.7</span> Relation to Hypothesis Testing</a></li>
  <li><a href="#balancing-errors" id="toc-balancing-errors" class="nav-link" data-scroll-target="#balancing-errors"><span class="header-section-number">1.10.8</span> Balancing Errors</a></li>
  <li><a href="#choosing-metrics" id="toc-choosing-metrics" class="nav-link" data-scroll-target="#choosing-metrics"><span class="header-section-number">1.10.9</span> Choosing Metrics</a></li>
  <li><a href="#interpretation-of-results-simulating-xor-gate-with-mlp-as-a-classification-problem" id="toc-interpretation-of-results-simulating-xor-gate-with-mlp-as-a-classification-problem" class="nav-link" data-scroll-target="#interpretation-of-results-simulating-xor-gate-with-mlp-as-a-classification-problem"><span class="header-section-number">1.10.10</span> Interpretation of Results: Simulating XOR Gate with MLP as a classification problem</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Basics of Artificial Neural Network</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction-to-artificial-neural-networks-anns" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="introduction-to-artificial-neural-networks-anns"><span class="header-section-number">1.1</span> Introduction to Artificial Neural Networks (ANNs)</h2>
<p>Machine learning has undeniably become a prominent and dynamic field, with its vast array of algorithms sometimes making it challenging to discern key concepts. To gain a clearer understanding, it is valuable to explore various machine learning algorithms in greater detail, focusing not only on their theoretical foundations but also on their step-by-step implementation.</p>
<p>In brief, machine learning is defined as a field that enables computers to learn from data without explicit programming (Arthur Samuel, 1959). It involves the development of algorithms capable of recognizing patterns in data and making decisions based on statistical analysis, probability theory, combinatorics, and optimization techniques.</p>
<p>This discussion begins with an exploration of perceptrons and ADALINE (Adaptive Linear Neuron), which are part of single-layer neural networks. The perceptron is the first algorithmically defined learning algorithm and serves as an intuitive, easy-to-implement introduction to modern machine learning algorithms, particularly artificial neural networks (or “deep learning”). ADALINE, an improvement on the perceptron, provides an excellent opportunity to understand gradient descent, a widely-used optimization method in machine learning.</p>
<section id="introduction" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1.1.1</span> Introduction</h3>
<p><strong>Artificial Neural Networks (ANNs)</strong> are a class of computational models inspired by the biological neural networks in the human brain. These models have revolutionized numerous fields, including machine learning, computer vision, natural language processing, and data analytics, by mimicking the complex processing power of the human brain in a simplified computational framework. ANNs are at the core of deep learning algorithms, enabling machines to learn from vast amounts of data, recognize patterns, and make decisions with little to no human intervention.</p>
<p>The concept of ANNs can be traced back to the 1940s when pioneers like Warren McCulloch and Walter Pitts introduced the first simplified model of the neuron. In more intuitive terms, neurons can be understood as the subunits of a neural network in a biological brain. Here, the signals of variable magnitudes arrive at the dendrites. Those input signals are then accumulated in the cell body of the neuron, and if the accumulated signal exceeds a certain threshold, a output signal is generated that which will be passed on by the axon. This model, known as the <strong>McCulloch-Pitts neuron</strong>, was a logical abstraction that represented a binary decision-making process based on weighted inputs. However, it wasn’t until the 1980s, with the development of the backpropagation algorithm by Geoffrey Hinton and others, that ANNs began to demonstrate significant potential for learning complex patterns and tasks.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Concepts in Artificial Neural Networks
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><strong>Neurons</strong>: The fundamental units in an ANN, inspired by biological neurons. Each neuron receives one or more inputs, processes them, and produces an output. The output is typically determined by applying an activation function to a weighted sum of the inputs.</p></li>
<li><p><strong>Architecture</strong>: An ANN is composed of layers of neurons:</p>
<ul>
<li><strong>Input Layer</strong>: The first layer that receives input data.</li>
<li><strong>Hidden Layers</strong>: Intermediate layers where computations occur and complex features are learned.</li>
<li><strong>Output Layer</strong>: The final layer that produces the output or prediction.</li>
</ul>
<p>The number of layers and the number of neurons in each layer are important design considerations that influence the network’s ability to learn complex relationships.</p></li>
<li><p><strong>Weights and Biases</strong>: Each connection between neurons has an associated weight, which determines the importance of the input. Biases are added to the weighted sum to allow the network to better model the data and shift the activation function.</p></li>
<li><p><strong>Activation Function</strong>: The activation function introduces non-linearity into the model, enabling it to learn and represent complex patterns. Common activation functions include:</p>
<ul>
<li><strong>Sigmoid</strong>: A logistic function that outputs values between 0 and 1.</li>
<li><strong>Tanh</strong>: A hyperbolic tangent function that outputs values between -1 and 1.</li>
<li><strong>ReLU (Rectified Linear Unit)</strong>: Outputs zero for negative inputs and the input value itself for positive inputs, helping mitigate the vanishing gradient problem in deep networks.</li>
</ul></li>
<li><p><strong>Learning Process</strong>: Training an ANN involves adjusting the weights and biases through a process called <strong>optimization</strong>, typically using the <strong>gradient descent</strong> algorithm. During training, the network’s predictions are compared to the actual outcomes, and the difference, known as the <strong>loss</strong> or <strong>error</strong>, is minimized using optimization techniques.</p></li>
<li><p><strong>Backpropagation</strong>: This algorithm computes the gradient of the loss function with respect to each weight by applying the chain rule of calculus. This information is used to update the weights in a way that reduces the overall error, allowing the network to improve over time.</p></li>
</ol>
</div>
</div>
</section>
<section id="historical-development-and-evolution" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="historical-development-and-evolution"><span class="header-section-number">1.1.2</span> Historical Development and Evolution</h3>
<p>The origins of neural networks lie in the early 20th century, with key milestones such as the <strong>Perceptron</strong> (developed by Frank Rosenblatt in 1958) and the <strong>Backpropagation Algorithm</strong> (1986), which was a breakthrough in training multilayer networks. The development of the <strong>Deep Learning</strong> paradigm in the 2000s, fueled by advances in computing power, large datasets, and efficient algorithms, further accelerated the application of ANNs. Notable examples include <strong>Convolutional Neural Networks (CNNs)</strong> for image recognition and <strong>Recurrent Neural Networks (RNNs)</strong> for sequence modeling, such as speech and language processing.</p>
</section>
<section id="modern-applications-of-artificial-neural-networks" class="level3" data-number="1.1.3">
<h3 data-number="1.1.3" class="anchored" data-anchor-id="modern-applications-of-artificial-neural-networks"><span class="header-section-number">1.1.3</span> Modern Applications of Artificial Neural Networks</h3>
<p>In recent years, the ability of ANNs to perform high-level tasks has grown substantially. Some of the transformative applications include:</p>
<ul>
<li><strong>Computer Vision</strong>: ANNs are used in image classification, object detection, facial recognition, and medical image analysis.</li>
<li><strong>Natural Language Processing (NLP)</strong>: ANNs, particularly <strong>transformer models</strong>, power state-of-the-art techniques in machine translation, sentiment analysis, and chatbots.</li>
<li><strong>Robotics and Autonomous Systems</strong>: Neural networks enable robots to perceive their environment and make real-time decisions.</li>
<li><strong>Healthcare</strong>: ANNs are applied in predictive analytics for disease diagnosis, treatment planning, and drug discovery.</li>
<li><strong>Finance</strong>: ANNs help in fraud detection, algorithmic trading, and customer behavior prediction.</li>
</ul>
</section>
<section id="challenges-and-future-directions" class="level3" data-number="1.1.4">
<h3 data-number="1.1.4" class="anchored" data-anchor-id="challenges-and-future-directions"><span class="header-section-number">1.1.4</span> Challenges and Future Directions</h3>
<p>Despite their powerful capabilities, ANNs face several challenges:</p>
<ul>
<li><strong>Overfitting</strong>: Neural networks can become too specialized to the training data, losing the ability to generalize to new, unseen data.</li>
<li><strong>Interpretability</strong>: The “black-box” nature of ANNs makes it difficult to understand how they arrive at specific decisions, which can be problematic in fields requiring explainability (e.g., healthcare, law).</li>
<li><strong>Data and Computation</strong>: Training deep neural networks requires large amounts of labeled data and significant computational resources, which can be limiting in certain contexts.</li>
</ul>
<p>Future research directions aim to address these challenges, including the development of more interpretable models, reducing the data and computation requirements, and creating more robust systems that can generalize across different domains.</p>
</section>
</section>
<section id="perceptron" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="perceptron"><span class="header-section-number">1.2</span> Perceptron</h2>
<p>To continue with the story, a few years after McCulloch and Walter Pitt, Frank Rosenblatt published the first concept of the Perceptron learning rule. The perceptron is one of the earliest neural network models <span class="citation" data-cites="rosenblatt1957perceptron">(<a href="references.html#ref-rosenblatt1957perceptron" role="doc-biblioref">Rosenblatt 1957</a>)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Frank Rosenblatt’s Perceptron
</div>
</div>
<div class="callout-body-container callout-body">
<p>The main idea was to define an algorithm in order to learn the values of the weights <span class="math inline">\(w\)</span> that are then multiplied with the input features in order to make a decision whether a neuron fires or not. In context of pattern classification, such an algorithm could be useful to determine if a sample belongs to one class or the other.</p>
</div>
</div>
<p>It models a single artificial neuron capable of binary classification for linearly separable data. Despite its limitations, the perceptron laid the foundation for more complex architectures like Multilayer Perceptrons (MLPs) and deep neural networks, which use sigmoid neurons for non-linear decision boundaries.</p>
<section id="perceptron-model" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="perceptron-model"><span class="header-section-number">1.2.1</span> Perceptron Model</h3>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    X1(Input X1) --&gt; W1(Weight W1)
    X2(Input X2) --&gt; W2(Weight W2)
    Xn(Input Xn) --&gt; Wn(Weight Wn)
    W1 --&gt; SUM[Summation Σ]
    W2 --&gt; SUM
    Wn --&gt; SUM
    B(Bias) --&gt; SUM
    SUM --&gt; ACT[Activation Function]
    ACT --&gt; Y(Output Y)
    
    style SUM fill:#f9f,stroke:#333,stroke-width:2px
    style ACT fill:#bbf,stroke:#333,stroke-width:2px
    style Y fill:#bfb,stroke:#333,stroke-width:2px
    classDef input fill:#ff9,stroke:#333,stroke-width:2px;
    classDef weight fill:#9cf,stroke:#333,stroke-width:2px;

    class X1,X2,Xn input;
    class W1,W2,Wn weight;
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The perceptron computes a weighted sum of inputs and applies a step function for classification: <span class="math display">\[
z = \sum_{i=1}^n w_i x_i + b
\]</span> <span class="math display">\[
\hat{y} = \begin{cases}
      1 &amp; \text{if } z &gt; 0 \\
      0 &amp; \text{otherwise}
   \end{cases}
\]</span></p>
</section>
<section id="perceptron-learning-rule" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="perceptron-learning-rule"><span class="header-section-number">1.2.2</span> Perceptron Learning Rule</h3>
<p>Weights are updated iteratively based on the error: <span class="math display">\[
w_i \gets w_i + \eta (y - \hat{y}) x_i
\]</span> where <span class="math inline">\(\eta\)</span> is the learning rate.</p>
</section>
<section id="perceptron-algorithm" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="perceptron-algorithm"><span class="header-section-number">1.2.3</span> Perceptron Algorithm</h3>
<p><strong>Input:</strong> A dataset <span class="math inline">\(D = \{(x_i, y_i)\}\)</span>, where <span class="math inline">\(x_i\)</span> is the feature vector and <span class="math inline">\(y_i\in \{+1,-1\}\)</span> is the label.<br>
<strong>Output:</strong> A weight vector <span class="math inline">\(\mathbf{w}\)</span>.</p>
<ol type="1">
<li>Initialize <span class="math inline">\(\mathbf{w} = \mathbf{0}\)</span>.</li>
<li><strong>Repeat until convergence</strong>:
<ul>
<li>Set <span class="math inline">\(m = 0\)</span>.</li>
<li>For each <span class="math inline">\((x_i, y_i) \in D\)</span>:
<ul>
<li>If <span class="math inline">\(y_i (\mathbf{w}^T \cdot \mathbf{x_i}) \leq 0\)</span>:
<ol type="1">
<li>Update <span class="math inline">\(\mathbf{w} \gets \mathbf{w} + y_i \mathbf{x_i}\)</span>.</li>
<li>Increment <span class="math inline">\(m \gets m + 1\)</span>.</li>
</ol></li>
</ul></li>
<li>If <span class="math inline">\(m = 0\)</span>, terminate the algorithm.</li>
</ul></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Perceptron Convergence
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Perceptron was arguably the first algorithm with a strong formal guarantee. If a data set is linearly separable, the Perceptron will find a separating hyperplane in a finite number of updates. (If the data is not linearly separable, it will loop forever.)</p>
</div>
</div>
</section>
<section id="perceptron-theorem-and-margin" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="perceptron-theorem-and-margin"><span class="header-section-number">1.2.4</span> Perceptron Theorem and Margin</h3>
<p>The <strong>Perceptron Mistake Bound Theorem</strong> states that the Perceptron algorithm makes at most <span class="math inline">\(\frac{1}{\gamma^2}\)</span> updates (mistakes), where <span class="math inline">\(\gamma\)</span> is the margin of separability of the data. The <strong>margin</strong> is defined as the smallest distance between any data point and the decision boundary, normalized by the magnitude of the weight vector: <span class="math display">\[
\gamma = \frac{\min_{i} y_i (\mathbf{w}^T \mathbf{x_i})}{\|\mathbf{w}\|}
\]</span> where: - <span class="math inline">\(\mathbf{w}\)</span> is the weight vector. - <span class="math inline">\(\mathbf{x_i}\)</span> is a data point. - <span class="math inline">\(y_i\)</span> is the corresponding label ((+1) or (-1)).</p>
</section>
<section id="implications-of-the-theorem" class="level3" data-number="1.2.5">
<h3 data-number="1.2.5" class="anchored" data-anchor-id="implications-of-the-theorem"><span class="header-section-number">1.2.5</span> Implications of the Theorem</h3>
<ol type="1">
<li><p><strong>Large Margin is Desirable</strong>:</p>
<ul>
<li>A larger margin <span class="math inline">\(\gamma\)</span> implies fewer mistakes because the mistake bound decreases as <span class="math inline">\(\gamma\)</span> increases.</li>
<li>Intuitively, a larger margin means the data points are farther from the decision boundary, making them less likely to be misclassified.</li>
</ul></li>
<li><p><strong>Quick Convergence</strong>:</p>
<ul>
<li>The algorithm will converge faster on datasets with a larger margin since fewer updates (or mistakes) are required.</li>
<li>Conversely, if <span class="math inline">\(\gamma\)</span> is small (data points are closer to the boundary), the algorithm requires more updates to separate the data correctly.</li>
</ul></li>
</ol>
</section>
<section id="characterizing-data-sets-for-fast-convergence" class="level3" data-number="1.2.6">
<h3 data-number="1.2.6" class="anchored" data-anchor-id="characterizing-data-sets-for-fast-convergence"><span class="header-section-number">1.2.6</span> Characterizing Data Sets for Fast Convergence</h3>
<p>Datasets for which the Perceptron algorithm converges quickly share the following properties:</p>
<ol type="1">
<li><p><strong>Large Margin</strong>:</p>
<ul>
<li>Data points are well-separated from the decision boundary.</li>
<li>The decision boundary can be drawn with minimal ambiguity.</li>
</ul></li>
<li><p><strong>Linearly Separable Data</strong>:</p>
<ul>
<li>The dataset must be linearly separable for the Perceptron algorithm to converge.</li>
<li>Overlapping or inseparable datasets will cause the algorithm to loop indefinitely.</li>
</ul></li>
</ol>
</section>
<section id="example-of-a-dataset-with-large-margin" class="level3" data-number="1.2.7">
<h3 data-number="1.2.7" class="anchored" data-anchor-id="example-of-a-dataset-with-large-margin"><span class="header-section-number">1.2.7</span> Example of a Dataset with Large Margin</h3>
<p>Consider the following dataset in two-dimensional space:</p>
<ul>
<li>Positive class (<span class="math inline">\(y_i = +1\)</span>): Points <span class="math inline">\((3, 3), (4, 4), (5, 5)\)</span>.</li>
<li>Negative class (<span class="math inline">\(y_i = -1\)</span>): Points <span class="math inline">\((-3, -3), (-4, -4), (-5, -5)\)</span>.</li>
</ul>
<p>The data is linearly separable with a large margin (distance between closest points and the decision boundary).</p>
<div id="f54bdabc" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataset</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>positive_points <span class="op">=</span> np.array([[<span class="dv">3</span>, <span class="dv">3</span>], [<span class="dv">4</span>, <span class="dv">4</span>], [<span class="dv">5</span>, <span class="dv">5</span>]])</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>negative_points <span class="op">=</span> np.array([[<span class="op">-</span><span class="dv">3</span>, <span class="op">-</span><span class="dv">3</span>], [<span class="op">-</span><span class="dv">4</span>, <span class="op">-</span><span class="dv">4</span>], [<span class="op">-</span><span class="dv">5</span>, <span class="op">-</span><span class="dv">5</span>]])</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define weights and bias for the Perceptron hyperplane</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Example weights (assuming the Perceptron learned these weights)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>])  <span class="co"># w1 and w2</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="dv">0</span>                 <span class="co"># Bias term</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate x1 values for plotting</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">100</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute x2 values from the hyperplane equation w1*x1 + w2*x2 + b = 0</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> <span class="op">-</span>(w[<span class="dv">0</span>] <span class="op">*</span> x1 <span class="op">+</span> b) <span class="op">/</span> w[<span class="dv">1</span>]</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the dataset</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.scatter(positive_points[:, <span class="dv">0</span>], positive_points[:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Positive Class ($y=+1$)'</span>, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.scatter(negative_points[:, <span class="dv">0</span>], negative_points[:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Negative Class ($y=-1$)'</span>, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the decision boundary (hyperplane)</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.plot(x1, x2, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'Decision Boundary ($w^T x = 0$)'</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Formatting the plot</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>, linestyle<span class="op">=</span><span class="st">'-'</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>, linestyle<span class="op">=</span><span class="st">'-'</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Linearly Separable Dataset with Perceptron Decision Boundary'</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$x_1$'</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$x_2$'</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-2-output-1.png" width="592" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="simulating-or-gate-using-perceptron" class="level3" data-number="1.2.8">
<h3 data-number="1.2.8" class="anchored" data-anchor-id="simulating-or-gate-using-perceptron"><span class="header-section-number">1.2.8</span> Simulating <code>OR</code> gate using Perceptron</h3>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<svg width="672" height="480" viewbox="0.00 0.00 303.15 399.20" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 395.2)">
<title>perceptron</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-395.2 299.15,-395.2 299.15,4 -4,4"></polygon>
<!-- input1 -->
<g id="node1" class="node">
<title>input1</title>
<ellipse fill="lightblue" stroke="black" cx="64.57" cy="-373.2" rx="20.89" ry="18"></ellipse>
<text text-anchor="middle" x="64.57" y="-369" font-family="Times,serif" font-size="14.00">x1</text>
</g>
<!-- weight1 -->
<g id="node4" class="node">
<title>weight1</title>
<ellipse fill="lightblue" stroke="black" cx="64.57" cy="-284.4" rx="22.68" ry="18"></ellipse>
<text text-anchor="middle" x="64.57" y="-280.2" font-family="Times,serif" font-size="14.00">w1</text>
</g>
<!-- input1&#45;&gt;weight1 -->
<g id="edge1" class="edge">
<title>input1-&gt;weight1</title>
<path fill="none" stroke="black" d="M64.57,-355.05C64.57,-342.92 64.57,-326.42 64.57,-312.52"></path>
<polygon fill="black" stroke="black" points="68.07,-312.51 64.57,-302.51 61.07,-312.51 68.07,-312.51"></polygon>
<text text-anchor="middle" x="73.13" y="-324.6" font-family="Times,serif" font-size="14.00">w1</text>
</g>
<!-- input2 -->
<g id="node2" class="node">
<title>input2</title>
<ellipse fill="lightblue" stroke="black" cx="230.57" cy="-373.2" rx="20.89" ry="18"></ellipse>
<text text-anchor="middle" x="230.57" y="-369" font-family="Times,serif" font-size="14.00">x2</text>
</g>
<!-- weight2 -->
<g id="node5" class="node">
<title>weight2</title>
<ellipse fill="lightblue" stroke="black" cx="230.57" cy="-284.4" rx="22.68" ry="18"></ellipse>
<text text-anchor="middle" x="230.57" y="-280.2" font-family="Times,serif" font-size="14.00">w2</text>
</g>
<!-- input2&#45;&gt;weight2 -->
<g id="edge2" class="edge">
<title>input2-&gt;weight2</title>
<path fill="none" stroke="black" d="M230.57,-355.05C230.57,-342.92 230.57,-326.42 230.57,-312.52"></path>
<polygon fill="black" stroke="black" points="234.07,-312.51 230.57,-302.51 227.07,-312.51 234.07,-312.51"></polygon>
<text text-anchor="middle" x="239.13" y="-324.6" font-family="Times,serif" font-size="14.00">w2</text>
</g>
<!-- bias -->
<g id="node3" class="node">
<title>bias</title>
<ellipse fill="lightgreen" stroke="black" cx="147.57" cy="-284.4" rx="41.76" ry="18"></ellipse>
<text text-anchor="middle" x="147.57" y="-280.2" font-family="Times,serif" font-size="14.00">Bias (b)</text>
</g>
<!-- weighted_sum -->
<g id="node6" class="node">
<title>weighted_sum</title>
<polygon fill="lightblue" stroke="black" points="219.08,-213.6 76.06,-213.6 76.06,-177.6 219.08,-177.6 219.08,-213.6"></polygon>
<text text-anchor="middle" x="147.57" y="-191.4" font-family="Times,serif" font-size="14.00">w1 * x1 + w2 * x2 + b</text>
</g>
<!-- bias&#45;&gt;weighted_sum -->
<g id="edge5" class="edge">
<title>bias-&gt;weighted_sum</title>
<path fill="none" stroke="black" d="M147.57,-266.25C147.57,-254.12 147.57,-237.62 147.57,-223.72"></path>
<polygon fill="black" stroke="black" points="151.07,-223.71 147.57,-213.71 144.07,-223.71 151.07,-223.71"></polygon>
<text text-anchor="middle" x="158.52" y="-235.8" font-family="Times,serif" font-size="14.00"> + b</text>
</g>
<!-- weight1&#45;&gt;weighted_sum -->
<g id="edge3" class="edge">
<title>weight1-&gt;weighted_sum</title>
<path fill="none" stroke="black" d="M77.91,-269.45C90.35,-256.44 109.19,-236.74 124.15,-221.1"></path>
<polygon fill="black" stroke="black" points="126.75,-223.44 131.13,-213.8 121.69,-218.61 126.75,-223.44"></polygon>
<text text-anchor="middle" x="125.82" y="-235.8" font-family="Times,serif" font-size="14.00">* x1</text>
</g>
<!-- weight2&#45;&gt;weighted_sum -->
<g id="edge4" class="edge">
<title>weight2-&gt;weighted_sum</title>
<path fill="none" stroke="black" d="M217.24,-269.45C204.79,-256.44 185.95,-236.74 171,-221.1"></path>
<polygon fill="black" stroke="black" points="173.46,-218.61 164.02,-213.8 168.4,-223.44 173.46,-218.61"></polygon>
<text text-anchor="middle" x="208.82" y="-235.8" font-family="Times,serif" font-size="14.00">* x2</text>
</g>
<!-- activation -->
<g id="node7" class="node">
<title>activation</title>
<polygon fill="lightblue" stroke="black" points="295.22,-124.8 -0.07,-124.8 -0.07,-88.8 295.22,-88.8 295.22,-124.8"></polygon>
<text text-anchor="middle" x="147.57" y="-102.6" font-family="Times,serif" font-size="14.00">Activation Step Function: f(x) = 1 if x &gt;= 0 else 0</text>
</g>
<!-- weighted_sum&#45;&gt;activation -->
<g id="edge6" class="edge">
<title>weighted_sum-&gt;activation</title>
<path fill="none" stroke="black" d="M147.57,-177.45C147.57,-165.32 147.57,-148.82 147.57,-134.92"></path>
<polygon fill="black" stroke="black" points="151.07,-134.91 147.57,-124.91 144.07,-134.91 151.07,-134.91"></polygon>
<text text-anchor="middle" x="166.82" y="-147" font-family="Times,serif" font-size="14.00">pass to</text>
</g>
<!-- output -->
<g id="node8" class="node">
<title>output</title>
<ellipse fill="lightyellow" stroke="black" cx="147.57" cy="-18" rx="51.57" ry="18"></ellipse>
<text text-anchor="middle" x="147.57" y="-13.8" font-family="Times,serif" font-size="14.00">y (Output)</text>
</g>
<!-- activation&#45;&gt;output -->
<g id="edge7" class="edge">
<title>activation-&gt;output</title>
<path fill="none" stroke="black" d="M147.57,-88.65C147.57,-76.52 147.57,-60.02 147.57,-46.12"></path>
<polygon fill="black" stroke="black" points="151.07,-46.11 147.57,-36.11 144.07,-46.11 151.07,-46.11"></polygon>
<text text-anchor="middle" x="183.93" y="-58.2" font-family="Times,serif" font-size="14.00">step function</text>
</g>
<!-- output&#45;&gt;output -->
<g id="edge8" class="edge">
<title>output-&gt;output</title>
<path fill="none" stroke="black" d="M193.71,-26.22C206.92,-25.9 217.11,-23.16 217.11,-18 217.11,-14.25 211.73,-11.78 203.72,-10.59"></path>
<polygon fill="black" stroke="black" points="203.96,-7.1 193.71,-9.78 203.4,-14.07 203.96,-7.1"></polygon>
<text text-anchor="middle" x="220.61" y="-13.8" font-family="Times,serif" font-size="14.00">y</text>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The following python code will simulate the OR gate using the logic of perceptron.</p>
<div id="db631331" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Step function (activation function)</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step_function(x):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;=</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Perceptron training algorithm</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> perceptron(X, y, learning_rate<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize weights and bias</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> np.zeros(X.shape[<span class="dv">1</span>])</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    bias <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Training process</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        total_error <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(X)):</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate weighted sum (z)</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> np.dot(X[i], weights) <span class="op">+</span> bias</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Apply step function to get prediction</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>            prediction <span class="op">=</span> step_function(z)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate error</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>            error <span class="op">=</span> y[i] <span class="op">-</span> prediction</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>            total_error <span class="op">+=</span> <span class="bu">abs</span>(error)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update weights and bias based on the error</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>            weights <span class="op">+=</span> learning_rate <span class="op">*</span> error <span class="op">*</span> X[i]</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>            bias <span class="op">+=</span> learning_rate <span class="op">*</span> error</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optionally, print the error for each epoch</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">: Total Error = </span><span class="sc">{</span>total_error<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> weights, bias</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Perceptron test function</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(X, weights, bias):</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> []</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(X)):</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> np.dot(X[i], weights) <span class="op">+</span> bias</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> step_function(z)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        predictions.append(prediction)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> predictions</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="co"># OR Gate Inputs and Outputs</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Input X: [A, B] where A and B are the inputs</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Output y: The corresponding OR operation output</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>])  <span class="co"># OR gate outputs</span></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the perceptron</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>weights, bias <span class="op">=</span> perceptron(X, y, learning_rate<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the perceptron</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> predict(X, weights, bias)</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Print predictions</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Predictions:"</span>)</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(X)):</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Input: </span><span class="sc">{</span>X[i]<span class="sc">}</span><span class="ss">, Predicted Output: </span><span class="sc">{</span>predictions[i]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1: Total Error = 2
Epoch 2: Total Error = 2
Epoch 3: Total Error = 1
Epoch 4: Total Error = 0
Epoch 5: Total Error = 0
Epoch 6: Total Error = 0
Epoch 7: Total Error = 0
Epoch 8: Total Error = 0
Epoch 9: Total Error = 0
Epoch 10: Total Error = 0

Predictions:
Input: [0 0], Predicted Output: 0
Input: [0 1], Predicted Output: 1
Input: [1 0], Predicted Output: 1
Input: [1 1], Predicted Output: 1</code></pre>
</div>
</div>
</section>
<section id="simulating-and-gate-using-a-perceptron" class="level3" data-number="1.2.9">
<h3 data-number="1.2.9" class="anchored" data-anchor-id="simulating-and-gate-using-a-perceptron"><span class="header-section-number">1.2.9</span> Simulating <code>AND</code> gate using a perceptron</h3>
<p>The perceptron model for an AND gate is shown below.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<svg width="672" height="480" viewbox="0.00 0.00 303.15 399.20" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 395.2)">
<title>perceptron</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-395.2 299.15,-395.2 299.15,4 -4,4"></polygon>
<!-- input1 -->
<g id="node1" class="node">
<title>input1</title>
<ellipse fill="lightblue" stroke="black" cx="64.57" cy="-373.2" rx="20.89" ry="18"></ellipse>
<text text-anchor="middle" x="64.57" y="-369" font-family="Times,serif" font-size="14.00">x1</text>
</g>
<!-- weight1 -->
<g id="node4" class="node">
<title>weight1</title>
<ellipse fill="lightblue" stroke="black" cx="64.57" cy="-284.4" rx="22.68" ry="18"></ellipse>
<text text-anchor="middle" x="64.57" y="-280.2" font-family="Times,serif" font-size="14.00">w1</text>
</g>
<!-- input1&#45;&gt;weight1 -->
<g id="edge1" class="edge">
<title>input1-&gt;weight1</title>
<path fill="none" stroke="black" d="M64.57,-355.05C64.57,-342.92 64.57,-326.42 64.57,-312.52"></path>
<polygon fill="black" stroke="black" points="68.07,-312.51 64.57,-302.51 61.07,-312.51 68.07,-312.51"></polygon>
<text text-anchor="middle" x="73.13" y="-324.6" font-family="Times,serif" font-size="14.00">w1</text>
</g>
<!-- input2 -->
<g id="node2" class="node">
<title>input2</title>
<ellipse fill="lightblue" stroke="black" cx="230.57" cy="-373.2" rx="20.89" ry="18"></ellipse>
<text text-anchor="middle" x="230.57" y="-369" font-family="Times,serif" font-size="14.00">x2</text>
</g>
<!-- weight2 -->
<g id="node5" class="node">
<title>weight2</title>
<ellipse fill="lightblue" stroke="black" cx="230.57" cy="-284.4" rx="22.68" ry="18"></ellipse>
<text text-anchor="middle" x="230.57" y="-280.2" font-family="Times,serif" font-size="14.00">w2</text>
</g>
<!-- input2&#45;&gt;weight2 -->
<g id="edge2" class="edge">
<title>input2-&gt;weight2</title>
<path fill="none" stroke="black" d="M230.57,-355.05C230.57,-342.92 230.57,-326.42 230.57,-312.52"></path>
<polygon fill="black" stroke="black" points="234.07,-312.51 230.57,-302.51 227.07,-312.51 234.07,-312.51"></polygon>
<text text-anchor="middle" x="239.13" y="-324.6" font-family="Times,serif" font-size="14.00">w2</text>
</g>
<!-- bias -->
<g id="node3" class="node">
<title>bias</title>
<ellipse fill="lightgreen" stroke="black" cx="147.57" cy="-284.4" rx="41.76" ry="18"></ellipse>
<text text-anchor="middle" x="147.57" y="-280.2" font-family="Times,serif" font-size="14.00">Bias (b)</text>
</g>
<!-- weighted_sum -->
<g id="node6" class="node">
<title>weighted_sum</title>
<polygon fill="lightblue" stroke="black" points="219.08,-213.6 76.06,-213.6 76.06,-177.6 219.08,-177.6 219.08,-213.6"></polygon>
<text text-anchor="middle" x="147.57" y="-191.4" font-family="Times,serif" font-size="14.00">w1 * x1 + w2 * x2 + b</text>
</g>
<!-- bias&#45;&gt;weighted_sum -->
<g id="edge5" class="edge">
<title>bias-&gt;weighted_sum</title>
<path fill="none" stroke="black" d="M147.57,-266.25C147.57,-254.12 147.57,-237.62 147.57,-223.72"></path>
<polygon fill="black" stroke="black" points="151.07,-223.71 147.57,-213.71 144.07,-223.71 151.07,-223.71"></polygon>
<text text-anchor="middle" x="158.52" y="-235.8" font-family="Times,serif" font-size="14.00"> + b</text>
</g>
<!-- weight1&#45;&gt;weighted_sum -->
<g id="edge3" class="edge">
<title>weight1-&gt;weighted_sum</title>
<path fill="none" stroke="black" d="M77.91,-269.45C90.35,-256.44 109.19,-236.74 124.15,-221.1"></path>
<polygon fill="black" stroke="black" points="126.75,-223.44 131.13,-213.8 121.69,-218.61 126.75,-223.44"></polygon>
<text text-anchor="middle" x="125.82" y="-235.8" font-family="Times,serif" font-size="14.00">* x1</text>
</g>
<!-- weight2&#45;&gt;weighted_sum -->
<g id="edge4" class="edge">
<title>weight2-&gt;weighted_sum</title>
<path fill="none" stroke="black" d="M217.24,-269.45C204.79,-256.44 185.95,-236.74 171,-221.1"></path>
<polygon fill="black" stroke="black" points="173.46,-218.61 164.02,-213.8 168.4,-223.44 173.46,-218.61"></polygon>
<text text-anchor="middle" x="208.82" y="-235.8" font-family="Times,serif" font-size="14.00">* x2</text>
</g>
<!-- activation -->
<g id="node7" class="node">
<title>activation</title>
<polygon fill="lightblue" stroke="black" points="295.22,-124.8 -0.07,-124.8 -0.07,-88.8 295.22,-88.8 295.22,-124.8"></polygon>
<text text-anchor="middle" x="147.57" y="-102.6" font-family="Times,serif" font-size="14.00">Activation Step Function: f(x) = 1 if x &gt;= 0 else 0</text>
</g>
<!-- weighted_sum&#45;&gt;activation -->
<g id="edge6" class="edge">
<title>weighted_sum-&gt;activation</title>
<path fill="none" stroke="black" d="M147.57,-177.45C147.57,-165.32 147.57,-148.82 147.57,-134.92"></path>
<polygon fill="black" stroke="black" points="151.07,-134.91 147.57,-124.91 144.07,-134.91 151.07,-134.91"></polygon>
<text text-anchor="middle" x="166.82" y="-147" font-family="Times,serif" font-size="14.00">pass to</text>
</g>
<!-- output -->
<g id="node8" class="node">
<title>output</title>
<ellipse fill="lightyellow" stroke="black" cx="147.57" cy="-18" rx="51.57" ry="18"></ellipse>
<text text-anchor="middle" x="147.57" y="-13.8" font-family="Times,serif" font-size="14.00">y (Output)</text>
</g>
<!-- activation&#45;&gt;output -->
<g id="edge7" class="edge">
<title>activation-&gt;output</title>
<path fill="none" stroke="black" d="M147.57,-88.65C147.57,-76.52 147.57,-60.02 147.57,-46.12"></path>
<polygon fill="black" stroke="black" points="151.07,-46.11 147.57,-36.11 144.07,-46.11 151.07,-46.11"></polygon>
<text text-anchor="middle" x="183.93" y="-58.2" font-family="Times,serif" font-size="14.00">step function</text>
</g>
<!-- output&#45;&gt;output -->
<g id="edge8" class="edge">
<title>output-&gt;output</title>
<path fill="none" stroke="black" d="M193.71,-26.22C206.92,-25.9 217.11,-23.16 217.11,-18 217.11,-14.25 211.73,-11.78 203.72,-10.59"></path>
<polygon fill="black" stroke="black" points="203.96,-7.1 193.71,-9.78 203.4,-14.07 203.96,-7.1"></polygon>
<text text-anchor="middle" x="220.61" y="-13.8" font-family="Times,serif" font-size="14.00">y</text>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The python code for simulating the <code>AND</code> gate using the perceptron is shown below.</p>
<div id="473d5aa7" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Step function (activation function)</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step_function(x):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;=</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Perceptron training algorithm</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> perceptron(X, y, learning_rate<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize weights and bias</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> np.zeros(X.shape[<span class="dv">1</span>])</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    bias <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Training process</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        total_error <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(X)):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate weighted sum (z)</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> np.dot(X[i], weights) <span class="op">+</span> bias</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Apply step function to get prediction</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>            prediction <span class="op">=</span> step_function(z)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate error</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>            error <span class="op">=</span> y[i] <span class="op">-</span> prediction</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>            total_error <span class="op">+=</span> <span class="bu">abs</span>(error)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update weights and bias based on the error</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>            weights <span class="op">+=</span> learning_rate <span class="op">*</span> error <span class="op">*</span> X[i]</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>            bias <span class="op">+=</span> learning_rate <span class="op">*</span> error</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optionally, print the error for each epoch</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">: Total Error = </span><span class="sc">{</span>total_error<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> weights, bias</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Perceptron test function</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(X, weights, bias):</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> []</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(X)):</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> np.dot(X[i], weights) <span class="op">+</span> bias</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> step_function(z)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        predictions.append(prediction)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> predictions</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="co"># OR Gate Inputs and Outputs</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Input X: [A, B] where A and B are the inputs</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Output y: The corresponding OR operation output</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>])  <span class="co"># OR gate outputs</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the perceptron</span></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>weights, bias <span class="op">=</span> perceptron(X, y, learning_rate<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the perceptron</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> predict(X, weights, bias)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Print predictions</span></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Predictions:"</span>)</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(X)):</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Input: </span><span class="sc">{</span>X[i]<span class="sc">}</span><span class="ss">, Predicted Output: </span><span class="sc">{</span>predictions[i]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1: Total Error = 2
Epoch 2: Total Error = 2
Epoch 3: Total Error = 1
Epoch 4: Total Error = 0
Epoch 5: Total Error = 0
Epoch 6: Total Error = 0
Epoch 7: Total Error = 0
Epoch 8: Total Error = 0
Epoch 9: Total Error = 0
Epoch 10: Total Error = 0

Predictions:
Input: [0 0], Predicted Output: 0
Input: [0 1], Predicted Output: 1
Input: [1 0], Predicted Output: 1
Input: [1 1], Predicted Output: 1</code></pre>
</div>
</div>
</section>
<section id="simulating-all-logic-gates-using-the-perceptron" class="level3" data-number="1.2.10">
<h3 data-number="1.2.10" class="anchored" data-anchor-id="simulating-all-logic-gates-using-the-perceptron"><span class="header-section-number">1.2.10</span> Simulating all logic gates using the Perceptron</h3>
<p>Here is the Python code to simulate all logic gates.</p>
<div id="0e218c82" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceptron:</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, learning_rate<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_dim <span class="op">=</span> input_dim</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Include bias weight with size (input_dim + 1)</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.random.randn(input_dim <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.01</span>  <span class="co"># +1 for the bias term</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> activation(<span class="va">self</span>, net_input):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">        Apply the activation function (step function).</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co">        net_input (numpy.ndarray): Net input values.</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co">        numpy.ndarray: Activated output (0 or 1).</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.where(net_input <span class="op">&gt;</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="co">        Perform a forward pass to calculate the output of the perceptron.</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="co">        X (numpy.ndarray): Input data of shape (n_samples, input_dim).</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="co">        numpy.ndarray: Predicted binary outputs (0 or 1), shape (n_samples,).</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Ensure X is a 2D array (n_samples, n_features)</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> X.ndim <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>            X <span class="op">=</span> X.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># Reshape if it's a single sample</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add bias term to input before predicting</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>        X_bias <span class="op">=</span> np.c_[X, np.ones(X.shape[<span class="dv">0</span>])]  <span class="co"># Add bias column for prediction</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>        net_input <span class="op">=</span> np.dot(X_bias, <span class="va">self</span>.weights)  <span class="co"># Dot product with weights</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.activation(net_input)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, X, y, epochs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="co">        Train the perceptron using the provided data.</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a><span class="co">        X (numpy.ndarray): Input data of shape (n_samples, input_dim).</span></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a><span class="co">        y (numpy.ndarray): Target labels of shape (n_samples,).</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a><span class="co">        epochs (int): Number of iterations over the dataset.</span></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> xi, yi <span class="kw">in</span> <span class="bu">zip</span>(X, y):</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>                xi_bias <span class="op">=</span> np.append(xi, <span class="dv">1</span>)  <span class="co"># Add bias term manually for training</span></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Compute the weighted sum (net input) directly here</span></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>                weighted_input <span class="op">=</span> np.dot(xi_bias, <span class="va">self</span>.weights)  <span class="co"># Dot product with weights</span></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>                prediction <span class="op">=</span> <span class="va">self</span>.activation(weighted_input)  <span class="co"># Apply activation</span></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>                error <span class="op">=</span> yi <span class="op">-</span> prediction  <span class="co"># Compute error</span></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.weights <span class="op">+=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> error <span class="op">*</span> xi_bias  <span class="co"># Update weights</span></span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Non-member function to simulate logic gates</span></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simulate_logic_gate(gate_type):</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a><span class="co">    Simulate a logic gate using the Perceptron.</span></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a><span class="co">    gate_type (str): The type of logic gate ('AND', 'OR', 'NAND', 'NOR', 'XOR').</span></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a><span class="co">    Prints the results of the simulation.</span></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define input and target outputs for different gates</span></span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>    gate_targets <span class="op">=</span> {</span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>        <span class="st">'AND'</span>: np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]),</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>        <span class="st">'OR'</span>: np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]),</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a>        <span class="st">'NAND'</span>: np.array([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>]),</span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>        <span class="st">'NOR'</span>: np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]),</span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a>        <span class="st">'XOR'</span>: np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> gate_type <span class="kw">not</span> <span class="kw">in</span> gate_targets:</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Unsupported gate type: </span><span class="sc">{</span>gate_type<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>    <span class="co">#if gate_type == 'XOR':</span></span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    print(f"Warning: XOR is not linearly separable and cannot be solved by a single perceptron!")</span></span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    return</span></span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> gate_targets[gate_type]</span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize perceptron</span></span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a>    perceptron <span class="op">=</span> Perceptron(input_dim<span class="op">=</span><span class="dv">2</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train perceptron</span></span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>    perceptron.train(X, y, epochs<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test predictions</span></span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Simulating </span><span class="sc">{</span>gate_type<span class="sc">}</span><span class="ss"> gate"</span>)</span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Trained weights:"</span>, perceptron.weights[:<span class="op">-</span><span class="dv">1</span>])  <span class="co"># Exclude bias term from display</span></span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Trained bias:"</span>, perceptron.weights[<span class="op">-</span><span class="dv">1</span>])  <span class="co"># Display bias term separately</span></span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">0</span>]):  <span class="co"># Iterate over the rows of X (4 samples)</span></span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a>        input_data <span class="op">=</span> X[i]  <span class="co"># Extract the i-th row as a 1D array</span></span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> perceptron.predict(input_data)  <span class="co"># Make prediction</span></span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Input: </span><span class="sc">{</span>input_data<span class="sc">}</span><span class="ss">, Prediction: </span><span class="sc">{</span>prediction<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate AND, OR, NAND, NOR, and XOR gates</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The simulation program is executed below:</p>
<div id="1cc3d2e7" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> gate <span class="kw">in</span> [<span class="st">'AND'</span>, <span class="st">'OR'</span>, <span class="st">'NAND'</span>, <span class="st">'NOR'</span>, <span class="st">'XOR'</span>]:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    simulate_logic_gate(gate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Simulating AND gate
Trained weights: [0.19498255 0.10392607]
Trained bias: -0.2882517529383646
Input: [0 0], Prediction: [0]
Input: [0 1], Prediction: [0]
Input: [1 0], Prediction: [0]
Input: [1 1], Prediction: [1]
Simulating OR gate
Trained weights: [0.1166473  0.09319517]
Trained bias: -0.09265357721473033
Input: [0 0], Prediction: [0]
Input: [0 1], Prediction: [1]
Input: [1 0], Prediction: [1]
Input: [1 1], Prediction: [1]
Simulating NAND gate
Trained weights: [-0.19887624 -0.09682712]
Trained bias: 0.20933520906978198
Input: [0 0], Prediction: [1]
Input: [0 1], Prediction: [1]
Input: [1 0], Prediction: [1]
Input: [1 1], Prediction: [0]
Simulating NOR gate
Trained weights: [-0.09794677 -0.11003972]
Trained bias: 0.09665155437250492
Input: [0 0], Prediction: [1]
Input: [0 1], Prediction: [0]
Input: [1 0], Prediction: [0]
Input: [1 1], Prediction: [0]
Simulating XOR gate
Trained weights: [-0.08718506 -0.00030597]
Trained bias: 0.012018877077856385
Input: [0 0], Prediction: [1]
Input: [0 1], Prediction: [1]
Input: [1 0], Prediction: [0]
Input: [1 1], Prediction: [0]</code></pre>
</div>
</div>
<p>whether the perceptron win on XOR gate? No, the perceptron cannot learn the XOR gate. This is a well-known limitation of the perceptron model, and it’s related to the fact that the XOR function is non-linearly separable.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Perceptron fails!
</div>
</div>
<div class="callout-body-container callout-body">
<p>A perceptron can only solve problems that are linearly separable, meaning that the classes (outputs) can be separated by a straight line (or a hyperplane in higher dimensions). The XOR gate outputs 1 when the inputs are (0, 1) or (1, 0), and outputs 0 when the inputs are (0, 0) or (1, 1).</p>
<p>If you try to plot these points, you’ll see that you can’t separate the positive examples (output 1) from the negative examples (output 0) with a single straight line.</p>
</div>
</div>
<p>As solution to this problem, we need the cocept of Multi-layer perceptron.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Perceptron neuron- the foundation of modern Machine Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Perceptron model, introduced by Frank Rosenblatt in 1958, marked one of the earliest developments in artificial intelligence and neural networks. Initially conceived as a model for pattern recognition and early neural computation, the Perceptron was designed to simulate a biological neuron, learning to classify inputs into two categories through a simple linear decision boundary. Despite its early promise, the limitations of the basic Perceptron were exposed in the 1960s, particularly its inability to solve non-linearly separable problems, famously highlighted in Marvin Minsky and Seymour Papert’s book Perceptrons (1969). However, with the advent of more sophisticated algorithms and architectures, such as multi-layer perceptrons (MLPs) and the backpropagation algorithm in the 1980s, the Perceptron concept was revitalized. Today, it forms the foundational concept for deep learning models and modern neural networks, which are widely applied in various fields, including image and speech recognition, natural language processing, and autonomous systems, demonstrating its enduring relevance and adaptability in tackling complex, non-linear real-world problems.</p>
</div>
</div>
</section>
<section id="introduction-to-the-sigmoid-activation-function" class="level3" data-number="1.2.11">
<h3 data-number="1.2.11" class="anchored" data-anchor-id="introduction-to-the-sigmoid-activation-function"><span class="header-section-number">1.2.11</span> Introduction to the Sigmoid Activation Function</h3>
<p>The <strong>sigmoid function</strong>, defined as</p>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1 + e^{-x}},
\]</span></p>
<p>maps any real-valued input to an output between 0 and 1, making it ideal for binary classification tasks. It has played a pivotal role in the development of neural networks, especially in overcoming the limitations of the <strong>step function</strong> used in early perceptrons.</p>
</section>
<section id="historical-context" class="level3" data-number="1.2.12">
<h3 data-number="1.2.12" class="anchored" data-anchor-id="historical-context"><span class="header-section-number">1.2.12</span> Historical Context</h3>
<p>In the 1950s, <strong>Frank Rosenblatt’s perceptron</strong> used the step function, which works well for linearly separable problems but fails with more complex datasets, like the XOR problem. The introduction of the <strong>sigmoid activation function</strong> in the 1980s addressed this by enabling smooth decision boundaries and facilitating the <strong>backpropagation</strong> algorithm, allowing neural networks to learn from data effectively.</p>
</section>
<section id="relevance-to-modern-neural-networks" class="level3" data-number="1.2.13">
<h3 data-number="1.2.13" class="anchored" data-anchor-id="relevance-to-modern-neural-networks"><span class="header-section-number">1.2.13</span> Relevance to Modern Neural Networks</h3>
<p>The sigmoid function’s differentiability makes it ideal for <strong>gradient-based optimization</strong>, which is essential for training deep neural networks. Its output is a <strong>probability</strong>, making it suitable for <strong>binary classification</strong> problems. Additionally, the derivative of the sigmoid is easy to compute:</p>
<p><span class="math display">\[
\sigma'(x) = \sigma(x)(1 - \sigma(x)),
\]</span></p>
<p>which aids in <strong>backpropagation</strong> by allowing efficient weight updates. Despite some limitations, such as the <strong>vanishing gradient problem</strong>, the sigmoid function is widely used in the output layers of networks for tasks requiring probabilistic outputs.</p>
</section>
<section id="applications" class="level3" data-number="1.2.14">
<h3 data-number="1.2.14" class="anchored" data-anchor-id="applications"><span class="header-section-number">1.2.14</span> Applications</h3>
<ul>
<li><strong>Binary classification</strong> (e.g., logistic regression).</li>
<li><strong>Output layer</strong> in neural networks for binary classification.</li>
<li><strong>Probabilistic models</strong> in machine learning and AI.</li>
</ul>
<p>While alternatives like <strong>ReLU</strong> are often used in deeper layers due to the vanishing gradient problem, sigmoid remains a powerful tool for <strong>probabilistic predictions</strong>.</p>
</section>
<section id="sigmoid-neuron" class="level3" data-number="1.2.15">
<h3 data-number="1.2.15" class="anchored" data-anchor-id="sigmoid-neuron"><span class="header-section-number">1.2.15</span> Sigmoid Neuron</h3>
<p>The sigmoid neuron replaces the step function with the sigmoid function: <span class="math display">\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]</span> This allows for smooth gradients, enabling the use of backpropagation for training MLPs.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<svg width="672" height="480" viewbox="0.00 0.00 640.98 279.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 275)">
<title>NeuralNetwork</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-275 636.98,-275 636.98,4 -4,4"></polygon>
<g id="clust2" class="cluster">
<title>cluster_weights</title>
<polygon fill="none" stroke="black" stroke-dasharray="1,5" points="109.21,-46 109.21,-263 171.97,-263 171.97,-46 109.21,-46"></polygon>
<text text-anchor="middle" x="140.59" y="-246.4" font-family="Times,serif" font-size="14.00">Weights</text>
</g>
<g id="clust3" class="cluster">
<title>cluster_neuron</title>
<polygon fill="none" stroke="black" stroke-dasharray="1,5" points="200.71,-83 200.71,-160 521.98,-160 521.98,-83 200.71,-83"></polygon>
<text text-anchor="middle" x="361.34" y="-143.4" font-family="Times,serif" font-size="14.00">Sigmoid Neuron</text>
</g>
<g id="clust4" class="cluster">
<title>cluster_output</title>
<polygon fill="none" stroke="black" stroke-dasharray="1,5" points="541.98,-83 541.98,-160 632.98,-160 632.98,-83 541.98,-83"></polygon>
<text text-anchor="middle" x="587.48" y="-143.4" font-family="Times,serif" font-size="14.00">Output Layer</text>
</g>
<g id="clust1" class="cluster">
<title>cluster_input</title>
<polygon fill="none" stroke="black" stroke-dasharray="1,5" points="0,-53 0,-256 81.47,-256 81.47,-53 0,-53"></polygon>
<text text-anchor="middle" x="40.74" y="-239.4" font-family="Times,serif" font-size="14.00">Input Layer</text>
</g>
<!-- X1 -->
<g id="node1" class="node">
<title>X1</title>
<ellipse fill="white" stroke="black" cx="40.24" cy="-202" rx="21.47" ry="21.47"></ellipse>
<text text-anchor="middle" x="40.24" y="-197.8" font-family="Helvetica,sans-Serif" font-size="14.00">x1</text>
</g>
<!-- W1 -->
<g id="node4" class="node">
<title>W1</title>
<ellipse fill="white" stroke="black" cx="140.59" cy="-207" rx="23.26" ry="23.26"></ellipse>
<text text-anchor="middle" x="140.59" y="-202.8" font-family="Helvetica,sans-Serif" font-size="14.00">w1</text>
</g>
<!-- X1&#45;&gt;W1 -->
<g id="edge1" class="edge">
<title>X1-&gt;W1</title>
<path fill="none" stroke="black" d="M61.61,-203.04C74.65,-203.7 91.86,-204.57 106.8,-205.33"></path>
<polygon fill="black" stroke="black" points="107.01,-208.85 117.18,-205.86 107.37,-201.86 107.01,-208.85"></polygon>
</g>
<!-- X2 -->
<g id="node2" class="node">
<title>X2</title>
<ellipse fill="white" stroke="black" cx="40.24" cy="-142" rx="21.47" ry="21.47"></ellipse>
<text text-anchor="middle" x="40.24" y="-137.8" font-family="Helvetica,sans-Serif" font-size="14.00">x2</text>
</g>
<!-- W2 -->
<g id="node5" class="node">
<title>W2</title>
<ellipse fill="white" stroke="black" cx="140.59" cy="-142" rx="23.26" ry="23.26"></ellipse>
<text text-anchor="middle" x="140.59" y="-137.8" font-family="Helvetica,sans-Serif" font-size="14.00">w2</text>
</g>
<!-- X2&#45;&gt;W2 -->
<g id="edge3" class="edge">
<title>X2-&gt;W2</title>
<path fill="none" stroke="black" d="M61.61,-142C74.65,-142 91.86,-142 106.8,-142"></path>
<polygon fill="black" stroke="black" points="107.18,-145.5 117.18,-142 107.18,-138.5 107.18,-145.5"></polygon>
</g>
<!-- Xn -->
<g id="node3" class="node">
<title>Xn</title>
<ellipse fill="white" stroke="black" cx="40.24" cy="-82" rx="21.47" ry="21.47"></ellipse>
<text text-anchor="middle" x="40.24" y="-77.8" font-family="Helvetica,sans-Serif" font-size="14.00">xn</text>
</g>
<!-- Wn -->
<g id="node6" class="node">
<title>Wn</title>
<ellipse fill="white" stroke="black" cx="140.59" cy="-77" rx="23.26" ry="23.26"></ellipse>
<text text-anchor="middle" x="140.59" y="-72.8" font-family="Helvetica,sans-Serif" font-size="14.00">wn</text>
</g>
<!-- Xn&#45;&gt;Wn -->
<g id="edge5" class="edge">
<title>Xn-&gt;Wn</title>
<path fill="none" stroke="black" d="M61.61,-80.96C74.65,-80.3 91.86,-79.43 106.8,-78.67"></path>
<polygon fill="black" stroke="black" points="107.37,-82.14 117.18,-78.14 107.01,-75.15 107.37,-82.14"></polygon>
</g>
<!-- SUM -->
<g id="node7" class="node">
<title>SUM</title>
<polygon fill="white" stroke="black" points="325.91,-127 208.64,-127 208.64,-91 325.91,-91 325.91,-127"></polygon>
<text text-anchor="middle" x="267.27" y="-104.8" font-family="Helvetica,sans-Serif" font-size="14.00">Σ = Σ(wi * xi) + b</text>
</g>
<!-- W1&#45;&gt;SUM -->
<g id="edge2" class="edge">
<title>W1-&gt;SUM</title>
<path fill="none" stroke="black" d="M159.45,-192.96C179.21,-177.43 211.4,-152.13 235.29,-133.35"></path>
<polygon fill="black" stroke="black" points="237.66,-135.94 243.36,-127.01 233.33,-130.44 237.66,-135.94"></polygon>
</g>
<!-- W2&#45;&gt;SUM -->
<g id="edge4" class="edge">
<title>W2-&gt;SUM</title>
<path fill="none" stroke="black" d="M163.31,-136.25C173.37,-133.59 185.94,-130.26 198.79,-126.86"></path>
<polygon fill="black" stroke="black" points="199.88,-130.19 208.65,-124.25 198.09,-123.43 199.88,-130.19"></polygon>
</g>
<!-- Wn&#45;&gt;SUM -->
<g id="edge6" class="edge">
<title>Wn-&gt;SUM</title>
<path fill="none" stroke="black" d="M163.31,-82.57C173.37,-85.16 185.94,-88.38 198.79,-91.68"></path>
<polygon fill="black" stroke="black" points="198.09,-95.11 208.65,-94.21 199.83,-88.33 198.09,-95.11"></polygon>
</g>
<!-- SIGMOID -->
<g id="node8" class="node">
<title>SIGMOID</title>
<polygon fill="white" stroke="black" points="514.05,-127 361.77,-127 361.77,-91 514.05,-91 514.05,-127"></polygon>
<text text-anchor="middle" x="437.91" y="-104.8" font-family="Helvetica,sans-Serif" font-size="14.00">σ(z) = 1 / (1 + exp(-z))</text>
</g>
<!-- SUM&#45;&gt;SIGMOID -->
<g id="edge8" class="edge">
<title>SUM-&gt;SIGMOID</title>
<path fill="none" stroke="black" d="M326.31,-109C334.53,-109 343.12,-109 351.71,-109"></path>
<polygon fill="black" stroke="black" points="351.79,-112.5 361.79,-109 351.79,-105.5 351.79,-112.5"></polygon>
</g>
<!-- Y -->
<g id="node9" class="node">
<title>Y</title>
<ellipse fill="white" stroke="black" cx="586.98" cy="-109" rx="18" ry="18"></ellipse>
<text text-anchor="middle" x="586.98" y="-104.8" font-family="Helvetica,sans-Serif" font-size="14.00">y</text>
</g>
<!-- SIGMOID&#45;&gt;Y -->
<g id="edge9" class="edge">
<title>SIGMOID-&gt;Y</title>
<path fill="none" stroke="black" d="M514.17,-109C530.02,-109 545.77,-109 558.46,-109"></path>
<polygon fill="black" stroke="black" points="558.75,-112.5 568.75,-109 558.75,-105.5 558.75,-112.5"></polygon>
</g>
<!-- B -->
<g id="node10" class="node">
<title>B</title>
<polygon fill="none" stroke="black" stroke-dasharray="5,2" points="172.82,-36 108.35,-36 108.35,0 172.82,0 172.82,-36"></polygon>
<text text-anchor="middle" x="140.59" y="-13.8" font-family="Helvetica,sans-Serif" font-size="14.00">b (Bias)</text>
</g>
<!-- B&#45;&gt;SUM -->
<g id="edge7" class="edge">
<title>B-&gt;SUM</title>
<path fill="none" stroke="black" d="M166.52,-36.2C185.63,-50.15 212.19,-69.53 233.1,-84.79"></path>
<polygon fill="black" stroke="black" points="231.24,-87.77 241.39,-90.84 235.37,-82.11 231.24,-87.77"></polygon>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Following python code demonstrate the effective use of the sigmoid activation function in the simulation of logic gates.</p>
<div id="d7e10c25" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceptron:</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, learning_rate<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_dim <span class="op">=</span> input_dim</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights including bias term (input_dim + 1)</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.random.randn(input_dim <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.01</span>  <span class="co"># +1 for bias</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sigmoid(<span class="va">self</span>, x):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">        Sigmoid activation function.</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sigmoid_derivative(<span class="va">self</span>, x):</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Derivative of sigmoid function.</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> x)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> activation(<span class="va">self</span>, net_input):</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co">        Apply sigmoid activation function to net input.</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.sigmoid(net_input)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="co">        Predict the output for given input data using the perceptron.</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> X.ndim <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>            X <span class="op">=</span> X.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># Reshape if it's a single sample</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add bias term for prediction</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>        X_bias <span class="op">=</span> np.c_[X, np.ones(X.shape[<span class="dv">0</span>])]</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>        net_input <span class="op">=</span> np.dot(X_bias, <span class="va">self</span>.weights)  <span class="co"># Calculate net input</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.activation(net_input)  <span class="co"># Apply sigmoid activation</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply threshold: if output &gt; 0.5, interpret as 1; else 0</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.where(output <span class="op">&gt;</span> <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, X, y, epochs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a><span class="co">        Train the perceptron using the provided data.</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> xi, yi <span class="kw">in</span> <span class="bu">zip</span>(X, y):</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>                xi_bias <span class="op">=</span> np.append(xi, <span class="dv">1</span>)  <span class="co"># Add bias term to input</span></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>                net_input <span class="op">=</span> np.dot(xi_bias, <span class="va">self</span>.weights)  <span class="co"># Calculate net input</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>                prediction <span class="op">=</span> <span class="va">self</span>.activation(net_input)  <span class="co"># Get the prediction</span></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>                error <span class="op">=</span> yi <span class="op">-</span> prediction  <span class="co"># Calculate error</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Update weights using the gradient of the sigmoid function</span></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.weights <span class="op">+=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> error <span class="op">*</span> prediction <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> prediction) <span class="op">*</span> xi_bias</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Non-member function to simulate logic gates</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simulate_logic_gate(gate_type):</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a><span class="co">    Simulate a logic gate using the Perceptron.</span></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define input and target outputs for different gates</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>    gate_targets <span class="op">=</span> {</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>        <span class="st">'AND'</span>: np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]),</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>        <span class="st">'OR'</span>: np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]),</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>        <span class="st">'NAND'</span>: np.array([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>]),</span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>        <span class="st">'NOR'</span>: np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]),</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>        <span class="st">'XOR'</span>: np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> gate_type <span class="kw">not</span> <span class="kw">in</span> gate_targets:</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Unsupported gate type: </span><span class="sc">{</span>gate_type<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> gate_targets[gate_type]</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize perceptron</span></span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>    perceptron <span class="op">=</span> Perceptron(input_dim<span class="op">=</span><span class="dv">2</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train perceptron</span></span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>    perceptron.train(X, y, epochs<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Test predictions</span></span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Simulating </span><span class="sc">{</span>gate_type<span class="sc">}</span><span class="ss"> gate"</span>)</span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Trained weights:"</span>, perceptron.weights[:<span class="op">-</span><span class="dv">1</span>])  <span class="co"># Exclude bias term from display</span></span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Trained bias:"</span>, perceptron.weights[<span class="op">-</span><span class="dv">1</span>])  <span class="co"># Display bias term separately</span></span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">0</span>]):  <span class="co"># Iterate over the rows of X (4 samples)</span></span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a>        input_data <span class="op">=</span> X[i]  <span class="co"># Extract the i-th row as a 1D array</span></span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> perceptron.predict(input_data)  <span class="co"># Make prediction</span></span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Input: </span><span class="sc">{</span>input_data<span class="sc">}</span><span class="ss">, Prediction: </span><span class="sc">{</span>prediction<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Simulation code is here:</p>
<div id="684440fd" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> gate <span class="kw">in</span> [<span class="st">'AND'</span>, <span class="st">'OR'</span>, <span class="st">'NAND'</span>, <span class="st">'NOR'</span>, <span class="st">'XOR'</span>]:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    simulate_logic_gate(gate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Simulating AND gate
Trained weights: [2.62591605 2.62222932]
Trained bias: -4.054951257335541
Input: [0 0], Prediction: [0]
Input: [0 1], Prediction: [0]
Input: [1 0], Prediction: [0]
Input: [1 1], Prediction: [1]
Simulating OR gate
Trained weights: [3.3080444  3.30966025]
Trained bias: -1.3436916260629446
Input: [0 0], Prediction: [0]
Input: [0 1], Prediction: [1]
Input: [1 0], Prediction: [1]
Input: [1 1], Prediction: [1]
Simulating NAND gate
Trained weights: [-2.62329787 -2.61964744]
Trained bias: 4.051108047248814
Input: [0 0], Prediction: [1]
Input: [0 1], Prediction: [1]
Input: [1 0], Prediction: [1]
Input: [1 1], Prediction: [0]
Simulating NOR gate
Trained weights: [-3.30968679 -3.31152847]
Trained bias: 1.3446534385164841
Input: [0 0], Prediction: [1]
Input: [0 1], Prediction: [0]
Input: [1 0], Prediction: [0]
Input: [1 1], Prediction: [0]
Simulating XOR gate
Trained weights: [-0.02476972 -0.01218682]
Trained bias: 0.012113375259234867
Input: [0 0], Prediction: [1]
Input: [0 1], Prediction: [0]
Input: [1 0], Prediction: [0]
Input: [1 1], Prediction: [0]</code></pre>
</div>
</div>
</section>
</section>
<section id="important-theorems-and-results" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="important-theorems-and-results"><span class="header-section-number">1.3</span> Important Theorems and Results</h2>
<ol type="1">
<li><strong>Perceptron Convergence Theorem</strong>: If the data is linearly separable, the perceptron will converge to a solution in a finite number of steps.</li>
<li><strong>Universal Approximation Theorem</strong>: An MLP with a single hidden layer and non-linear activation functions can approximate any continuous function.</li>
</ol>
<hr>
</section>
<section id="examples" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="examples"><span class="header-section-number">1.4</span> Examples</h2>
<section id="simple-example-perceptron-for-and-gate-with-native-python-code" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="simple-example-perceptron-for-and-gate-with-native-python-code"><span class="header-section-number">1.4.1</span> Simple Example: Perceptron for AND Gate with native <code>Python</code> code</h3>
<div id="4bdad4b7" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Inputs (x1, x2) and outputs (y)</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>])  <span class="co"># AND Gate Output</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize weights and bias</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.random.rand(<span class="dv">2</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> np.random.rand(<span class="dv">1</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Activation function</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step_function(z):</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="cf">if</span> z <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):  <span class="co"># 10 epochs</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(X)):</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> np.dot(X[i], weights) <span class="op">+</span> bias</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> step_function(z)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> y[i] <span class="op">-</span> y_pred</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">+=</span> learning_rate <span class="op">*</span> error <span class="op">*</span> X[i]</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>        bias <span class="op">+=</span> learning_rate <span class="op">*</span> error</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Trained Weights: </span><span class="sc">{</span>weights<span class="sc">}</span><span class="ss">, Bias: </span><span class="sc">{</span>bias<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Trained Weights: [0.15624762 0.09191255], Bias: [-0.21553744]</code></pre>
</div>
</div>
</section>
</section>
<section id="the-perceptron-and-the-xor-problem" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="the-perceptron-and-the-xor-problem"><span class="header-section-number">1.5</span> The Perceptron and the XOR Problem</h2>
<p>In the early stages of neural network research, the <strong>perceptron</strong> was introduced as a simple model to classify linearly separable patterns. A perceptron consists of a single neuron, which receives input, applies weights, sums them up, and passes the result through an activation function to produce an output. For problems that can be linearly separated, a perceptron performs exceptionally well.</p>
<p>However, the perceptron faced a significant limitation: it cannot solve problems that are not linearly separable. A classic example is the <strong>XOR (exclusive OR) gate</strong>. The XOR gate outputs <code>1</code> only when the inputs are different (i.e., for inputs <code>(0,1)</code> or <code>(1,0)</code>), and outputs <code>0</code> when the inputs are the same (i.e., for inputs <code>(0,0)</code> or <code>(1,1)</code>). This pattern is not linearly separable, meaning it cannot be solved by a single perceptron, as the data cannot be separated with a straight line.</p>
<section id="why-the-perceptron-fails-at-xor" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="why-the-perceptron-fails-at-xor"><span class="header-section-number">1.5.1</span> Why the Perceptron Fails at XOR</h3>
<p>For the XOR gate, the following input-output pairs exist:</p>
<ul>
<li>(0, 0) → 0</li>
<li>(0, 1) → 1</li>
<li>(1, 0) → 1</li>
<li>(1, 1) → 0</li>
</ul>
<p>If you try to plot these points on a 2D plane, you’ll notice that no straight line can separate the <code>1</code> outputs from the <code>0</code> outputs, making the XOR problem an example of a <strong>non-linearly separable</strong> problem. This is where the perceptron fails.</p>
</section>
</section>
<section id="multi-layer-perceptron-mlp" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="multi-layer-perceptron-mlp"><span class="header-section-number">1.6</span> Multi-Layer Perceptron (MLP)</h2>
<p>To overcome the limitations of the perceptron, we need a more powerful model that can learn non-linear decision boundaries. The <strong>Multi-Layer Perceptron (MLP)</strong> addresses this limitation by introducing multiple layers of neurons. Unlike a single perceptron, an MLP has:</p>
<ol type="1">
<li><strong>Input Layer</strong>: Receives the input features (in this case, the two binary inputs of the XOR gate).</li>
<li><strong>Hidden Layers</strong>: One or more layers of neurons that allow the network to learn complex, non-linear mappings from input to output.</li>
<li><strong>Output Layer</strong>: Produces the final prediction (in this case, the XOR output).</li>
</ol>
<p>The addition of hidden layers and the use of non-linear activation functions (such as <strong>sigmoid</strong> or <strong>ReLU</strong>) enables the MLP to learn and model non-linear relationships. This makes MLPs capable of solving the XOR problem, as the network can form non-linear decision boundaries.</p>
</section>
<section id="the-need-for-an-appropriate-learning-algorithm-in-multi-layer-networks" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="the-need-for-an-appropriate-learning-algorithm-in-multi-layer-networks"><span class="header-section-number">1.7</span> The Need for an Appropriate Learning Algorithm in Multi-Layer Networks</h2>
<p>In machine learning, training a multi-layer neural network (also known as a <strong>deep neural network</strong>) presents a significant challenge compared to simpler models like the Perceptron. This complexity arises from the structure of the network and the way information flows through multiple layers. The primary challenge is that the weight updates at each layer must be carefully adjusted to ensure the network learns effectively.</p>
<section id="challenge-of-multi-layer-networks" class="level3" data-number="1.7.1">
<h3 data-number="1.7.1" class="anchored" data-anchor-id="challenge-of-multi-layer-networks"><span class="header-section-number">1.7.1</span> Challenge of Multi-Layer Networks</h3>
<p>A <strong>multi-layer neural network</strong> consists of several layers:</p>
<ul>
<li><strong>Input Layer</strong>: Receives the raw data.</li>
<li><strong>Hidden Layers</strong>: Transform the input into meaningful representations.</li>
<li><strong>Output Layer</strong>: Produces the final predictions or classifications.</li>
</ul>
<p>As we move deeper into the network, each layer influences the output indirectly through many intermediate transformations. This creates a situation where updating the weights at deeper layers is not straightforward because the error at the output layer is spread across multiple layers.</p>
</section>
<section id="why-a-learning-algorithm-is-needed" class="level3" data-number="1.7.2">
<h3 data-number="1.7.2" class="anchored" data-anchor-id="why-a-learning-algorithm-is-needed"><span class="header-section-number">1.7.2</span> Why a Learning Algorithm is Needed?</h3>
<p>For multi-layer networks, we require a <strong>learning algorithm</strong> to adjust the weights effectively. In simpler models like the Perceptron, weights are updated based on the error between the predicted and actual outputs. However, for multi-layer networks, the error must be propagated back through the layers to determine how much each weight in each layer contributed to the final error.</p>
<p>Without a structured learning approach, the network would fail to update its weights in a coherent way, preventing it from learning the underlying patterns in the data. This is where <strong>Backpropagation</strong> comes into play.</p>
</section>
<section id="backpropagation-the-core-learning-algorithm" class="level3" data-number="1.7.3">
<h3 data-number="1.7.3" class="anchored" data-anchor-id="backpropagation-the-core-learning-algorithm"><span class="header-section-number">1.7.3</span> Backpropagation: The Core Learning Algorithm</h3>
<p><strong>Backpropagation</strong> is the most widely used algorithm for training multi-layer networks. It is based on the principle of <strong>gradient descent</strong>, a method used to minimize the loss function by adjusting the weights. Here’s how backpropagation works:</p>
<ul>
<li><strong>Forward Pass</strong>: The input data is passed through the network, and activations for each layer are computed.</li>
<li><strong>Loss Calculation</strong>: The error between the predicted and actual output is computed using a loss function.</li>
<li><strong>Backward Pass</strong>: The error is propagated back through the network, and gradients of the loss function with respect to each weight are computed.</li>
<li><strong>Weight Update</strong>: The weights are updated using the computed gradients.</li>
</ul>
<section id="key-features-of-backpropagation" class="level4" data-number="1.7.3.1">
<h4 data-number="1.7.3.1" class="anchored" data-anchor-id="key-features-of-backpropagation"><span class="header-section-number">1.7.3.1</span> Key Features of Backpropagation:</h4>
<ul>
<li><strong>Error Propagation</strong>: Backpropagation calculates how much each weight in the network contributed to the error at the output, and this error is propagated backward through the layers.</li>
<li><strong>Gradient Calculation</strong>: By using the chain rule of calculus, backpropagation calculates the gradients of the loss function with respect to each weight in the network.</li>
<li><strong>Weight Updates</strong>: These gradients are then used in gradient descent to adjust the weights in such a way that the loss function is minimized.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Gradient Descent: Not Enough by Itself
</div>
</div>
<div class="callout-body-container callout-body">
<p>While <strong>gradient descent</strong> is a general optimization algorithm, it cannot directly address the challenge of updating weights in deep neural networks. In shallow networks (with fewer layers), gradient descent can work directly on the loss function. However, in deep networks, the error needs to be distributed back through the multiple layers. <strong>Backpropagation</strong> is the algorithm that enables this efficient error propagation, making the use of gradient descent feasible for deep networks.</p>
</div>
</div>
</section>
</section>
</section>
<section id="mathematical-background-of-backpropagation" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="mathematical-background-of-backpropagation"><span class="header-section-number">1.8</span> Mathematical Background of Backpropagation</h2>
<p>Backpropagation is the core learning algorithm for training multi-layer neural networks. It involves adjusting the weights of the network by computing the gradients of a loss function with respect to each weight. This process allows the network to learn how to minimize the loss and improve its predictions.</p>
<section id="loss-function" class="level3" data-number="1.8.1">
<h3 data-number="1.8.1" class="anchored" data-anchor-id="loss-function"><span class="header-section-number">1.8.1</span> Loss Function</h3>
<p>The <strong>loss function</strong> quantifies the difference between the network’s predictions and the true labels. For a simple regression task, the loss function is often the <strong>mean squared error (MSE)</strong>:</p>
<p><span class="math display">\[
L = \frac{1}{2} (y - \hat{y})^2
\]</span></p>
<p>where: - <span class="math inline">\(y\)</span> is the true label, - <span class="math inline">\(\hat{y}\)</span> is the predicted output of the network.</p>
<p>The factor <span class="math inline">\(\frac{1}{2}\)</span> is included to simplify the calculation of the derivative later.</p>
</section>
<section id="forward-pass" class="level3" data-number="1.8.2">
<h3 data-number="1.8.2" class="anchored" data-anchor-id="forward-pass"><span class="header-section-number">1.8.2</span> Forward Pass</h3>
<p>During the <strong>forward pass</strong>, the input <span class="math inline">\(x\)</span> is passed through each layer of the network. Each layer computes a weighted sum of its inputs followed by a non-linear activation function <span class="math inline">\(\sigma\)</span>. The output of a neuron in layer <span class="math inline">\(l\)</span> is calculated as:</p>
<p><span class="math display">\[
a_i^l = \sigma \left( \sum_j w_{ij}^{l-1} a_j^{l-1} + b_i^l \right)
\]</span></p>
<p>where: - <span class="math inline">\(w_{ij}^{l-1}\)</span> is the weight connecting neuron <span class="math inline">\(j\)</span> in layer <span class="math inline">\(l-1\)</span> to neuron <span class="math inline">\(i\)</span> in layer <span class="math inline">\(l\)</span>, - <span class="math inline">\(a_j^{l-1}\)</span> is the activation of neuron <span class="math inline">\(j\)</span> in the previous layer, - <span class="math inline">\(b_i^l\)</span> is the bias of neuron <span class="math inline">\(i\)</span> in layer <span class="math inline">\(l\)</span>, - <span class="math inline">\(\sigma\)</span> is the activation function (e.g., sigmoid, ReLU, etc.).</p>
<p>The output of the network is:</p>
<p><span class="math display">\[
\hat{y} = a^L
\]</span></p>
<p>where <span class="math inline">\(L\)</span> is the last layer.</p>
</section>
<section id="backward-pass-computing-gradients" class="level3" data-number="1.8.3">
<h3 data-number="1.8.3" class="anchored" data-anchor-id="backward-pass-computing-gradients"><span class="header-section-number">1.8.3</span> Backward Pass: Computing Gradients</h3>
<p>The main objective of backpropagation is to compute the <strong>gradients</strong> of the loss function with respect to the weights and biases of the network. This is achieved through the <strong>chain rule of calculus</strong>, which allows the error to be propagated backward through the network.</p>
<section id="gradient-of-the-loss-with-respect-to-output-layer" class="level4" data-number="1.8.3.1">
<h4 data-number="1.8.3.1" class="anchored" data-anchor-id="gradient-of-the-loss-with-respect-to-output-layer"><span class="header-section-number">1.8.3.1</span> Gradient of the Loss with respect to Output Layer</h4>
<p>The first step is to calculate the gradient of the loss function with respect to the predicted output <span class="math inline">\(\hat{y}\)</span>:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial \hat{y}} = \hat{y} - y
\]</span></p>
<p>For the output layer <span class="math inline">\(L\)</span>, the gradient of the loss with respect to the activation <span class="math inline">\(a_i^L\)</span> is:</p>
<p><span class="math display">\[
\delta_i^L = \frac{\partial L}{\partial a_i^L} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a_i^L}
\]</span></p>
<p>For a typical activation function <span class="math inline">\(\sigma\)</span>, we have:</p>
<p><span class="math display">\[
\frac{\partial \hat{y}}{\partial a_i^L} = \sigma'(a_i^L)
\]</span></p>
<p>where <span class="math inline">\(\sigma'(a_i^L)\)</span> is the derivative of the activation function at <span class="math inline">\(a_i^L\)</span>.</p>
<p>Thus, the error term for the output layer is:</p>
<p><span class="math display">\[
\delta_i^L = (\hat{y} - y) \cdot \sigma'(a_i^L)
\]</span></p>
</section>
<section id="gradient-for-hidden-layers" class="level4" data-number="1.8.3.2">
<h4 data-number="1.8.3.2" class="anchored" data-anchor-id="gradient-for-hidden-layers"><span class="header-section-number">1.8.3.2</span> Gradient for Hidden Layers</h4>
<p>For each hidden layer <span class="math inline">\(l\)</span>, the error term for neuron <span class="math inline">\(i\)</span> in layer <span class="math inline">\(l\)</span> is:</p>
<p><span class="math display">\[
\delta_i^l = \left( \sum_j w_{ij}^l \delta_j^{l+1} \right) \cdot \sigma'(a_i^l)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\delta_j^{l+1}\)</span> is the error term for neuron <span class="math inline">\(j\)</span> in the next layer <span class="math inline">\(l+1\)</span>,</li>
<li><span class="math inline">\(w_{ij}^l\)</span> is the weight connecting neuron <span class="math inline">\(j\)</span> in layer <span class="math inline">\(l+1\)</span> to neuron <span class="math inline">\(i\)</span> in layer <span class="math inline">\(l\)</span>,</li>
<li><span class="math inline">\(\sigma'(a_i^l)\)</span> is the derivative of the activation function for neuron <span class="math inline">\(i\)</span> in layer <span class="math inline">\(l\)</span>.</li>
</ul>
</section>
<section id="gradients-of-the-weights-and-biases" class="level4" data-number="1.8.3.3">
<h4 data-number="1.8.3.3" class="anchored" data-anchor-id="gradients-of-the-weights-and-biases"><span class="header-section-number">1.8.3.3</span> Gradients of the Weights and Biases</h4>
<p>After calculating the error terms, we compute the gradients of the weights and biases. The gradient of the loss with respect to the weight <span class="math inline">\(w_{ij}^l\)</span> is:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial w_{ij}^l} = a_j^{l-1} \cdot \delta_i^l
\]</span></p>
<p>And the gradient of the loss with respect to the bias <span class="math inline">\(b_i^l\)</span> is:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial b_i^l} = \delta_i^l
\]</span></p>
</section>
</section>
<section id="weight-update-rule-gradient-descent" class="level3" data-number="1.8.4">
<h3 data-number="1.8.4" class="anchored" data-anchor-id="weight-update-rule-gradient-descent"><span class="header-section-number">1.8.4</span> Weight Update Rule (Gradient Descent)</h3>
<p>Once the gradients are computed, we update the weights and biases using <strong>gradient descent</strong>. The update rule for the weights is:</p>
<p><span class="math display">\[
w_{ij}^l \leftarrow w_{ij}^l - \eta \cdot \frac{\partial L}{\partial w_{ij}^l}
\]</span></p>
<p>And for the biases:</p>
<p><span class="math display">\[
b_i^l \leftarrow b_i^l - \eta \cdot \frac{\partial L}{\partial b_i^l}
\]</span></p>
<p>where <span class="math inline">\(\eta\)</span> is the <strong>learning rate</strong>, controlling the step size for weight updates.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Summary of Backpropagation
</div>
</div>
<div class="callout-body-container callout-body">
<p>The backpropagation algorithm involves the following steps:</p>
<ol type="1">
<li><p><strong>Forward pass</strong>: Compute the activations for each layer.</p></li>
<li><p><strong>Compute loss</strong>: Calculate the loss function based on the predicted and true values.</p></li>
<li><p><strong>Backward pass</strong>:</p>
<ul>
<li>Compute error terms for the output layer.</li>
<li>Propagate error backward through the network using the chain rule.</li>
</ul></li>
<li><p><strong>Compute gradients</strong>: Compute the gradients of the loss with respect to weights and biases.</p></li>
<li><p><strong>Update weights</strong>: Use gradient descent to adjust the weights and biases.</p></li>
</ol>
</div>
</div>
</section>
<section id="why-mlp-can-solve-xor" class="level3" data-number="1.8.5">
<h3 data-number="1.8.5" class="anchored" data-anchor-id="why-mlp-can-solve-xor"><span class="header-section-number">1.8.5</span> Why MLP Can Solve XOR?</h3>
<p>The MLP can learn the XOR function by introducing <strong>hidden layers</strong> that allow it to combine the inputs in non-linear ways. A network with just one hidden layer is sufficient to approximate the XOR gate. The hidden layer transforms the inputs into a higher-dimensional space where the problem becomes linearly separable, allowing the output layer to make correct predictions.</p>
</section>
</section>
<section id="implementing-xor-with-mlp" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="implementing-xor-with-mlp"><span class="header-section-number">1.9</span> Implementing XOR with MLP</h2>
<p>With the MLP’s ability to model non-linearly separable data, we can now implement the XOR gate. The MLP will consist of:</p>
<ul>
<li>An <strong>input layer</strong> that takes in two binary values.</li>
<li>A <strong>hidden layer</strong> that helps in learning the non-linear relationship.</li>
<li>An <strong>output layer</strong> that produces the binary XOR result.</li>
</ul>
<p>We will train the MLP on the four possible inputs for the XOR gate and use backpropagation to update the weights in the network, eventually enabling the MLP to predict the XOR output correctly.</p>
<p>In the next code implementation, we will implement the XOR gate using an MLP, demonstrating how the network can learn the XOR function through training.</p>
<div id="bde79100" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceptron:</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, output_dim, activation<span class="op">=</span><span class="st">"sigmoid"</span>):</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_dim <span class="op">=</span> input_dim</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_dim <span class="op">=</span> output_dim</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> activation</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.random.randn(input_dim, output_dim)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.biases <span class="op">=</span> np.zeros((<span class="dv">1</span>, output_dim))</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs  <span class="co"># Store inputs for later use in backpropagation</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> np.dot(inputs, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.biases</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.activation <span class="op">==</span> <span class="st">"sigmoid"</span>:</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))  <span class="co"># Store output for backpropagation</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.output</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.activation <span class="op">==</span> <span class="st">"relu"</span>:</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output <span class="op">=</span> np.maximum(<span class="dv">0</span>, z)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.output</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:  <span class="co"># Linear activation by default</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output <span class="op">=</span> z</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.output</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, output_error, learning_rate):</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.activation <span class="op">==</span> <span class="st">"sigmoid"</span>:</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>            activation_derivative <span class="op">=</span> <span class="va">self</span>.output <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.output)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.activation <span class="op">==</span> <span class="st">"relu"</span>:</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>            activation_derivative <span class="op">=</span> np.where(<span class="va">self</span>.output <span class="op">&gt;</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:  <span class="co"># Linear activation</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>            activation_derivative <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        delta <span class="op">=</span> output_error <span class="op">*</span> activation_derivative</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>        input_error <span class="op">=</span> np.dot(delta, <span class="va">self</span>.weights.T)</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update weights and biases</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">+=</span> learning_rate <span class="op">*</span> np.dot(<span class="va">self</span>.inputs.T, delta)</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.biases <span class="op">+=</span> learning_rate <span class="op">*</span> np.<span class="bu">sum</span>(delta, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> input_error</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP:</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, hidden_dim, output_dim, learning_rate<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> [</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>            Perceptron(input_dim, hidden_dim, activation<span class="op">=</span><span class="st">"sigmoid"</span>),</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>            Perceptron(hidden_dim, output_dim, activation<span class="op">=</span><span class="st">"sigmoid"</span>)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> X</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> layer.forward(output)</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, X, y, epochs<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>.forward(X)</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Backward pass (backpropagation)</span></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>            error <span class="op">=</span> y <span class="op">-</span> output</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">reversed</span>(<span class="va">self</span>.layers):</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>                error <span class="op">=</span> layer.backward(error, <span class="va">self</span>.learning_rate)</span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> np.mean(np.square(y <span class="op">-</span> <span class="va">self</span>.forward(X)))</span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.<span class="bu">round</span>(<span class="va">self</span>.forward(X))</span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simulate_logic_gate(gate_type):</span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a>    gate_targets <span class="op">=</span> {</span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a>        <span class="st">'XOR'</span>: np.array([[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">0</span>]])  <span class="co"># XOR output</span></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> gate_type <span class="kw">not</span> <span class="kw">in</span> gate_targets:</span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Unsupported gate type: </span><span class="sc">{</span>gate_type<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> gate_targets[gate_type]</span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a>    mlp <span class="op">=</span> MLP(input_dim<span class="op">=</span><span class="dv">2</span>, hidden_dim<span class="op">=</span><span class="dv">2</span>, output_dim<span class="op">=</span><span class="dv">1</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a>    mlp.train(X, y, epochs<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Simulating </span><span class="sc">{</span>gate_type<span class="sc">}</span><span class="ss"> gate"</span>)</span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> input_data, prediction <span class="kw">in</span> <span class="bu">zip</span>(X, mlp.predict(X)):</span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Input: </span><span class="sc">{</span>input_data<span class="sc">}</span><span class="ss">, Prediction: </span><span class="sc">{</span>prediction<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a>simulate_logic_gate(<span class="st">'XOR'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, Loss: 0.255295280231149
Epoch 1000, Loss: 0.22686866220906776
Epoch 2000, Loss: 0.12394747722959651
Epoch 3000, Loss: 0.023914693380518173
Epoch 4000, Loss: 0.010461269736354674
Epoch 5000, Loss: 0.006385752812267632
Epoch 6000, Loss: 0.004514959615043102
Epoch 7000, Loss: 0.0034617110935167665
Epoch 8000, Loss: 0.00279294282209194
Epoch 9000, Loss: 0.0023333487860607554
Simulating XOR gate
Input: [0 0], Prediction: [0.]
Input: [0 1], Prediction: [1.]
Input: [1 0], Prediction: [1.]
Input: [1 1], Prediction: [0.]</code></pre>
</div>
</div>
<p>Above <code>Python</code> code creates a successful MLP with two (fixed) hidden layers that simulate the <code>XOR</code> gate. There is no golden rule to fix number of hidden layers in an MLP to solve a specific problem. So we have to restructure the OOPs architecture of MLP to handle its structure in more robust way. In this approach, the user can ‘add’ as many layers as he wish to develop a required MLP architecture. Following code will do this job.</p>
<div id="646227ad" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Layer:</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, output_dim, activation<span class="op">=</span><span class="st">"sigmoid"</span>):</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_dim <span class="op">=</span> output_dim</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> activation</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> <span class="va">None</span>  <span class="co"># Will be initialized later</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.biases <span class="op">=</span> <span class="va">None</span>    <span class="co"># Will be initialized later</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inputs <span class="op">=</span> <span class="va">None</span>   <span class="co"># Will store inputs for backpropagation</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs  <span class="co"># Store inputs for backpropagation</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> np.dot(inputs, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.biases</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.activation <span class="op">==</span> <span class="st">"sigmoid"</span>:</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))  <span class="co"># Store output for backpropagation</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.output</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.activation <span class="op">==</span> <span class="st">"relu"</span>:</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output <span class="op">=</span> np.maximum(<span class="dv">0</span>, z)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.output</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:  <span class="co"># Linear activation by default</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output <span class="op">=</span> z</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.output</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, output_error, learning_rate):</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.activation <span class="op">==</span> <span class="st">"sigmoid"</span>:</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>            activation_derivative <span class="op">=</span> <span class="va">self</span>.output <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.output)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.activation <span class="op">==</span> <span class="st">"relu"</span>:</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>            activation_derivative <span class="op">=</span> np.where(<span class="va">self</span>.output <span class="op">&gt;</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:  <span class="co"># Linear activation</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>            activation_derivative <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>        delta <span class="op">=</span> output_error <span class="op">*</span> activation_derivative</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>        input_error <span class="op">=</span> np.dot(delta, <span class="va">self</span>.weights.T)</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update weights and biases</span></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">+=</span> learning_rate <span class="op">*</span> np.dot(<span class="va">self</span>.inputs.T, delta)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.biases <span class="op">+=</span> learning_rate <span class="op">*</span> np.<span class="bu">sum</span>(delta, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> input_error</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP:</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, learning_rate<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_dim <span class="op">=</span> input_dim</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> []</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> add(<span class="va">self</span>, layer):</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers.append(layer)</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="bu">compile</span>(<span class="va">self</span>):</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights and biases for each layer</span></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>        prev_dim <span class="op">=</span> <span class="va">self</span>.input_dim</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>            layer.weights <span class="op">=</span> np.random.randn(prev_dim, layer.output_dim)</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>            layer.biases <span class="op">=</span> np.zeros((<span class="dv">1</span>, layer.output_dim))</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>            prev_dim <span class="op">=</span> layer.output_dim</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> X</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> layer.forward(output)</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, X, y, epochs<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass</span></span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>.forward(X)</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Backward pass (backpropagation)</span></span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>            error <span class="op">=</span> y <span class="op">-</span> output</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">reversed</span>(<span class="va">self</span>.layers):</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>                error <span class="op">=</span> layer.backward(error, <span class="va">self</span>.learning_rate)</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> np.mean(np.square(y <span class="op">-</span> <span class="va">self</span>.forward(X)))</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.<span class="bu">round</span>(<span class="va">self</span>.forward(X))</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simulate_logic_gate(gate_type):</span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>    gate_targets <span class="op">=</span> {</span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a>        <span class="st">'AND'</span>: np.array([[<span class="dv">0</span>], [<span class="dv">0</span>], [<span class="dv">0</span>], [<span class="dv">1</span>]]),</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a>        <span class="st">'OR'</span>: np.array([[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">1</span>]]),</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a>        <span class="st">'NAND'</span>: np.array([[<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">0</span>]]),</span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a>        <span class="st">'NOR'</span>: np.array([[<span class="dv">1</span>], [<span class="dv">0</span>], [<span class="dv">0</span>], [<span class="dv">0</span>]]),</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>        <span class="st">'XOR'</span>: np.array([[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">0</span>]])</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> gate_type <span class="kw">not</span> <span class="kw">in</span> gate_targets:</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Unsupported gate type: </span><span class="sc">{</span>gate_type<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> gate_targets[gate_type]</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a>    mlp <span class="op">=</span> MLP(input_dim<span class="op">=</span><span class="dv">2</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a>    mlp.add(Layer(output_dim<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>))  <span class="co"># Hidden layer with ReLU</span></span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a>    mlp.add(Layer(output_dim<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>))</span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a>    mlp.add(Layer(output_dim<span class="op">=</span><span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>))  <span class="co"># Output layer with sigmoid</span></span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a>    mlp.<span class="bu">compile</span>()  <span class="co"># Initialize weights and biases</span></span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a>    mlp.train(X, y, epochs<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Simulating </span><span class="sc">{</span>gate_type<span class="sc">}</span><span class="ss"> gate"</span>)</span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> input_data, prediction <span class="kw">in</span> <span class="bu">zip</span>(X, mlp.predict(X)):</span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Input: </span><span class="sc">{</span>input_data<span class="sc">}</span><span class="ss">, Prediction: </span><span class="sc">{</span>prediction<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a>simulate_logic_gate(<span class="st">'XOR'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, Loss: 0.26675111093118614
Epoch 1000, Loss: 0.17207706006267234
Epoch 2000, Loss: 0.16779137618476658
Epoch 3000, Loss: 0.16724944687955118
Epoch 4000, Loss: 0.16704977869177204
Epoch 5000, Loss: 0.1669495367953534
Epoch 6000, Loss: 0.1668898844069387
Epoch 7000, Loss: 0.16684954673672653
Epoch 8000, Loss: 0.1668241650269377
Epoch 9000, Loss: 0.16680207123082322
Simulating XOR gate
Input: [0 0], Prediction: [0.]
Input: [0 1], Prediction: [1.]
Input: [1 0], Prediction: [0.]
Input: [1 1], Prediction: [0.]</code></pre>
</div>
</div>
</section>
<section id="performance-metrics-a-critical-component-of-prediction-problems" class="level2" data-number="1.10">
<h2 data-number="1.10" class="anchored" data-anchor-id="performance-metrics-a-critical-component-of-prediction-problems"><span class="header-section-number">1.10</span> Performance Metrics: A Critical Component of Prediction Problems</h2>
<p>In machine learning and predictive modeling, simply achieving a correct output is not sufficient to evaluate the effectiveness of a model. Various performance measures are essential for assessing the quality and reliability of predictions. These metrics provide a deeper understanding of how well the model performs across different aspects of a dataset.</p>
<section id="confusion-matrix" class="level3" data-number="1.10.1">
<h3 data-number="1.10.1" class="anchored" data-anchor-id="confusion-matrix"><span class="header-section-number">1.10.1</span> Confusion Matrix</h3>
<p>The confusion matrix summarizes the results of a binary classification task. It includes:</p>
<ul>
<li><strong>True Positives (TP):</strong> Correctly predicted positive cases.<br>
</li>
<li><strong>True Negatives (TN):</strong> Correctly predicted negative cases.<br>
</li>
<li><strong>False Positives (FP):</strong> Incorrectly predicted positive cases (Type I error).<br>
</li>
<li><strong>False Negatives (FN):</strong> Incorrectly predicted negative cases (Type II error).</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>Predicted Positive</th>
<th>Predicted Negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual Positive</strong></td>
<td>TP</td>
<td>FN</td>
</tr>
<tr class="even">
<td><strong>Actual Negative</strong></td>
<td>FP</td>
<td>TN</td>
</tr>
</tbody>
</table>
</section>
<section id="accuracy" class="level3" data-number="1.10.2">
<h3 data-number="1.10.2" class="anchored" data-anchor-id="accuracy"><span class="header-section-number">1.10.2</span> Accuracy</h3>
<p>Accuracy is the proportion of correctly classified cases out of the total number of cases:</p>
<p><span class="math display">\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]</span></p>
<p><strong>Interpretation:</strong> Accuracy is a straightforward measure but can be misleading for imbalanced datasets. For example, in fraud detection with a 99:1 class ratio, a model predicting all cases as non-fraud achieves 99% accuracy but fails to detect fraud.</p>
<hr>
</section>
<section id="precision" class="level3" data-number="1.10.3">
<h3 data-number="1.10.3" class="anchored" data-anchor-id="precision"><span class="header-section-number">1.10.3</span> Precision</h3>
<p>Precision measures the proportion of true positive predictions out of all positive predictions:</p>
<p><span class="math display">\[
\text{Precision} = \frac{TP}{TP + FP}
\]</span></p>
<p><strong>Interpretation:</strong> High precision indicates a low false positive rate, which is crucial in domains like medical diagnostics, where false alarms can lead to unnecessary procedures.</p>
<hr>
</section>
<section id="recall-sensitivity" class="level3" data-number="1.10.4">
<h3 data-number="1.10.4" class="anchored" data-anchor-id="recall-sensitivity"><span class="header-section-number">1.10.4</span> Recall (Sensitivity)</h3>
<p>Recall (or sensitivity) is the proportion of actual positives correctly identified:</p>
<p><span class="math display">\[
\text{Recall} = \frac{TP}{TP + FN}
\]</span></p>
<p><strong>Interpretation:</strong> Recall is critical when false negatives are costly, such as in cancer detection.</p>
<hr>
</section>
<section id="specificity" class="level3" data-number="1.10.5">
<h3 data-number="1.10.5" class="anchored" data-anchor-id="specificity"><span class="header-section-number">1.10.5</span> Specificity</h3>
<p>Specificity measures the proportion of actual negatives correctly identified:</p>
<p><span class="math display">\[
\text{Specificity} = \frac{TN}{TN + FP}
\]</span></p>
<p><strong>Interpretation:</strong> Specificity is important when the cost of false positives is high, such as in spam email detection.</p>
<hr>
</section>
<section id="f1-score" class="level3" data-number="1.10.6">
<h3 data-number="1.10.6" class="anchored" data-anchor-id="f1-score"><span class="header-section-number">1.10.6</span> F1 Score</h3>
<p>The F1 score combines precision and recall into a single metric using their harmonic mean:</p>
<p><span class="math display">\[
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\]</span></p>
<p><strong>Interpretation:</strong> The F1 score is useful when there is a trade-off between precision and recall and provides a balanced view of model performance.</p>
<hr>
</section>
<section id="relation-to-hypothesis-testing" class="level3" data-number="1.10.7">
<h3 data-number="1.10.7" class="anchored" data-anchor-id="relation-to-hypothesis-testing"><span class="header-section-number">1.10.7</span> Relation to Hypothesis Testing</h3>
<p>In statistical hypothesis testing:</p>
<ul>
<li><strong>Type I Error:</strong> Rejecting the null hypothesis when it is true (analogous to FP).<br>
</li>
<li><strong>Type II Error:</strong> Failing to reject the null hypothesis when it is false (analogous to FN).</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 37%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th>Error Type</th>
<th>Confusion Matrix Component</th>
<th>Hypothesis Testing Relation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Type I Error</strong></td>
<td>FP</td>
<td>Reject a true null hypothesis</td>
</tr>
<tr class="even">
<td><strong>Type II Error</strong></td>
<td>FN</td>
<td>Fail to reject a false null hypothesis</td>
</tr>
</tbody>
</table>
</section>
<section id="balancing-errors" class="level3" data-number="1.10.8">
<h3 data-number="1.10.8" class="anchored" data-anchor-id="balancing-errors"><span class="header-section-number">1.10.8</span> Balancing Errors</h3>
<p>The trade-off between Type I and Type II errors in hypothesis testing corresponds to optimizing precision and recall in machine learning. For instance, increasing sensitivity (recall) often reduces specificity, analogous to increasing the power of a test at the cost of a higher false positive rate.</p>
<hr>
</section>
<section id="choosing-metrics" class="level3" data-number="1.10.9">
<h3 data-number="1.10.9" class="anchored" data-anchor-id="choosing-metrics"><span class="header-section-number">1.10.9</span> Choosing Metrics</h3>
<ul>
<li>Use <strong>precision</strong> to minimize inconveniences caused by legitimate emails being flagged as spam.<br>
</li>
<li>Use <strong>recall</strong> to ensure spam emails are effectively caught.<br>
</li>
<li>Optimize the <strong>F1 score</strong> for a balanced performance.</li>
</ul>
</section>
<section id="interpretation-of-results-simulating-xor-gate-with-mlp-as-a-classification-problem" class="level3" data-number="1.10.10">
<h3 data-number="1.10.10" class="anchored" data-anchor-id="interpretation-of-results-simulating-xor-gate-with-mlp-as-a-classification-problem"><span class="header-section-number">1.10.10</span> Interpretation of Results: Simulating XOR Gate with MLP as a classification problem</h3>
<p>Now let’s redesign the XOR gate problem as a classification problem. The Multilayer Perceptron (MLP) was tested on the XOR gate problem, producing the following predictions and performance metrics:</p>
<hr>
<div id="4467ae37" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Layer:</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, output_dim, activation<span class="op">=</span><span class="st">"sigmoid"</span>):</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_dim <span class="op">=</span> output_dim</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> activation</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> <span class="va">None</span>  <span class="co"># Will be initialized later</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.biases <span class="op">=</span> <span class="va">None</span>    <span class="co"># Will be initialized later</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inputs <span class="op">=</span> <span class="va">None</span>   <span class="co"># Will store inputs for backpropagation</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs  <span class="co"># Store inputs for backpropagation</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> np.dot(inputs, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.biases</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.activation <span class="op">==</span> <span class="st">"sigmoid"</span>:</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))  <span class="co"># Store output for backpropagation</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.output</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.activation <span class="op">==</span> <span class="st">"relu"</span>:</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output <span class="op">=</span> np.maximum(<span class="dv">0</span>, z)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.output</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:  <span class="co"># Linear activation by default</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output <span class="op">=</span> z</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.output</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, output_error, learning_rate):</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.activation <span class="op">==</span> <span class="st">"sigmoid"</span>:</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>            activation_derivative <span class="op">=</span> <span class="va">self</span>.output <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.output)</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.activation <span class="op">==</span> <span class="st">"relu"</span>:</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>            activation_derivative <span class="op">=</span> np.where(<span class="va">self</span>.output <span class="op">&gt;</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:  <span class="co"># Linear activation</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>            activation_derivative <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>        delta <span class="op">=</span> output_error <span class="op">*</span> activation_derivative</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        input_error <span class="op">=</span> np.dot(delta, <span class="va">self</span>.weights.T)</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update weights and biases</span></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">+=</span> learning_rate <span class="op">*</span> np.dot(<span class="va">self</span>.inputs.T, delta)</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.biases <span class="op">+=</span> learning_rate <span class="op">*</span> np.<span class="bu">sum</span>(delta, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> input_error</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP:</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, learning_rate<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_dim <span class="op">=</span> input_dim</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> []</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> add(<span class="va">self</span>, layer):</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers.append(layer)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="bu">compile</span>(<span class="va">self</span>):</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights and biases for each layer</span></span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>        prev_dim <span class="op">=</span> <span class="va">self</span>.input_dim</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>            layer.weights <span class="op">=</span> np.random.randn(prev_dim, layer.output_dim)</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>            layer.biases <span class="op">=</span> np.zeros((<span class="dv">1</span>, layer.output_dim))</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>            prev_dim <span class="op">=</span> layer.output_dim</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> X</span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> layer.forward(output)</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, X, y, epochs<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass</span></span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>.forward(X)</span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Backward pass (backpropagation)</span></span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>            error <span class="op">=</span> y <span class="op">-</span> output</span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">reversed</span>(<span class="va">self</span>.layers):</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>                error <span class="op">=</span> layer.backward(error, <span class="va">self</span>.learning_rate)</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> np.mean(np.square(y <span class="op">-</span> <span class="va">self</span>.forward(X)))</span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.<span class="bu">round</span>(<span class="va">self</span>.forward(X))</span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_performance_metrics(y_true, y_pred):</span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a>    TP <span class="op">=</span> np.<span class="bu">sum</span>((y_true <span class="op">==</span> <span class="dv">1</span>) <span class="op">&amp;</span> (y_pred <span class="op">==</span> <span class="dv">1</span>))</span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a>    TN <span class="op">=</span> np.<span class="bu">sum</span>((y_true <span class="op">==</span> <span class="dv">0</span>) <span class="op">&amp;</span> (y_pred <span class="op">==</span> <span class="dv">0</span>))</span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a>    FP <span class="op">=</span> np.<span class="bu">sum</span>((y_true <span class="op">==</span> <span class="dv">0</span>) <span class="op">&amp;</span> (y_pred <span class="op">==</span> <span class="dv">1</span>))</span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a>    FN <span class="op">=</span> np.<span class="bu">sum</span>((y_true <span class="op">==</span> <span class="dv">1</span>) <span class="op">&amp;</span> (y_pred <span class="op">==</span> <span class="dv">0</span>))</span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> (TP <span class="op">+</span> TN) <span class="op">/</span> (TP <span class="op">+</span> TN <span class="op">+</span> FP <span class="op">+</span> FN)</span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> TP <span class="op">/</span> (TP <span class="op">+</span> FP) <span class="cf">if</span> (TP <span class="op">+</span> FP) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb18-89"><a href="#cb18-89" aria-hidden="true" tabindex="-1"></a>    recall <span class="op">=</span> TP <span class="op">/</span> (TP <span class="op">+</span> FN) <span class="cf">if</span> (TP <span class="op">+</span> FN) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true" tabindex="-1"></a>    sensitivity <span class="op">=</span> recall</span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a>    specificity <span class="op">=</span> TN <span class="op">/</span> (TN <span class="op">+</span> FP) <span class="cf">if</span> (TN <span class="op">+</span> FP) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a>    f1_score <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (precision <span class="op">*</span> recall) <span class="op">/</span> (precision <span class="op">+</span> recall) <span class="cf">if</span> (precision <span class="op">+</span> recall) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Performance Metrics:"</span>)</span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span>precision<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb18-97"><a href="#cb18-97" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Recall (Sensitivity): </span><span class="sc">{</span>sensitivity<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb18-98"><a href="#cb18-98" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Specificity: </span><span class="sc">{</span>specificity<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb18-99"><a href="#cb18-99" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"F1 Score: </span><span class="sc">{</span>f1_score<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb18-100"><a href="#cb18-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-101"><a href="#cb18-101" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display the confusion matrix</span></span>
<span id="cb18-102"><a href="#cb18-102" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Confusion Matrix:"</span>)</span>
<span id="cb18-103"><a href="#cb18-103" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"                Predicted Positive    Predicted Negative"</span>)</span>
<span id="cb18-104"><a href="#cb18-104" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Actual Positive          </span><span class="sc">{</span>TP<span class="sc">}</span><span class="ss">                    </span><span class="sc">{</span>FN<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-105"><a href="#cb18-105" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Actual Negative          </span><span class="sc">{</span>FP<span class="sc">}</span><span class="ss">                    </span><span class="sc">{</span>TN<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-106"><a href="#cb18-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-107"><a href="#cb18-107" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simulate_logic_gate(gate_type):</span>
<span id="cb18-108"><a href="#cb18-108" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb18-109"><a href="#cb18-109" aria-hidden="true" tabindex="-1"></a>    gate_targets <span class="op">=</span> {</span>
<span id="cb18-110"><a href="#cb18-110" aria-hidden="true" tabindex="-1"></a>        <span class="st">'AND'</span>: np.array([[<span class="dv">0</span>], [<span class="dv">0</span>], [<span class="dv">0</span>], [<span class="dv">1</span>]]),</span>
<span id="cb18-111"><a href="#cb18-111" aria-hidden="true" tabindex="-1"></a>        <span class="st">'OR'</span>: np.array([[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">1</span>]]),</span>
<span id="cb18-112"><a href="#cb18-112" aria-hidden="true" tabindex="-1"></a>        <span class="st">'NAND'</span>: np.array([[<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">0</span>]]),</span>
<span id="cb18-113"><a href="#cb18-113" aria-hidden="true" tabindex="-1"></a>        <span class="st">'NOR'</span>: np.array([[<span class="dv">1</span>], [<span class="dv">0</span>], [<span class="dv">0</span>], [<span class="dv">0</span>]]),</span>
<span id="cb18-114"><a href="#cb18-114" aria-hidden="true" tabindex="-1"></a>        <span class="st">'XOR'</span>: np.array([[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">0</span>]])</span>
<span id="cb18-115"><a href="#cb18-115" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb18-116"><a href="#cb18-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-117"><a href="#cb18-117" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> gate_type <span class="kw">not</span> <span class="kw">in</span> gate_targets:</span>
<span id="cb18-118"><a href="#cb18-118" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Unsupported gate type: </span><span class="sc">{</span>gate_type<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-119"><a href="#cb18-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb18-120"><a href="#cb18-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-121"><a href="#cb18-121" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> gate_targets[gate_type]</span>
<span id="cb18-122"><a href="#cb18-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-123"><a href="#cb18-123" aria-hidden="true" tabindex="-1"></a>    mlp <span class="op">=</span> MLP(input_dim<span class="op">=</span><span class="dv">2</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb18-124"><a href="#cb18-124" aria-hidden="true" tabindex="-1"></a>    mlp.add(Layer(output_dim<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>))  <span class="co"># Hidden layer with ReLU</span></span>
<span id="cb18-125"><a href="#cb18-125" aria-hidden="true" tabindex="-1"></a>    mlp.add(Layer(output_dim<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>))</span>
<span id="cb18-126"><a href="#cb18-126" aria-hidden="true" tabindex="-1"></a>    mlp.add(Layer(output_dim<span class="op">=</span><span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>))  <span class="co"># Output layer with sigmoid</span></span>
<span id="cb18-127"><a href="#cb18-127" aria-hidden="true" tabindex="-1"></a>    mlp.<span class="bu">compile</span>()  <span class="co"># Initialize weights and biases</span></span>
<span id="cb18-128"><a href="#cb18-128" aria-hidden="true" tabindex="-1"></a>    mlp.train(X, y, epochs<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb18-129"><a href="#cb18-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-130"><a href="#cb18-130" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Simulating </span><span class="sc">{</span>gate_type<span class="sc">}</span><span class="ss"> gate"</span>)</span>
<span id="cb18-131"><a href="#cb18-131" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> mlp.predict(X)</span>
<span id="cb18-132"><a href="#cb18-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-133"><a href="#cb18-133" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> input_data, prediction <span class="kw">in</span> <span class="bu">zip</span>(X, y_pred):</span>
<span id="cb18-134"><a href="#cb18-134" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Input: </span><span class="sc">{</span>input_data<span class="sc">}</span><span class="ss">, Prediction: </span><span class="sc">{</span>prediction<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-135"><a href="#cb18-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-136"><a href="#cb18-136" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate and print performance metrics</span></span>
<span id="cb18-137"><a href="#cb18-137" aria-hidden="true" tabindex="-1"></a>    calculate_performance_metrics(y, y_pred)</span>
<span id="cb18-138"><a href="#cb18-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-139"><a href="#cb18-139" aria-hidden="true" tabindex="-1"></a>simulate_logic_gate(<span class="st">'XOR'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, Loss: 0.28381504754791204
Epoch 1000, Loss: 0.07725535343862173
Epoch 2000, Loss: 0.010525085847779664
Epoch 3000, Loss: 0.004882839386145349
Epoch 4000, Loss: 0.00306872487547098
Epoch 5000, Loss: 0.002205583460570259
Epoch 6000, Loss: 0.001709204884404058
Epoch 7000, Loss: 0.00138957106702216
Epoch 8000, Loss: 0.0011676523600389656
Epoch 9000, Loss: 0.0010051402046983272
Simulating XOR gate
Input: [0 0], Prediction: [0.]
Input: [0 1], Prediction: [1.]
Input: [1 0], Prediction: [1.]
Input: [1 1], Prediction: [0.]
Performance Metrics:
Accuracy: 1.00
Precision: 1.00
Recall (Sensitivity): 1.00
Specificity: 1.00
F1 Score: 1.00

Confusion Matrix:
                Predicted Positive    Predicted Negative
Actual Positive          2                    0
Actual Negative          0                    2</code></pre>
</div>
</div>
<section id="predictions" class="level4" data-number="1.10.10.1">
<h4 data-number="1.10.10.1" class="anchored" data-anchor-id="predictions"><span class="header-section-number">1.10.10.1</span> Predictions</h4>
<p>The predictions for the XOR gate are as follows:</p>
<ul>
<li><strong>Input [0, 0]: Prediction [0.]</strong> (Correct)</li>
<li><strong>Input [0, 1]: Prediction [0.]</strong> (Incorrect)</li>
<li><strong>Input [1, 0]: Prediction [1.]</strong> (Correct)</li>
<li><strong>Input [1, 1]: Prediction [0.]</strong> (Incorrect)</li>
</ul>
<p>The model correctly classified 3 out of 4 inputs, resulting in an <strong>accuracy of 75%</strong>.</p>
<hr>
</section>
<section id="performance-metrics" class="level4" data-number="1.10.10.2">
<h4 data-number="1.10.10.2" class="anchored" data-anchor-id="performance-metrics"><span class="header-section-number">1.10.10.2</span> Performance Metrics</h4>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Accuracy</strong></td>
<td>0.75</td>
</tr>
<tr class="even">
<td><strong>Precision</strong></td>
<td>1.00</td>
</tr>
<tr class="odd">
<td><strong>Recall</strong></td>
<td>0.50</td>
</tr>
<tr class="even">
<td><strong>Specificity</strong></td>
<td>1.00</td>
</tr>
<tr class="odd">
<td><strong>F1 Score</strong></td>
<td>0.67</td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>Accuracy</strong>: Indicates the proportion of correct predictions. The model correctly classified 3 out of 4 inputs.</p></li>
<li><p><strong>Precision</strong>: Perfectly predicted the single positive case without any false positives.</p>
<p><span class="math display">\[ \text{Precision} = \frac{\text{TP}}{\text{TP + FP}} = \frac{1}{1 + 0} = 1.0 \]</span></p></li>
<li><p><strong>Recall</strong>: Captures the proportion of actual positives correctly identified. Out of 2 actual positives, only 1 was correctly predicted.</p>
<p><span class="math display">\[ \text{Recall} = \frac{\text{TP}}{\text{TP + FN}} = \frac{1}{1 + 1} = 0.5 \]</span></p></li>
<li><p><strong>Specificity</strong>: Perfectly identified all negative cases without any false positives.</p>
<p><span class="math display">\[ \text{Specificity} = \frac{\text{TN}}{\text{TN + FP}} = \frac{2}{2 + 0} = 1.0 \]</span></p></li>
<li><p><strong>F1 Score</strong>: Balances Precision and Recall, providing a comprehensive measure of model performance.</p>
<p><span class="math display">\[ \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = 2 \cdot \frac{1.0 \cdot 0.5}{1.0 + 0.5} = 0.67 \]</span></p></li>
</ul>
<hr>
</section>
<section id="confusion-matrix-1" class="level4" data-number="1.10.10.3">
<h4 data-number="1.10.10.3" class="anchored" data-anchor-id="confusion-matrix-1"><span class="header-section-number">1.10.10.3</span> Confusion Matrix</h4>
<p>The confusion matrix provides a breakdown of the model’s predictions:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Predicted Positive</strong></th>
<th><strong>Predicted Negative</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual Positive</strong></td>
<td>1 (True Positive)</td>
<td>1 (False Negative)</td>
</tr>
<tr class="even">
<td><strong>Actual Negative</strong></td>
<td>0 (False Positive)</td>
<td>2 (True Negative)</td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="key-observations" class="level4" data-number="1.10.10.4">
<h4 data-number="1.10.10.4" class="anchored" data-anchor-id="key-observations"><span class="header-section-number">1.10.10.4</span> Key Observations</h4>
<ol type="1">
<li><strong>Strengths</strong>:
<ul>
<li>The model demonstrates excellent precision and specificity, correctly predicting all negative cases without any false positives.</li>
<li>It has an overall accuracy of 75%, showing its general capability to classify XOR gate outputs.</li>
</ul></li>
<li><strong>Limitations</strong>:
<ul>
<li>The model struggles with Recall, as it misclassifies 1 out of 2 actual positives (false negative).</li>
<li>The F1 Score of 0.67 indicates a moderate balance between Precision and Recall, but it reflects room for improvement in handling positive cases.</li>
</ul></li>
</ol>
<hr>
</section>
<section id="skill-assessment-of-the-mlp" class="level4" data-number="1.10.10.5">
<h4 data-number="1.10.10.5" class="anchored" data-anchor-id="skill-assessment-of-the-mlp"><span class="header-section-number">1.10.10.5</span> Skill Assessment of the MLP</h4>
<p>The XOR gate problem is a classic non-linear function. While MLPs are theoretically capable of solving such problems, the performance here suggests potential limitations:</p>
<ul>
<li><strong>Model Architecture</strong>: The MLP may need more layers or neurons to fully capture XOR’s non-linear behavior.</li>
<li><strong>Training Process</strong>: Adjustments to hyperparameters like learning rate, epochs, or weight initialization might enhance performance.</li>
</ul>
<hr>
</section>
<section id="relation-to-hypothesis-testing-1" class="level4" data-number="1.10.10.6">
<h4 data-number="1.10.10.6" class="anchored" data-anchor-id="relation-to-hypothesis-testing-1"><span class="header-section-number">1.10.10.6</span> Relation to Hypothesis Testing</h4>
<p>The results can be related to statistical hypothesis testing concepts:</p>
<ul>
<li><strong>Type I Error (False Positive)</strong>: No such errors occurred in this case.</li>
<li><strong>Type II Error (False Negative)</strong>: The model failed to identify 1 actual positive case, corresponding to a Type II Error.</li>
</ul>
<hr>
<p>This MLP exhibits reasonable performance with an accuracy of 75% but requires further refinement to improve its ability to generalize the XOR gate’s non-linear nature, particularly in predicting positive outputs. Improvements in architecture and training can likely address these limitations.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-rosenblatt1957perceptron" class="csl-entry" role="listitem">
Rosenblatt, Frank. 1957. <em>The Perceptron, a Perceiving and Recognizing Automaton Project Para</em>. Cornell Aeronautical Laboratory.
</div>
</div>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Preface">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./benchmarkDL.html" class="pagination-link" aria-label="Benchmarking Deep Learning with Libraries">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Benchmarking Deep Learning with Libraries</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>