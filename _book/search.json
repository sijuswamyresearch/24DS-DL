[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "24DS-DL",
    "section": "",
    "text": "Preface\nThis is a Quarto book.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Basics of Artificial Neural Network",
    "section": "",
    "text": "1.1 Introduction to Artificial Neural Networks (ANNs)\nMachine learning has undeniably become a prominent and dynamic field, with its vast array of algorithms sometimes making it challenging to discern key concepts. To gain a clearer understanding, it is valuable to explore various machine learning algorithms in greater detail, focusing not only on their theoretical foundations but also on their step-by-step implementation.\nIn brief, machine learning is defined as a field that enables computers to learn from data without explicit programming (Arthur Samuel, 1959). It involves the development of algorithms capable of recognizing patterns in data and making decisions based on statistical analysis, probability theory, combinatorics, and optimization techniques.\nThis discussion begins with an exploration of perceptrons and ADALINE (Adaptive Linear Neuron), which are part of single-layer neural networks. The perceptron is the first algorithmically defined learning algorithm and serves as an intuitive, easy-to-implement introduction to modern machine learning algorithms, particularly artificial neural networks (or “deep learning”). ADALINE, an improvement on the perceptron, provides an excellent opportunity to understand gradient descent, a widely-used optimization method in machine learning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of Artificial Neural Network</span>"
    ]
  },
  {
    "objectID": "intro.html#introduction-to-artificial-neural-networks-anns",
    "href": "intro.html#introduction-to-artificial-neural-networks-anns",
    "title": "1  Basics of Artificial Neural Network",
    "section": "",
    "text": "1.1.1 Introduction\nArtificial Neural Networks (ANNs) are a class of computational models inspired by the biological neural networks in the human brain. These models have revolutionized numerous fields, including machine learning, computer vision, natural language processing, and data analytics, by mimicking the complex processing power of the human brain in a simplified computational framework. ANNs are at the core of deep learning algorithms, enabling machines to learn from vast amounts of data, recognize patterns, and make decisions with little to no human intervention.\nThe concept of ANNs can be traced back to the 1940s when pioneers like Warren McCulloch and Walter Pitts introduced the first simplified model of the neuron. In more intuitive terms, neurons can be understood as the subunits of a neural network in a biological brain. Here, the signals of variable magnitudes arrive at the dendrites. Those input signals are then accumulated in the cell body of the neuron, and if the accumulated signal exceeds a certain threshold, a output signal is generated that which will be passed on by the axon. This model, known as the McCulloch-Pitts neuron, was a logical abstraction that represented a binary decision-making process based on weighted inputs. However, it wasn’t until the 1980s, with the development of the backpropagation algorithm by Geoffrey Hinton and others, that ANNs began to demonstrate significant potential for learning complex patterns and tasks.\n\n\n\n\n\n\nKey Concepts in Artificial Neural Networks\n\n\n\n\nNeurons: The fundamental units in an ANN, inspired by biological neurons. Each neuron receives one or more inputs, processes them, and produces an output. The output is typically determined by applying an activation function to a weighted sum of the inputs.\nArchitecture: An ANN is composed of layers of neurons:\n\nInput Layer: The first layer that receives input data.\nHidden Layers: Intermediate layers where computations occur and complex features are learned.\nOutput Layer: The final layer that produces the output or prediction.\n\nThe number of layers and the number of neurons in each layer are important design considerations that influence the network’s ability to learn complex relationships.\nWeights and Biases: Each connection between neurons has an associated weight, which determines the importance of the input. Biases are added to the weighted sum to allow the network to better model the data and shift the activation function.\nActivation Function: The activation function introduces non-linearity into the model, enabling it to learn and represent complex patterns. Common activation functions include:\n\nSigmoid: A logistic function that outputs values between 0 and 1.\nTanh: A hyperbolic tangent function that outputs values between -1 and 1.\nReLU (Rectified Linear Unit): Outputs zero for negative inputs and the input value itself for positive inputs, helping mitigate the vanishing gradient problem in deep networks.\n\nLearning Process: Training an ANN involves adjusting the weights and biases through a process called optimization, typically using the gradient descent algorithm. During training, the network’s predictions are compared to the actual outcomes, and the difference, known as the loss or error, is minimized using optimization techniques.\nBackpropagation: This algorithm computes the gradient of the loss function with respect to each weight by applying the chain rule of calculus. This information is used to update the weights in a way that reduces the overall error, allowing the network to improve over time.\n\n\n\n\n\n1.1.2 Historical Development and Evolution\nThe origins of neural networks lie in the early 20th century, with key milestones such as the Perceptron (developed by Frank Rosenblatt in 1958) and the Backpropagation Algorithm (1986), which was a breakthrough in training multilayer networks. The development of the Deep Learning paradigm in the 2000s, fueled by advances in computing power, large datasets, and efficient algorithms, further accelerated the application of ANNs. Notable examples include Convolutional Neural Networks (CNNs) for image recognition and Recurrent Neural Networks (RNNs) for sequence modeling, such as speech and language processing.\n\n\n1.1.3 Modern Applications of Artificial Neural Networks\nIn recent years, the ability of ANNs to perform high-level tasks has grown substantially. Some of the transformative applications include:\n\nComputer Vision: ANNs are used in image classification, object detection, facial recognition, and medical image analysis.\nNatural Language Processing (NLP): ANNs, particularly transformer models, power state-of-the-art techniques in machine translation, sentiment analysis, and chatbots.\nRobotics and Autonomous Systems: Neural networks enable robots to perceive their environment and make real-time decisions.\nHealthcare: ANNs are applied in predictive analytics for disease diagnosis, treatment planning, and drug discovery.\nFinance: ANNs help in fraud detection, algorithmic trading, and customer behavior prediction.\n\n\n\n1.1.4 Challenges and Future Directions\nDespite their powerful capabilities, ANNs face several challenges:\n\nOverfitting: Neural networks can become too specialized to the training data, losing the ability to generalize to new, unseen data.\nInterpretability: The “black-box” nature of ANNs makes it difficult to understand how they arrive at specific decisions, which can be problematic in fields requiring explainability (e.g., healthcare, law).\nData and Computation: Training deep neural networks requires large amounts of labeled data and significant computational resources, which can be limiting in certain contexts.\n\nFuture research directions aim to address these challenges, including the development of more interpretable models, reducing the data and computation requirements, and creating more robust systems that can generalize across different domains.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of Artificial Neural Network</span>"
    ]
  },
  {
    "objectID": "intro.html#perceptron",
    "href": "intro.html#perceptron",
    "title": "1  Basics of Artificial Neural Network",
    "section": "1.2 Perceptron",
    "text": "1.2 Perceptron\nTo continue with the story, a few years after McCulloch and Walter Pitt, Frank Rosenblatt published the first concept of the Perceptron learning rule. The perceptron is one of the earliest neural network models (Rosenblatt 1957).\n\n\n\n\n\n\nFrank Rosenblatt’s Perceptron\n\n\n\nThe main idea was to define an algorithm in order to learn the values of the weights \\(w\\) that are then multiplied with the input features in order to make a decision whether a neuron fires or not. In context of pattern classification, such an algorithm could be useful to determine if a sample belongs to one class or the other.\n\n\nIt models a single artificial neuron capable of binary classification for linearly separable data. Despite its limitations, the perceptron laid the foundation for more complex architectures like Multilayer Perceptrons (MLPs) and deep neural networks, which use sigmoid neurons for non-linear decision boundaries.\n\n1.2.1 Perceptron Model\n\n\n\n\n\ngraph TD\n    X1(Input X1) --&gt; W1(Weight W1)\n    X2(Input X2) --&gt; W2(Weight W2)\n    Xn(Input Xn) --&gt; Wn(Weight Wn)\n    W1 --&gt; SUM[Summation Σ]\n    W2 --&gt; SUM\n    Wn --&gt; SUM\n    B(Bias) --&gt; SUM\n    SUM --&gt; ACT[Activation Function]\n    ACT --&gt; Y(Output Y)\n    \n    style SUM fill:#f9f,stroke:#333,stroke-width:2px\n    style ACT fill:#bbf,stroke:#333,stroke-width:2px\n    style Y fill:#bfb,stroke:#333,stroke-width:2px\n    classDef input fill:#ff9,stroke:#333,stroke-width:2px;\n    classDef weight fill:#9cf,stroke:#333,stroke-width:2px;\n\n    class X1,X2,Xn input;\n    class W1,W2,Wn weight;\n\n\n\n\n\n\nThe perceptron computes a weighted sum of inputs and applies a step function for classification: \\[\nz = \\sum_{i=1}^n w_i x_i + b\n\\] \\[\n\\hat{y} = \\begin{cases}\n      1 & \\text{if } z &gt; 0 \\\\\n      0 & \\text{otherwise}\n   \\end{cases}\n\\]\n\n\n1.2.2 Perceptron Learning Rule\nWeights are updated iteratively based on the error: \\[\nw_i \\gets w_i + \\eta (y - \\hat{y}) x_i\n\\] where \\(\\eta\\) is the learning rate.\n\n\n1.2.3 Perceptron Algorithm\nInput: A dataset \\(D = \\{(x_i, y_i)\\}\\), where \\(x_i\\) is the feature vector and \\(y_i\\in \\{+1,-1\\}\\) is the label.\nOutput: A weight vector \\(\\mathbf{w}\\).\n\nInitialize \\(\\mathbf{w} = \\mathbf{0}\\).\nRepeat until convergence:\n\nSet \\(m = 0\\).\nFor each \\((x_i, y_i) \\in D\\):\n\nIf \\(y_i (\\mathbf{w}^T \\cdot \\mathbf{x_i}) \\leq 0\\):\n\nUpdate \\(\\mathbf{w} \\gets \\mathbf{w} + y_i \\mathbf{x_i}\\).\nIncrement \\(m \\gets m + 1\\).\n\n\nIf \\(m = 0\\), terminate the algorithm.\n\n\n\n\n\n\n\n\nPerceptron Convergence\n\n\n\nThe Perceptron was arguably the first algorithm with a strong formal guarantee. If a data set is linearly separable, the Perceptron will find a separating hyperplane in a finite number of updates. (If the data is not linearly separable, it will loop forever.)\n\n\n\n\n1.2.4 Perceptron Theorem and Margin\nThe Perceptron Mistake Bound Theorem states that the Perceptron algorithm makes at most \\(\\frac{1}{\\gamma^2}\\) updates (mistakes), where \\(\\gamma\\) is the margin of separability of the data. The margin is defined as the smallest distance between any data point and the decision boundary, normalized by the magnitude of the weight vector: \\[\n\\gamma = \\frac{\\min_{i} y_i (\\mathbf{w}^T \\mathbf{x_i})}{\\|\\mathbf{w}\\|}\n\\] where: - \\(\\mathbf{w}\\) is the weight vector. - \\(\\mathbf{x_i}\\) is a data point. - \\(y_i\\) is the corresponding label ((+1) or (-1)).\n\n\n1.2.5 Implications of the Theorem\n\nLarge Margin is Desirable:\n\nA larger margin \\(\\gamma\\) implies fewer mistakes because the mistake bound decreases as \\(\\gamma\\) increases.\nIntuitively, a larger margin means the data points are farther from the decision boundary, making them less likely to be misclassified.\n\nQuick Convergence:\n\nThe algorithm will converge faster on datasets with a larger margin since fewer updates (or mistakes) are required.\nConversely, if \\(\\gamma\\) is small (data points are closer to the boundary), the algorithm requires more updates to separate the data correctly.\n\n\n\n\n1.2.6 Characterizing Data Sets for Fast Convergence\nDatasets for which the Perceptron algorithm converges quickly share the following properties:\n\nLarge Margin:\n\nData points are well-separated from the decision boundary.\nThe decision boundary can be drawn with minimal ambiguity.\n\nLinearly Separable Data:\n\nThe dataset must be linearly separable for the Perceptron algorithm to converge.\nOverlapping or inseparable datasets will cause the algorithm to loop indefinitely.\n\n\n\n\n1.2.7 Example of a Dataset with Large Margin\nConsider the following dataset in two-dimensional space:\n\nPositive class (\\(y_i = +1\\)): Points \\((3, 3), (4, 4), (5, 5)\\).\nNegative class (\\(y_i = -1\\)): Points \\((-3, -3), (-4, -4), (-5, -5)\\).\n\nThe data is linearly separable with a large margin (distance between closest points and the decision boundary).\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Dataset\npositive_points = np.array([[3, 3], [4, 4], [5, 5]])\nnegative_points = np.array([[-3, -3], [-4, -4], [-5, -5]])\n\n# Define weights and bias for the Perceptron hyperplane\n# Example weights (assuming the Perceptron learned these weights)\nw = np.array([1, 1])  # w1 and w2\nb = 0                 # Bias term\n\n# Generate x1 values for plotting\nx1 = np.linspace(-6, 6, 100)\n\n# Compute x2 values from the hyperplane equation w1*x1 + w2*x2 + b = 0\nx2 = -(w[0] * x1 + b) / w[1]\n\n# Plot the dataset\nplt.scatter(positive_points[:, 0], positive_points[:, 1], color='blue', label='Positive Class ($y=+1$)', s=100)\nplt.scatter(negative_points[:, 0], negative_points[:, 1], color='red', label='Negative Class ($y=-1$)', s=100)\n\n# Plot the decision boundary (hyperplane)\nplt.plot(x1, x2, color='black', linestyle='--', label='Decision Boundary ($w^T x = 0$)')\n\n# Formatting the plot\nplt.axhline(0, color='black', linewidth=0.5, linestyle='-')\nplt.axvline(0, color='black', linewidth=0.5, linestyle='-')\nplt.grid(alpha=0.3)\nplt.legend()\nplt.title('Linearly Separable Dataset with Perceptron Decision Boundary')\nplt.xlabel('$x_1$')\nplt.ylabel('$x_2$')\nplt.xlim(-6, 6)\nplt.ylim(-6, 6)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.2.8 Simulating OR gate using Perceptron\n\n\n\n\n\n\n\nperceptron\n\n\n\ninput1\n\nx1\n\n\n\nweight1\n\nw1\n\n\n\ninput1-&gt;weight1\n\n\nw1\n\n\n\ninput2\n\nx2\n\n\n\nweight2\n\nw2\n\n\n\ninput2-&gt;weight2\n\n\nw2\n\n\n\nbias\n\nBias (b)\n\n\n\nweighted_sum\n\nw1 * x1 + w2 * x2 + b\n\n\n\nbias-&gt;weighted_sum\n\n\n + b\n\n\n\nweight1-&gt;weighted_sum\n\n\n* x1\n\n\n\nweight2-&gt;weighted_sum\n\n\n* x2\n\n\n\nactivation\n\nActivation Step Function: f(x) = 1 if x &gt;= 0 else 0\n\n\n\nweighted_sum-&gt;activation\n\n\npass to\n\n\n\noutput\n\ny (Output)\n\n\n\nactivation-&gt;output\n\n\nstep function\n\n\n\noutput-&gt;output\n\n\ny\n\n\n\n\n\n\n\n\nThe following python code will simulate the OR gate using the logic of perceptron.\n\nimport numpy as np\n\n# Step function (activation function)\ndef step_function(x):\n    return 1 if x &gt;= 0 else 0\n\n# Perceptron training algorithm\ndef perceptron(X, y, learning_rate=0.1, epochs=10):\n    # Initialize weights and bias\n    weights = np.zeros(X.shape[1])\n    bias = 0\n    \n    # Training process\n    for epoch in range(epochs):\n        total_error = 0\n        for i in range(len(X)):\n            # Calculate weighted sum (z)\n            z = np.dot(X[i], weights) + bias\n            # Apply step function to get prediction\n            prediction = step_function(z)\n            \n            # Calculate error\n            error = y[i] - prediction\n            total_error += abs(error)\n            \n            # Update weights and bias based on the error\n            weights += learning_rate * error * X[i]\n            bias += learning_rate * error\n        \n        # Optionally, print the error for each epoch\n        print(f'Epoch {epoch + 1}: Total Error = {total_error}')\n    \n    return weights, bias\n\n# Perceptron test function\ndef predict(X, weights, bias):\n    predictions = []\n    for i in range(len(X)):\n        z = np.dot(X[i], weights) + bias\n        prediction = step_function(z)\n        predictions.append(prediction)\n    return predictions\n\n# OR Gate Inputs and Outputs\n# Input X: [A, B] where A and B are the inputs\n# Output y: The corresponding OR operation output\nX = np.array([[0, 0],\n              [0, 1],\n              [1, 0],\n              [1, 1]])\n\ny = np.array([0, 1, 1, 1])  # OR gate outputs\n\n# Train the perceptron\nweights, bias = perceptron(X, y, learning_rate=0.1, epochs=10)\n\n# Test the perceptron\npredictions = predict(X, weights, bias)\n\n# Print predictions\nprint(\"\\nPredictions:\")\nfor i in range(len(X)):\n    print(f'Input: {X[i]}, Predicted Output: {predictions[i]}')\n\nEpoch 1: Total Error = 2\nEpoch 2: Total Error = 2\nEpoch 3: Total Error = 1\nEpoch 4: Total Error = 0\nEpoch 5: Total Error = 0\nEpoch 6: Total Error = 0\nEpoch 7: Total Error = 0\nEpoch 8: Total Error = 0\nEpoch 9: Total Error = 0\nEpoch 10: Total Error = 0\n\nPredictions:\nInput: [0 0], Predicted Output: 0\nInput: [0 1], Predicted Output: 1\nInput: [1 0], Predicted Output: 1\nInput: [1 1], Predicted Output: 1\n\n\n\n\n1.2.9 Simulating AND gate using a perceptron\nThe perceptron model for an AND gate is shown below.\n\n\n\n\n\n\n\nperceptron\n\n\n\ninput1\n\nx1\n\n\n\nweight1\n\nw1\n\n\n\ninput1-&gt;weight1\n\n\nw1\n\n\n\ninput2\n\nx2\n\n\n\nweight2\n\nw2\n\n\n\ninput2-&gt;weight2\n\n\nw2\n\n\n\nbias\n\nBias (b)\n\n\n\nweighted_sum\n\nw1 * x1 + w2 * x2 + b\n\n\n\nbias-&gt;weighted_sum\n\n\n + b\n\n\n\nweight1-&gt;weighted_sum\n\n\n* x1\n\n\n\nweight2-&gt;weighted_sum\n\n\n* x2\n\n\n\nactivation\n\nActivation Step Function: f(x) = 1 if x &gt;= 0 else 0\n\n\n\nweighted_sum-&gt;activation\n\n\npass to\n\n\n\noutput\n\ny (Output)\n\n\n\nactivation-&gt;output\n\n\nstep function\n\n\n\noutput-&gt;output\n\n\ny\n\n\n\n\n\n\n\n\nThe python code for simulating the AND gate using the perceptron is shown below.\n\nimport numpy as np\n\n# Step function (activation function)\ndef step_function(x):\n    return 1 if x &gt;= 0 else 0\n\n# Perceptron training algorithm\ndef perceptron(X, y, learning_rate=0.1, epochs=10):\n    # Initialize weights and bias\n    weights = np.zeros(X.shape[1])\n    bias = 0\n    \n    # Training process\n    for epoch in range(epochs):\n        total_error = 0\n        for i in range(len(X)):\n            # Calculate weighted sum (z)\n            z = np.dot(X[i], weights) + bias\n            # Apply step function to get prediction\n            prediction = step_function(z)\n            \n            # Calculate error\n            error = y[i] - prediction\n            total_error += abs(error)\n            \n            # Update weights and bias based on the error\n            weights += learning_rate * error * X[i]\n            bias += learning_rate * error\n        \n        # Optionally, print the error for each epoch\n        print(f'Epoch {epoch + 1}: Total Error = {total_error}')\n    \n    return weights, bias\n\n# Perceptron test function\ndef predict(X, weights, bias):\n    predictions = []\n    for i in range(len(X)):\n        z = np.dot(X[i], weights) + bias\n        prediction = step_function(z)\n        predictions.append(prediction)\n    return predictions\n\n# OR Gate Inputs and Outputs\n# Input X: [A, B] where A and B are the inputs\n# Output y: The corresponding OR operation output\nX = np.array([[0, 0],\n              [0, 1],\n              [1, 0],\n              [1, 1]])\n\ny = np.array([0, 1, 1, 1])  # OR gate outputs\n\n# Train the perceptron\nweights, bias = perceptron(X, y, learning_rate=0.1, epochs=10)\n\n# Test the perceptron\npredictions = predict(X, weights, bias)\n\n# Print predictions\nprint(\"\\nPredictions:\")\nfor i in range(len(X)):\n    print(f'Input: {X[i]}, Predicted Output: {predictions[i]}')\n\nEpoch 1: Total Error = 2\nEpoch 2: Total Error = 2\nEpoch 3: Total Error = 1\nEpoch 4: Total Error = 0\nEpoch 5: Total Error = 0\nEpoch 6: Total Error = 0\nEpoch 7: Total Error = 0\nEpoch 8: Total Error = 0\nEpoch 9: Total Error = 0\nEpoch 10: Total Error = 0\n\nPredictions:\nInput: [0 0], Predicted Output: 0\nInput: [0 1], Predicted Output: 1\nInput: [1 0], Predicted Output: 1\nInput: [1 1], Predicted Output: 1\n\n\n\n\n1.2.10 Simulating all logic gates using the Perceptron\nHere is the Python code to simulate all logic gates.\n\nimport numpy as np\n\nclass Perceptron:\n    def __init__(self, input_dim, learning_rate=0.01):\n        self.input_dim = input_dim\n        self.learning_rate = learning_rate\n        # Include bias weight with size (input_dim + 1)\n        self.weights = np.random.randn(input_dim + 1) * 0.01  # +1 for the bias term\n\n    def activation(self, net_input):\n        \"\"\"\n        Apply the activation function (step function).\n\n        Parameters:\n        net_input (numpy.ndarray): Net input values.\n\n        Returns:\n        numpy.ndarray: Activated output (0 or 1).\n        \"\"\"\n        return np.where(net_input &gt; 0, 1, 0)\n\n    def predict(self, X):\n        \"\"\"\n        Perform a forward pass to calculate the output of the perceptron.\n\n        Parameters:\n        X (numpy.ndarray): Input data of shape (n_samples, input_dim).\n\n        Returns:\n        numpy.ndarray: Predicted binary outputs (0 or 1), shape (n_samples,).\n        \"\"\"\n        # Ensure X is a 2D array (n_samples, n_features)\n        if X.ndim == 1:\n            X = X.reshape(1, -1)  # Reshape if it's a single sample\n        # Add bias term to input before predicting\n        X_bias = np.c_[X, np.ones(X.shape[0])]  # Add bias column for prediction\n        net_input = np.dot(X_bias, self.weights)  # Dot product with weights\n        return self.activation(net_input)\n\n    def train(self, X, y, epochs=100):\n        \"\"\"\n        Train the perceptron using the provided data.\n\n        Parameters:\n        X (numpy.ndarray): Input data of shape (n_samples, input_dim).\n        y (numpy.ndarray): Target labels of shape (n_samples,).\n        epochs (int): Number of iterations over the dataset.\n        \"\"\"\n        for epoch in range(epochs):\n            for xi, yi in zip(X, y):\n                xi_bias = np.append(xi, 1)  # Add bias term manually for training\n                # Compute the weighted sum (net input) directly here\n                weighted_input = np.dot(xi_bias, self.weights)  # Dot product with weights\n                prediction = self.activation(weighted_input)  # Apply activation\n                error = yi - prediction  # Compute error\n                self.weights += self.learning_rate * error * xi_bias  # Update weights\n\n# Non-member function to simulate logic gates\ndef simulate_logic_gate(gate_type):\n    \"\"\"\n    Simulate a logic gate using the Perceptron.\n\n    Parameters:\n    gate_type (str): The type of logic gate ('AND', 'OR', 'NAND', 'NOR', 'XOR').\n\n    Prints the results of the simulation.\n    \"\"\"\n    # Define input and target outputs for different gates\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    gate_targets = {\n        'AND': np.array([0, 0, 0, 1]),\n        'OR': np.array([0, 1, 1, 1]),\n        'NAND': np.array([1, 1, 1, 0]),\n        'NOR': np.array([1, 0, 0, 0]),\n        'XOR': np.array([0, 1, 1, 0])\n    }\n\n    if gate_type not in gate_targets:\n        print(f\"Unsupported gate type: {gate_type}\")\n        return\n\n    #if gate_type == 'XOR':\n    #    print(f\"Warning: XOR is not linearly separable and cannot be solved by a single perceptron!\")\n    #    return\n\n    y = gate_targets[gate_type]\n\n    # Initialize perceptron\n    perceptron = Perceptron(input_dim=2, learning_rate=0.1)\n\n    # Train perceptron\n    perceptron.train(X, y, epochs=10)\n\n    # Test predictions\n    print(f\"Simulating {gate_type} gate\")\n    print(\"Trained weights:\", perceptron.weights[:-1])  # Exclude bias term from display\n    print(\"Trained bias:\", perceptron.weights[-1])  # Display bias term separately\n    for i in range(X.shape[0]):  # Iterate over the rows of X (4 samples)\n        input_data = X[i]  # Extract the i-th row as a 1D array\n        prediction = perceptron.predict(input_data)  # Make prediction\n        print(f\"Input: {input_data}, Prediction: {prediction}\")\n    # Simulate AND, OR, NAND, NOR, and XOR gates\n\nThe simulation program is executed below:\n\nfor gate in ['AND', 'OR', 'NAND', 'NOR', 'XOR']:\n    simulate_logic_gate(gate)\n\nSimulating AND gate\nTrained weights: [0.19255948 0.01568185]\nTrained bias: -0.19724924822640671\nInput: [0 0], Prediction: [0]\nInput: [0 1], Prediction: [0]\nInput: [1 0], Prediction: [0]\nInput: [1 1], Prediction: [1]\nSimulating OR gate\nTrained weights: [0.11987909 0.1806191 ]\nTrained bias: -0.09898150070115283\nInput: [0 0], Prediction: [0]\nInput: [0 1], Prediction: [1]\nInput: [1 0], Prediction: [1]\nInput: [1 1], Prediction: [1]\nSimulating NAND gate\nTrained weights: [-0.19294302 -0.0100726 ]\nTrained bias: 0.20135888956267056\nInput: [0 0], Prediction: [1]\nInput: [0 1], Prediction: [1]\nInput: [1 0], Prediction: [1]\nInput: [1 1], Prediction: [0]\nSimulating NOR gate\nTrained weights: [-0.08542362 -0.08811517]\nTrained bias: 0.00012291793471830292\nInput: [0 0], Prediction: [1]\nInput: [0 1], Prediction: [0]\nInput: [1 0], Prediction: [0]\nInput: [1 1], Prediction: [0]\nSimulating XOR gate\nTrained weights: [-0.10566772 -0.00842584]\nTrained bias: 0.0949770271172862\nInput: [0 0], Prediction: [1]\nInput: [0 1], Prediction: [1]\nInput: [1 0], Prediction: [0]\nInput: [1 1], Prediction: [0]\n\n\nwhether the perceptron win on XOR gate? No, the perceptron cannot learn the XOR gate. This is a well-known limitation of the perceptron model, and it’s related to the fact that the XOR function is non-linearly separable.\n\n\n\n\n\n\nPerceptron fails!\n\n\n\nA perceptron can only solve problems that are linearly separable, meaning that the classes (outputs) can be separated by a straight line (or a hyperplane in higher dimensions). The XOR gate outputs 1 when the inputs are (0, 1) or (1, 0), and outputs 0 when the inputs are (0, 0) or (1, 1).\nIf you try to plot these points, you’ll see that you can’t separate the positive examples (output 1) from the negative examples (output 0) with a single straight line.\n\n\nAs solution to this problem, we need the cocept of Multi-layer perceptron.\n\n\n\n\n\n\nPerceptron neuron- the foundation of modern Machine Learning\n\n\n\nThe Perceptron model, introduced by Frank Rosenblatt in 1958, marked one of the earliest developments in artificial intelligence and neural networks. Initially conceived as a model for pattern recognition and early neural computation, the Perceptron was designed to simulate a biological neuron, learning to classify inputs into two categories through a simple linear decision boundary. Despite its early promise, the limitations of the basic Perceptron were exposed in the 1960s, particularly its inability to solve non-linearly separable problems, famously highlighted in Marvin Minsky and Seymour Papert’s book Perceptrons (1969). However, with the advent of more sophisticated algorithms and architectures, such as multi-layer perceptrons (MLPs) and the backpropagation algorithm in the 1980s, the Perceptron concept was revitalized. Today, it forms the foundational concept for deep learning models and modern neural networks, which are widely applied in various fields, including image and speech recognition, natural language processing, and autonomous systems, demonstrating its enduring relevance and adaptability in tackling complex, non-linear real-world problems.\n\n\n\n\n1.2.11 Introduction to the Sigmoid Activation Function\nThe sigmoid function, defined as\n\\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}},\n\\]\nmaps any real-valued input to an output between 0 and 1, making it ideal for binary classification tasks. It has played a pivotal role in the development of neural networks, especially in overcoming the limitations of the step function used in early perceptrons.\n\n\n1.2.12 Historical Context\nIn the 1950s, Frank Rosenblatt’s perceptron used the step function, which works well for linearly separable problems but fails with more complex datasets, like the XOR problem. The introduction of the sigmoid activation function in the 1980s addressed this by enabling smooth decision boundaries and facilitating the backpropagation algorithm, allowing neural networks to learn from data effectively.\n\n\n1.2.13 Relevance to Modern Neural Networks\nThe sigmoid function’s differentiability makes it ideal for gradient-based optimization, which is essential for training deep neural networks. Its output is a probability, making it suitable for binary classification problems. Additionally, the derivative of the sigmoid is easy to compute:\n\\[\n\\sigma'(x) = \\sigma(x)(1 - \\sigma(x)),\n\\]\nwhich aids in backpropagation by allowing efficient weight updates. Despite some limitations, such as the vanishing gradient problem, the sigmoid function is widely used in the output layers of networks for tasks requiring probabilistic outputs.\n\n\n1.2.14 Applications\n\nBinary classification (e.g., logistic regression).\nOutput layer in neural networks for binary classification.\nProbabilistic models in machine learning and AI.\n\nWhile alternatives like ReLU are often used in deeper layers due to the vanishing gradient problem, sigmoid remains a powerful tool for probabilistic predictions.\n\n\n1.2.15 Sigmoid Neuron\nThe sigmoid neuron replaces the step function with the sigmoid function: \\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\] This allows for smooth gradients, enabling the use of backpropagation for training MLPs.\n\n\n\n\n\n\n\nNeuralNetwork\n\n\ncluster_input\n\nInput Layer\n\n\ncluster_output\n\nOutput Layer\n\n\ncluster_weights\n\nWeights\n\n\ncluster_neuron\n\nSigmoid Neuron\n\n\n\nX1\n\nx1\n\n\n\nW1\n\nw1\n\n\n\nX1-&gt;W1\n\n\n\n\n\nX2\n\nx2\n\n\n\nW2\n\nw2\n\n\n\nX2-&gt;W2\n\n\n\n\n\nXn\n\nxn\n\n\n\nWn\n\nwn\n\n\n\nXn-&gt;Wn\n\n\n\n\n\nSUM\n\nΣ = Σ(wi * xi) + b\n\n\n\nW1-&gt;SUM\n\n\n\n\n\nW2-&gt;SUM\n\n\n\n\n\nWn-&gt;SUM\n\n\n\n\n\nSIGMOID\n\nσ(z) = 1 / (1 + exp(-z))\n\n\n\nSUM-&gt;SIGMOID\n\n\n\n\n\nY\n\ny\n\n\n\nSIGMOID-&gt;Y\n\n\n\n\n\nB\n\nb (Bias)\n\n\n\nB-&gt;SUM\n\n\n\n\n\n\n\n\n\n\nFollowing python code demonstrate the effective use of the sigmoid activation function in the simulation of logic gates.\n\nimport numpy as np\n\nclass Perceptron:\n    def __init__(self, input_dim, learning_rate=0.01):\n        self.input_dim = input_dim\n        self.learning_rate = learning_rate\n        # Initialize weights including bias term (input_dim + 1)\n        self.weights = np.random.randn(input_dim + 1) * 0.01  # +1 for bias\n\n    def sigmoid(self, x):\n        \"\"\"\n        Sigmoid activation function.\n        \"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        \"\"\"\n        Derivative of sigmoid function.\n        \"\"\"\n        return x * (1 - x)\n\n    def activation(self, net_input):\n        \"\"\"\n        Apply sigmoid activation function to net input.\n        \"\"\"\n        return self.sigmoid(net_input)\n\n    def predict(self, X):\n        \"\"\"\n        Predict the output for given input data using the perceptron.\n        \"\"\"\n        if X.ndim == 1:\n            X = X.reshape(1, -1)  # Reshape if it's a single sample\n        # Add bias term for prediction\n        X_bias = np.c_[X, np.ones(X.shape[0])]\n        net_input = np.dot(X_bias, self.weights)  # Calculate net input\n        output = self.activation(net_input)  # Apply sigmoid activation\n        # Apply threshold: if output &gt; 0.5, interpret as 1; else 0\n        return np.where(output &gt; 0.5, 1, 0)\n\n    def train(self, X, y, epochs=100):\n        \"\"\"\n        Train the perceptron using the provided data.\n        \"\"\"\n        for epoch in range(epochs):\n            for xi, yi in zip(X, y):\n                xi_bias = np.append(xi, 1)  # Add bias term to input\n                net_input = np.dot(xi_bias, self.weights)  # Calculate net input\n                prediction = self.activation(net_input)  # Get the prediction\n                error = yi - prediction  # Calculate error\n                # Update weights using the gradient of the sigmoid function\n                self.weights += self.learning_rate * error * prediction * (1 - prediction) * xi_bias\n\n# Non-member function to simulate logic gates\ndef simulate_logic_gate(gate_type):\n    \"\"\"\n    Simulate a logic gate using the Perceptron.\n    \"\"\"\n    # Define input and target outputs for different gates\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    gate_targets = {\n        'AND': np.array([0, 0, 0, 1]),\n        'OR': np.array([0, 1, 1, 1]),\n        'NAND': np.array([1, 1, 1, 0]),\n        'NOR': np.array([1, 0, 0, 0]),\n        'XOR': np.array([0, 1, 1, 0])\n    }\n\n    if gate_type not in gate_targets:\n        print(f\"Unsupported gate type: {gate_type}\")\n        return\n\n    y = gate_targets[gate_type]\n\n    # Initialize perceptron\n    perceptron = Perceptron(input_dim=2, learning_rate=0.1)\n\n    # Train perceptron\n    perceptron.train(X, y, epochs=1000)\n\n    # Test predictions\n    print(f\"Simulating {gate_type} gate\")\n    print(\"Trained weights:\", perceptron.weights[:-1])  # Exclude bias term from display\n    print(\"Trained bias:\", perceptron.weights[-1])  # Display bias term separately\n    for i in range(X.shape[0]):  # Iterate over the rows of X (4 samples)\n        input_data = X[i]  # Extract the i-th row as a 1D array\n        prediction = perceptron.predict(input_data)  # Make prediction\n        print(f\"Input: {input_data}, Prediction: {prediction}\")\n\nSimulation code is here:\n\nfor gate in ['AND', 'OR', 'NAND', 'NOR', 'XOR']:\n    simulate_logic_gate(gate)\n\nSimulating AND gate\nTrained weights: [2.62578603 2.62209947]\nTrained bias: -4.054759188723422\nInput: [0 0], Prediction: [0]\nInput: [0 1], Prediction: [0]\nInput: [1 0], Prediction: [0]\nInput: [1 1], Prediction: [1]\nSimulating OR gate\nTrained weights: [3.31026374 3.31265317]\nTrained bias: -1.3451193235704602\nInput: [0 0], Prediction: [0]\nInput: [0 1], Prediction: [1]\nInput: [1 0], Prediction: [1]\nInput: [1 1], Prediction: [1]\nSimulating NAND gate\nTrained weights: [-2.62254321 -2.61881998]\nTrained bias: 4.049938849724983\nInput: [0 0], Prediction: [1]\nInput: [0 1], Prediction: [1]\nInput: [1 0], Prediction: [1]\nInput: [1 1], Prediction: [0]\nSimulating NOR gate\nTrained weights: [-3.30747512 -3.30937728]\nTrained bias: 1.3434579457538178\nInput: [0 0], Prediction: [1]\nInput: [0 1], Prediction: [0]\nInput: [1 0], Prediction: [0]\nInput: [1 1], Prediction: [0]\nSimulating XOR gate\nTrained weights: [-0.02471584 -0.01218502]\nTrained bias: 0.012080060162096387\nInput: [0 0], Prediction: [1]\nInput: [0 1], Prediction: [0]\nInput: [1 0], Prediction: [0]\nInput: [1 1], Prediction: [0]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of Artificial Neural Network</span>"
    ]
  },
  {
    "objectID": "intro.html#important-theorems-and-results",
    "href": "intro.html#important-theorems-and-results",
    "title": "1  Basics of Artificial Neural Network",
    "section": "1.3 Important Theorems and Results",
    "text": "1.3 Important Theorems and Results\n\nPerceptron Convergence Theorem: If the data is linearly separable, the perceptron will converge to a solution in a finite number of steps.\nUniversal Approximation Theorem: An MLP with a single hidden layer and non-linear activation functions can approximate any continuous function.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of Artificial Neural Network</span>"
    ]
  },
  {
    "objectID": "intro.html#examples",
    "href": "intro.html#examples",
    "title": "1  Basics of Artificial Neural Network",
    "section": "1.4 Examples",
    "text": "1.4 Examples\n\n1.4.1 Simple Example: Perceptron for AND Gate with native Python code\n\nimport numpy as np\n\n# Inputs (x1, x2) and outputs (y)\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([0, 0, 0, 1])  # AND Gate Output\n\n# Initialize weights and bias\nweights = np.random.rand(2)\nbias = np.random.rand(1)\nlearning_rate = 0.1\n\n# Activation function\ndef step_function(z):\n    return 1 if z &gt; 0 else 0\n\n# Training loop\nfor epoch in range(10):  # 10 epochs\n    for i in range(len(X)):\n        z = np.dot(X[i], weights) + bias\n        y_pred = step_function(z)\n        error = y[i] - y_pred\n        weights += learning_rate * error * X[i]\n        bias += learning_rate * error\n\nprint(f\"Trained Weights: {weights}, Bias: {bias}\")\n\nTrained Weights: [0.14429129 0.19358472], Bias: [-0.31827371]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of Artificial Neural Network</span>"
    ]
  },
  {
    "objectID": "intro.html#the-perceptron-and-the-xor-problem",
    "href": "intro.html#the-perceptron-and-the-xor-problem",
    "title": "1  Basics of Artificial Neural Network",
    "section": "1.5 The Perceptron and the XOR Problem",
    "text": "1.5 The Perceptron and the XOR Problem\nIn the early stages of neural network research, the perceptron was introduced as a simple model to classify linearly separable patterns. A perceptron consists of a single neuron, which receives input, applies weights, sums them up, and passes the result through an activation function to produce an output. For problems that can be linearly separated, a perceptron performs exceptionally well.\nHowever, the perceptron faced a significant limitation: it cannot solve problems that are not linearly separable. A classic example is the XOR (exclusive OR) gate. The XOR gate outputs 1 only when the inputs are different (i.e., for inputs (0,1) or (1,0)), and outputs 0 when the inputs are the same (i.e., for inputs (0,0) or (1,1)). This pattern is not linearly separable, meaning it cannot be solved by a single perceptron, as the data cannot be separated with a straight line.\n\n1.5.1 Why the Perceptron Fails at XOR\nFor the XOR gate, the following input-output pairs exist:\n\n(0, 0) → 0\n(0, 1) → 1\n(1, 0) → 1\n(1, 1) → 0\n\nIf you try to plot these points on a 2D plane, you’ll notice that no straight line can separate the 1 outputs from the 0 outputs, making the XOR problem an example of a non-linearly separable problem. This is where the perceptron fails.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of Artificial Neural Network</span>"
    ]
  },
  {
    "objectID": "intro.html#multi-layer-perceptron-mlp",
    "href": "intro.html#multi-layer-perceptron-mlp",
    "title": "1  Basics of Artificial Neural Network",
    "section": "1.6 Multi-Layer Perceptron (MLP)",
    "text": "1.6 Multi-Layer Perceptron (MLP)\nTo overcome the limitations of the perceptron, we need a more powerful model that can learn non-linear decision boundaries. The Multi-Layer Perceptron (MLP) addresses this limitation by introducing multiple layers of neurons. Unlike a single perceptron, an MLP has:\n\nInput Layer: Receives the input features (in this case, the two binary inputs of the XOR gate).\nHidden Layers: One or more layers of neurons that allow the network to learn complex, non-linear mappings from input to output.\nOutput Layer: Produces the final prediction (in this case, the XOR output).\n\nThe addition of hidden layers and the use of non-linear activation functions (such as sigmoid or ReLU) enables the MLP to learn and model non-linear relationships. This makes MLPs capable of solving the XOR problem, as the network can form non-linear decision boundaries.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of Artificial Neural Network</span>"
    ]
  },
  {
    "objectID": "intro.html#implementing-xor-with-mlp",
    "href": "intro.html#implementing-xor-with-mlp",
    "title": "1  Basics of Artificial Neural Network",
    "section": "1.9 Implementing XOR with MLP",
    "text": "1.9 Implementing XOR with MLP\nWith the MLP’s ability to model non-linearly separable data, we can now implement the XOR gate. The MLP will consist of:\n\nAn input layer that takes in two binary values.\nA hidden layer that helps in learning the non-linear relationship.\nAn output layer that produces the binary XOR result.\n\nWe will train the MLP on the four possible inputs for the XOR gate and use backpropagation to update the weights in the network, eventually enabling the MLP to predict the XOR output correctly.\nIn the next code implementation, we will implement the XOR gate using an MLP, demonstrating how the network can learn the XOR function through training.\n\nimport numpy as np\n\nclass Perceptron:\n    def __init__(self, input_dim, output_dim, activation=\"sigmoid\"):\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.activation = activation\n        self.weights = np.random.randn(input_dim, output_dim)\n        self.biases = np.zeros((1, output_dim))\n\n    def forward(self, inputs):\n        self.inputs = inputs  # Store inputs for later use in backpropagation\n        z = np.dot(inputs, self.weights) + self.biases\n        if self.activation == \"sigmoid\":\n            self.output = 1 / (1 + np.exp(-z))  # Store output for backpropagation\n            return self.output\n        elif self.activation == \"relu\":\n            self.output = np.maximum(0, z)\n            return self.output\n        else:  # Linear activation by default\n            self.output = z\n            return self.output\n\n    def backward(self, output_error, learning_rate):\n        if self.activation == \"sigmoid\":\n            activation_derivative = self.output * (1 - self.output)\n        elif self.activation == \"relu\":\n            activation_derivative = np.where(self.output &gt; 0, 1, 0)\n        else:  # Linear activation\n            activation_derivative = 1\n\n        delta = output_error * activation_derivative\n        input_error = np.dot(delta, self.weights.T)\n\n        # Update weights and biases\n        self.weights += learning_rate * np.dot(self.inputs.T, delta)\n        self.biases += learning_rate * np.sum(delta, axis=0, keepdims=True)\n\n        return input_error\n\nclass MLP:\n    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.1):\n        self.layers = [\n            Perceptron(input_dim, hidden_dim, activation=\"sigmoid\"),\n            Perceptron(hidden_dim, output_dim, activation=\"sigmoid\")\n        ]\n        self.learning_rate = learning_rate\n\n    def forward(self, X):\n        output = X\n        for layer in self.layers:\n            output = layer.forward(output)\n        return output\n\n    def train(self, X, y, epochs=10000):\n        for epoch in range(epochs):\n            # Forward pass\n            output = self.forward(X)\n\n            # Backward pass (backpropagation)\n            error = y - output\n            for layer in reversed(self.layers):\n                error = layer.backward(error, self.learning_rate)\n\n            if epoch % 1000 == 0:\n                loss = np.mean(np.square(y - self.forward(X)))\n                print(f'Epoch {epoch}, Loss: {loss}')\n\n    def predict(self, X):\n        return np.round(self.forward(X))\n\n        \ndef simulate_logic_gate(gate_type):\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    gate_targets = {\n        'XOR': np.array([[0], [1], [1], [0]])  # XOR output\n    }\n\n    if gate_type not in gate_targets:\n        print(f\"Unsupported gate type: {gate_type}\")\n        return\n\n    y = gate_targets[gate_type]\n\n    mlp = MLP(input_dim=2, hidden_dim=2, output_dim=1, learning_rate=0.1)\n    mlp.train(X, y, epochs=10000)\n\n    print(f\"Simulating {gate_type} gate\")\n    for input_data, prediction in zip(X, mlp.predict(X)):\n        print(f\"Input: {input_data}, Prediction: {prediction}\")\n\nsimulate_logic_gate('XOR')\n\nEpoch 0, Loss: 0.342650747517655\nEpoch 1000, Loss: 0.21995567921248071\nEpoch 2000, Loss: 0.1908413577352353\nEpoch 3000, Loss: 0.18015009457918718\nEpoch 4000, Loss: 0.17561482401540407\nEpoch 5000, Loss: 0.17323533417893028\nEpoch 6000, Loss: 0.1718017288602109\nEpoch 7000, Loss: 0.17085496961552293\nEpoch 8000, Loss: 0.1701880580279717\nEpoch 9000, Loss: 0.16969537604204576\nSimulating XOR gate\nInput: [0 0], Prediction: [0.]\nInput: [0 1], Prediction: [1.]\nInput: [1 0], Prediction: [1.]\nInput: [1 1], Prediction: [1.]\n\n\nAbove Python code creates a successful MLP with two (fixed) hidden layers that simulate the XOR gate. There is no golden rule to fix number of hidden layers in an MLP to solve a specific problem. So we have to restructure the OOPs architecture of MLP to handle its structure in more robust way. In this approach, the user can ‘add’ as many layers as he wish to develop a required MLP architecture. Following code will do this job.\n\nimport numpy as np\n\nclass Layer:\n    def __init__(self, output_dim, activation=\"sigmoid\"):\n        self.output_dim = output_dim\n        self.activation = activation\n        self.weights = None  # Will be initialized later\n        self.biases = None    # Will be initialized later\n        self.inputs = None   # Will store inputs for backpropagation\n\n    def forward(self, inputs):\n        self.inputs = inputs  # Store inputs for backpropagation\n        z = np.dot(inputs, self.weights) + self.biases\n        if self.activation == \"sigmoid\":\n            self.output = 1 / (1 + np.exp(-z))  # Store output for backpropagation\n            return self.output\n        elif self.activation == \"relu\":\n            self.output = np.maximum(0, z)\n            return self.output\n        else:  # Linear activation by default\n            self.output = z\n            return self.output\n\n    def backward(self, output_error, learning_rate):\n        if self.activation == \"sigmoid\":\n            activation_derivative = self.output * (1 - self.output)\n        elif self.activation == \"relu\":\n            activation_derivative = np.where(self.output &gt; 0, 1, 0)\n        else:  # Linear activation\n            activation_derivative = 1\n\n        delta = output_error * activation_derivative\n        input_error = np.dot(delta, self.weights.T)\n\n        # Update weights and biases\n        self.weights += learning_rate * np.dot(self.inputs.T, delta)\n        self.biases += learning_rate * np.sum(delta, axis=0, keepdims=True)\n\n        return input_error\n\nclass MLP:\n    def __init__(self, input_dim, learning_rate=0.1):\n        self.input_dim = input_dim\n        self.learning_rate = learning_rate\n        self.layers = []\n\n    def add(self, layer):\n        self.layers.append(layer)\n\n    def compile(self):\n        # Initialize weights and biases for each layer\n        prev_dim = self.input_dim\n        for layer in self.layers:\n            layer.weights = np.random.randn(prev_dim, layer.output_dim)\n            layer.biases = np.zeros((1, layer.output_dim))\n            prev_dim = layer.output_dim\n\n    def forward(self, X):\n        output = X\n        for layer in self.layers:\n            output = layer.forward(output)\n        return output\n\n    def train(self, X, y, epochs=10000):\n        for epoch in range(epochs):\n            # Forward pass\n            output = self.forward(X)\n\n            # Backward pass (backpropagation)\n            error = y - output\n            for layer in reversed(self.layers):\n                error = layer.backward(error, self.learning_rate)\n\n            if epoch % 1000 == 0:\n                loss = np.mean(np.square(y - self.forward(X)))\n                print(f'Epoch {epoch}, Loss: {loss}')\n\n    def predict(self, X):\n        return np.round(self.forward(X))\n\ndef simulate_logic_gate(gate_type):\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    gate_targets = {\n        'AND': np.array([[0], [0], [0], [1]]),\n        'OR': np.array([[0], [1], [1], [1]]),\n        'NAND': np.array([[1], [1], [1], [0]]),\n        'NOR': np.array([[1], [0], [0], [0]]),\n        'XOR': np.array([[0], [1], [1], [0]])\n    }\n\n    if gate_type not in gate_targets:\n        print(f\"Unsupported gate type: {gate_type}\")\n        return\n\n    y = gate_targets[gate_type]\n\n    mlp = MLP(input_dim=2, learning_rate=0.1)\n    mlp.add(Layer(output_dim=2, activation=\"relu\"))  # Hidden layer with ReLU\n    mlp.add(Layer(output_dim=2, activation=\"sigmoid\"))\n    mlp.add(Layer(output_dim=1, activation=\"sigmoid\"))  # Output layer with sigmoid\n    mlp.compile()  # Initialize weights and biases\n    mlp.train(X, y, epochs=10000)\n\n    print(f\"Simulating {gate_type} gate\")\n    for input_data, prediction in zip(X, mlp.predict(X)):\n        print(f\"Input: {input_data}, Prediction: {prediction}\")\n\nsimulate_logic_gate('XOR')\n\nEpoch 0, Loss: 0.3116290768741562\nEpoch 1000, Loss: 0.17151834969259316\nEpoch 2000, Loss: 0.16793141526529137\nEpoch 3000, Loss: 0.16733031078530863\nEpoch 4000, Loss: 0.16710681847909067\nEpoch 5000, Loss: 0.16699330559315\nEpoch 6000, Loss: 0.1669259198875758\nEpoch 7000, Loss: 0.16688051187530661\nEpoch 8000, Loss: 0.16684950624515743\nEpoch 9000, Loss: 0.1668250682504989\nSimulating XOR gate\nInput: [0 0], Prediction: [0.]\nInput: [0 1], Prediction: [0.]\nInput: [1 0], Prediction: [1.]\nInput: [1 1], Prediction: [0.]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of Artificial Neural Network</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "summary yet to be created.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Rosenblatt, Frank. 1957. The Perceptron, a Perceiving and\nRecognizing Automaton Project Para. Cornell Aeronautical\nLaboratory.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "intro.html#performance-metrics-a-critical-component-of-prediction-problems",
    "href": "intro.html#performance-metrics-a-critical-component-of-prediction-problems",
    "title": "1  Basics of Artificial Neural Network",
    "section": "1.10 Performance Metrics: A Critical Component of Prediction Problems",
    "text": "1.10 Performance Metrics: A Critical Component of Prediction Problems\nIn machine learning and predictive modeling, simply achieving a correct output is not sufficient to evaluate the effectiveness of a model. Various performance measures are essential for assessing the quality and reliability of predictions. These metrics provide a deeper understanding of how well the model performs across different aspects of a dataset.\n\n1.10.1 Confusion Matrix\nThe confusion matrix summarizes the results of a binary classification task. It includes:\n\nTrue Positives (TP): Correctly predicted positive cases.\n\nTrue Negatives (TN): Correctly predicted negative cases.\n\nFalse Positives (FP): Incorrectly predicted positive cases (Type I error).\n\nFalse Negatives (FN): Incorrectly predicted negative cases (Type II error).\n\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTP\nFN\n\n\nActual Negative\nFP\nTN\n\n\n\n\n\n1.10.2 Accuracy\nAccuracy is the proportion of correctly classified cases out of the total number of cases:\n\\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\nInterpretation: Accuracy is a straightforward measure but can be misleading for imbalanced datasets. For example, in fraud detection with a 99:1 class ratio, a model predicting all cases as non-fraud achieves 99% accuracy but fails to detect fraud.\n\n\n\n1.10.3 Precision\nPrecision measures the proportion of true positive predictions out of all positive predictions:\n\\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\]\nInterpretation: High precision indicates a low false positive rate, which is crucial in domains like medical diagnostics, where false alarms can lead to unnecessary procedures.\n\n\n\n1.10.4 Recall (Sensitivity)\nRecall (or sensitivity) is the proportion of actual positives correctly identified:\n\\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\]\nInterpretation: Recall is critical when false negatives are costly, such as in cancer detection.\n\n\n\n1.10.5 Specificity\nSpecificity measures the proportion of actual negatives correctly identified:\n\\[\n\\text{Specificity} = \\frac{TN}{TN + FP}\n\\]\nInterpretation: Specificity is important when the cost of false positives is high, such as in spam email detection.\n\n\n\n1.10.6 F1 Score\nThe F1 score combines precision and recall into a single metric using their harmonic mean:\n\\[\nF1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\nInterpretation: The F1 score is useful when there is a trade-off between precision and recall and provides a balanced view of model performance.\n\n\n\n1.10.7 Relation to Hypothesis Testing\nIn statistical hypothesis testing:\n\nType I Error: Rejecting the null hypothesis when it is true (analogous to FP).\n\nType II Error: Failing to reject the null hypothesis when it is false (analogous to FN).\n\n\n\n\n\n\n\n\n\nError Type\nConfusion Matrix Component\nHypothesis Testing Relation\n\n\n\n\nType I Error\nFP\nReject a true null hypothesis\n\n\nType II Error\nFN\nFail to reject a false null hypothesis\n\n\n\n\n\n1.10.8 Balancing Errors\nThe trade-off between Type I and Type II errors in hypothesis testing corresponds to optimizing precision and recall in machine learning. For instance, increasing sensitivity (recall) often reduces specificity, analogous to increasing the power of a test at the cost of a higher false positive rate.\n\n\n\n1.10.9 Choosing Metrics\n\nUse precision to minimize inconveniences caused by legitimate emails being flagged as spam.\n\nUse recall to ensure spam emails are effectively caught.\n\nOptimize the F1 score for a balanced performance.\n\n\n\n1.10.10 Interpretation of Results: Simulating XOR Gate with MLP as a classification problem\nNow let’s redesign the XOR gate problem as a classification problem. The Multilayer Perceptron (MLP) was tested on the XOR gate problem, producing the following predictions and performance metrics:\n\n\nimport numpy as np\n\nclass Layer:\n    def __init__(self, output_dim, activation=\"sigmoid\"):\n        self.output_dim = output_dim\n        self.activation = activation\n        self.weights = None  # Will be initialized later\n        self.biases = None    # Will be initialized later\n        self.inputs = None   # Will store inputs for backpropagation\n\n    def forward(self, inputs):\n        self.inputs = inputs  # Store inputs for backpropagation\n        z = np.dot(inputs, self.weights) + self.biases\n        if self.activation == \"sigmoid\":\n            self.output = 1 / (1 + np.exp(-z))  # Store output for backpropagation\n            return self.output\n        elif self.activation == \"relu\":\n            self.output = np.maximum(0, z)\n            return self.output\n        else:  # Linear activation by default\n            self.output = z\n            return self.output\n\n    def backward(self, output_error, learning_rate):\n        if self.activation == \"sigmoid\":\n            activation_derivative = self.output * (1 - self.output)\n        elif self.activation == \"relu\":\n            activation_derivative = np.where(self.output &gt; 0, 1, 0)\n        else:  # Linear activation\n            activation_derivative = 1\n\n        delta = output_error * activation_derivative\n        input_error = np.dot(delta, self.weights.T)\n\n        # Update weights and biases\n        self.weights += learning_rate * np.dot(self.inputs.T, delta)\n        self.biases += learning_rate * np.sum(delta, axis=0, keepdims=True)\n\n        return input_error\n\nclass MLP:\n    def __init__(self, input_dim, learning_rate=0.1):\n        self.input_dim = input_dim\n        self.learning_rate = learning_rate\n        self.layers = []\n\n    def add(self, layer):\n        self.layers.append(layer)\n\n    def compile(self):\n        # Initialize weights and biases for each layer\n        prev_dim = self.input_dim\n        for layer in self.layers:\n            layer.weights = np.random.randn(prev_dim, layer.output_dim)\n            layer.biases = np.zeros((1, layer.output_dim))\n            prev_dim = layer.output_dim\n\n    def forward(self, X):\n        output = X\n        for layer in self.layers:\n            output = layer.forward(output)\n        return output\n\n    def train(self, X, y, epochs=10000):\n        for epoch in range(epochs):\n            # Forward pass\n            output = self.forward(X)\n\n            # Backward pass (backpropagation)\n            error = y - output\n            for layer in reversed(self.layers):\n                error = layer.backward(error, self.learning_rate)\n\n            if epoch % 1000 == 0:\n                loss = np.mean(np.square(y - self.forward(X)))\n                print(f'Epoch {epoch}, Loss: {loss}')\n\n    def predict(self, X):\n        return np.round(self.forward(X))\n\ndef calculate_performance_metrics(y_true, y_pred):\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    TN = np.sum((y_true == 0) & (y_pred == 0))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n\n    accuracy = (TP + TN) / (TP + TN + FP + FN)\n    precision = TP / (TP + FP) if (TP + FP) &gt; 0 else 0\n    recall = TP / (TP + FN) if (TP + FN) &gt; 0 else 0\n    sensitivity = recall\n    specificity = TN / (TN + FP) if (TN + FP) &gt; 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0\n\n    print(\"Performance Metrics:\")\n    print(f\"Accuracy: {accuracy:.2f}\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall (Sensitivity): {sensitivity:.2f}\")\n    print(f\"Specificity: {specificity:.2f}\")\n    print(f\"F1 Score: {f1_score:.2f}\")\n\n    # Display the confusion matrix\n    print(\"\\nConfusion Matrix:\")\n    print(f\"                Predicted Positive    Predicted Negative\")\n    print(f\"Actual Positive          {TP}                    {FN}\")\n    print(f\"Actual Negative          {FP}                    {TN}\")\n\ndef simulate_logic_gate(gate_type):\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    gate_targets = {\n        'AND': np.array([[0], [0], [0], [1]]),\n        'OR': np.array([[0], [1], [1], [1]]),\n        'NAND': np.array([[1], [1], [1], [0]]),\n        'NOR': np.array([[1], [0], [0], [0]]),\n        'XOR': np.array([[0], [1], [1], [0]])\n    }\n\n    if gate_type not in gate_targets:\n        print(f\"Unsupported gate type: {gate_type}\")\n        return\n\n    y = gate_targets[gate_type]\n\n    mlp = MLP(input_dim=2, learning_rate=0.1)\n    mlp.add(Layer(output_dim=2, activation=\"relu\"))  # Hidden layer with ReLU\n    mlp.add(Layer(output_dim=2, activation=\"sigmoid\"))\n    mlp.add(Layer(output_dim=1, activation=\"sigmoid\"))  # Output layer with sigmoid\n    mlp.compile()  # Initialize weights and biases\n    mlp.train(X, y, epochs=10000)\n\n    print(f\"Simulating {gate_type} gate\")\n    y_pred = mlp.predict(X)\n\n    for input_data, prediction in zip(X, y_pred):\n        print(f\"Input: {input_data}, Prediction: {prediction}\")\n\n    # Calculate and print performance metrics\n    calculate_performance_metrics(y, y_pred)\n\nsimulate_logic_gate('XOR')\n\nEpoch 0, Loss: 0.24812851729575836\nEpoch 1000, Loss: 0.06256040570305801\nEpoch 2000, Loss: 0.00823773942460006\nEpoch 3000, Loss: 0.0038427121651104347\nEpoch 4000, Loss: 0.002427654419410833\nEpoch 5000, Loss: 0.0017544727845929323\nEpoch 6000, Loss: 0.0013665119361199157\nEpoch 7000, Loss: 0.0011158362295008249\nEpoch 8000, Loss: 0.0009413311744541015\nEpoch 9000, Loss: 0.0008131154872770428\nSimulating XOR gate\nInput: [0 0], Prediction: [0.]\nInput: [0 1], Prediction: [1.]\nInput: [1 0], Prediction: [1.]\nInput: [1 1], Prediction: [0.]\nPerformance Metrics:\nAccuracy: 1.00\nPrecision: 1.00\nRecall (Sensitivity): 1.00\nSpecificity: 1.00\nF1 Score: 1.00\n\nConfusion Matrix:\n                Predicted Positive    Predicted Negative\nActual Positive          2                    0\nActual Negative          0                    2\n\n\n\n1.10.10.1 Predictions\nThe predictions for the XOR gate are as follows:\n\nInput [0, 0]: Prediction [0.] (Correct)\nInput [0, 1]: Prediction [0.] (Incorrect)\nInput [1, 0]: Prediction [1.] (Correct)\nInput [1, 1]: Prediction [0.] (Incorrect)\n\nThe model correctly classified 3 out of 4 inputs, resulting in an accuracy of 75%.\n\n\n\n1.10.10.2 Performance Metrics\n\n\n\nMetric\nValue\n\n\n\n\nAccuracy\n0.75\n\n\nPrecision\n1.00\n\n\nRecall\n0.50\n\n\nSpecificity\n1.00\n\n\nF1 Score\n0.67\n\n\n\n\nAccuracy: Indicates the proportion of correct predictions. The model correctly classified 3 out of 4 inputs.\nPrecision: Perfectly predicted the single positive case without any false positives.\n\\[ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}} = \\frac{1}{1 + 0} = 1.0 \\]\nRecall: Captures the proportion of actual positives correctly identified. Out of 2 actual positives, only 1 was correctly predicted.\n\\[ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}} = \\frac{1}{1 + 1} = 0.5 \\]\nSpecificity: Perfectly identified all negative cases without any false positives.\n\\[ \\text{Specificity} = \\frac{\\text{TN}}{\\text{TN + FP}} = \\frac{2}{2 + 0} = 1.0 \\]\nF1 Score: Balances Precision and Recall, providing a comprehensive measure of model performance.\n\\[ \\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\cdot \\frac{1.0 \\cdot 0.5}{1.0 + 0.5} = 0.67 \\]\n\n\n\n\n1.10.10.3 Confusion Matrix\nThe confusion matrix provides a breakdown of the model’s predictions:\n\n\n\n\n\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\n1 (True Positive)\n1 (False Negative)\n\n\nActual Negative\n0 (False Positive)\n2 (True Negative)\n\n\n\n\n\n\n1.10.10.4 Key Observations\n\nStrengths:\n\nThe model demonstrates excellent precision and specificity, correctly predicting all negative cases without any false positives.\nIt has an overall accuracy of 75%, showing its general capability to classify XOR gate outputs.\n\nLimitations:\n\nThe model struggles with Recall, as it misclassifies 1 out of 2 actual positives (false negative).\nThe F1 Score of 0.67 indicates a moderate balance between Precision and Recall, but it reflects room for improvement in handling positive cases.\n\n\n\n\n\n1.10.10.5 Skill Assessment of the MLP\nThe XOR gate problem is a classic non-linear function. While MLPs are theoretically capable of solving such problems, the performance here suggests potential limitations:\n\nModel Architecture: The MLP may need more layers or neurons to fully capture XOR’s non-linear behavior.\nTraining Process: Adjustments to hyperparameters like learning rate, epochs, or weight initialization might enhance performance.\n\n\n\n\n1.10.10.6 Relation to Hypothesis Testing\nThe results can be related to statistical hypothesis testing concepts:\n\nType I Error (False Positive): No such errors occurred in this case.\nType II Error (False Negative): The model failed to identify 1 actual positive case, corresponding to a Type II Error.\n\n\nThis MLP exhibits reasonable performance with an accuracy of 75% but requires further refinement to improve its ability to generalize the XOR gate’s non-linear nature, particularly in predicting positive outputs. Improvements in architecture and training can likely address these limitations.\n\n\n\n\nRosenblatt, Frank. 1957. The Perceptron, a Perceiving and Recognizing Automaton Project Para. Cornell Aeronautical Laboratory.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of Artificial Neural Network</span>"
    ]
  },
  {
    "objectID": "intro.html#practical-example-in-ml",
    "href": "intro.html#practical-example-in-ml",
    "title": "1  Basics of Artificial Neural Network",
    "section": "1.9 Practical Example in ML",
    "text": "1.9 Practical Example in ML\nConsider a spam email detection system:\n\nTP: Emails correctly identified as spam.\n\nTN: Emails correctly identified as not spam.\n\nFP (Type I Error): Legitimate emails flagged as spam.\n\nFN (Type II Error): Spam emails missed as not spam.\n\n\n1.9.1 Choosing Metrics\n\nUse precision to minimize inconveniences caused by legitimate emails being flagged as spam.\n\nUse recall to ensure spam emails are effectively caught.\n\nOptimize the F1 score for a balanced performance.\n\nThis structured explanation links theoretical performance measures to practical applications and connects machine learning outcomes to foundational statistical principles, ensuring a clear understanding of their importance.\n\nimport numpy as np\n\nclass Layer:\n    def __init__(self, output_dim, activation=\"sigmoid\"):\n        self.output_dim = output_dim\n        self.activation = activation\n        self.weights = None  # Will be initialized later\n        self.biases = None    # Will be initialized later\n        self.inputs = None   # Will store inputs for backpropagation\n\n    def forward(self, inputs):\n        self.inputs = inputs  # Store inputs for backpropagation\n        z = np.dot(inputs, self.weights) + self.biases\n        if self.activation == \"sigmoid\":\n            self.output = 1 / (1 + np.exp(-z))  # Store output for backpropagation\n            return self.output\n        elif self.activation == \"relu\":\n            self.output = np.maximum(0, z)\n            return self.output\n        else:  # Linear activation by default\n            self.output = z\n            return self.output\n\n    def backward(self, output_error, learning_rate):\n        if self.activation == \"sigmoid\":\n            activation_derivative = self.output * (1 - self.output)\n        elif self.activation == \"relu\":\n            activation_derivative = np.where(self.output &gt; 0, 1, 0)\n        else:  # Linear activation\n            activation_derivative = 1\n\n        delta = output_error * activation_derivative\n        input_error = np.dot(delta, self.weights.T)\n\n        # Update weights and biases\n        self.weights += learning_rate * np.dot(self.inputs.T, delta)\n        self.biases += learning_rate * np.sum(delta, axis=0, keepdims=True)\n\n        return input_error\n\nclass MLP:\n    def __init__(self, input_dim, learning_rate=0.1):\n        self.input_dim = input_dim\n        self.learning_rate = learning_rate\n        self.layers = []\n\n    def add(self, layer):\n        self.layers.append(layer)\n\n    def compile(self):\n        # Initialize weights and biases for each layer\n        prev_dim = self.input_dim\n        for layer in self.layers:\n            layer.weights = np.random.randn(prev_dim, layer.output_dim)\n            layer.biases = np.zeros((1, layer.output_dim))\n            prev_dim = layer.output_dim\n\n    def forward(self, X):\n        output = X\n        for layer in self.layers:\n            output = layer.forward(output)\n        return output\n\n    def train(self, X, y, epochs=10000):\n        for epoch in range(epochs):\n            # Forward pass\n            output = self.forward(X)\n\n            # Backward pass (backpropagation)\n            error = y - output\n            for layer in reversed(self.layers):\n                error = layer.backward(error, self.learning_rate)\n\n            if epoch % 1000 == 0:\n                loss = np.mean(np.square(y - self.forward(X)))\n                print(f'Epoch {epoch}, Loss: {loss}')\n\n    def predict(self, X):\n        return np.round(self.forward(X))\n\ndef calculate_performance_metrics(y_true, y_pred):\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    TN = np.sum((y_true == 0) & (y_pred == 0))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n\n    accuracy = (TP + TN) / (TP + TN + FP + FN)\n    precision = TP / (TP + FP) if (TP + FP) &gt; 0 else 0\n    recall = TP / (TP + FN) if (TP + FN) &gt; 0 else 0\n    sensitivity = recall\n    specificity = TN / (TN + FP) if (TN + FP) &gt; 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0\n\n    print(\"Performance Metrics:\")\n    print(f\"Accuracy: {accuracy:.2f}\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall (Sensitivity): {sensitivity:.2f}\")\n    print(f\"Specificity: {specificity:.2f}\")\n    print(f\"F1 Score: {f1_score:.2f}\")\n\n    # Display the confusion matrix\n    print(\"\\nConfusion Matrix:\")\n    print(f\"                Predicted Positive    Predicted Negative\")\n    print(f\"Actual Positive          {TP}                    {FN}\")\n    print(f\"Actual Negative          {FP}                    {TN}\")\n\ndef simulate_logic_gate(gate_type):\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    gate_targets = {\n        'AND': np.array([[0], [0], [0], [1]]),\n        'OR': np.array([[0], [1], [1], [1]]),\n        'NAND': np.array([[1], [1], [1], [0]]),\n        'NOR': np.array([[1], [0], [0], [0]]),\n        'XOR': np.array([[0], [1], [1], [0]])\n    }\n\n    if gate_type not in gate_targets:\n        print(f\"Unsupported gate type: {gate_type}\")\n        return\n\n    y = gate_targets[gate_type]\n\n    mlp = MLP(input_dim=2, learning_rate=0.1)\n    mlp.add(Layer(output_dim=2, activation=\"relu\"))  # Hidden layer with ReLU\n    mlp.add(Layer(output_dim=2, activation=\"sigmoid\"))\n    mlp.add(Layer(output_dim=1, activation=\"sigmoid\"))  # Output layer with sigmoid\n    mlp.compile()  # Initialize weights and biases\n    mlp.train(X, y, epochs=10000)\n\n    print(f\"Simulating {gate_type} gate\")\n    y_pred = mlp.predict(X)\n\n    for input_data, prediction in zip(X, y_pred):\n        print(f\"Input: {input_data}, Prediction: {prediction}\")\n\n    # Calculate and print performance metrics\n    calculate_performance_metrics(y, y_pred)\n\nsimulate_logic_gate('XOR')\n\nEpoch 0, Loss: 0.2737422830382465\nEpoch 1000, Loss: 0.173410874486365\nEpoch 2000, Loss: 0.16791662156217388\nEpoch 3000, Loss: 0.1672727783756372\nEpoch 4000, Loss: 0.1670525072628475\nEpoch 5000, Loss: 0.1669478115105706\nEpoch 6000, Loss: 0.1668869574459088\nEpoch 7000, Loss: 0.16684513140576931\nEpoch 8000, Loss: 0.16681694324627952\nEpoch 9000, Loss: 0.1667968311721717\nSimulating XOR gate\nInput: [0 0], Prediction: [0.]\nInput: [0 1], Prediction: [0.]\nInput: [1 0], Prediction: [1.]\nInput: [1 1], Prediction: [0.]\nPerformance Metrics:\nAccuracy: 0.75\nPrecision: 1.00\nRecall (Sensitivity): 0.50\nSpecificity: 1.00\nF1 Score: 0.67\n\nConfusion Matrix:\n                Predicted Positive    Predicted Negative\nActual Positive          1                    1\nActual Negative          0                    2\n\n\n\n\n1.9.2 Interpretation of Results: Simulating XOR Gate with MLP\nThe Multilayer Perceptron (MLP) was tested on the XOR gate problem, producing the following predictions and performance metrics:\n\n\n1.9.2.1 Predictions\nThe predictions for the XOR gate are as follows: - Input [0, 0]: Prediction [0.] (Correct) - Input [0, 1]: Prediction [0.] (Incorrect) - Input [1, 0]: Prediction [1.] (Correct) - Input [1, 1]: Prediction [0.] (Incorrect)\nThe model correctly classified 3 out of 4 inputs, resulting in an accuracy of 75%.\n\n\n\n1.9.2.2 Performance Metrics\n\n\n\nMetric\nValue\n\n\n\n\nAccuracy\n0.75\n\n\nPrecision\n1.00\n\n\nRecall\n0.50\n\n\nSpecificity\n1.00\n\n\nF1 Score\n0.67\n\n\n\n\nAccuracy: Indicates the proportion of correct predictions. The model correctly classified 3 out of 4 inputs.\nPrecision: Perfectly predicted the single positive case without any false positives.\n\\[ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}} = \\frac{1}{1 + 0} = 1.0 \\]\nRecall: Captures the proportion of actual positives correctly identified. Out of 2 actual positives, only 1 was correctly predicted.\n\\[ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}} = \\frac{1}{1 + 1} = 0.5 \\]\nSpecificity: Perfectly identified all negative cases without any false positives.\n\\[ \\text{Specificity} = \\frac{\\text{TN}}{\\text{TN + FP}} = \\frac{2}{2 + 0} = 1.0 \\]\nF1 Score: Balances Precision and Recall, providing a comprehensive measure of model performance.\n\\[ \\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\cdot \\frac{1.0 \\cdot 0.5}{1.0 + 0.5} = 0.67 \\]\n\n\n\n\n1.9.2.3 Confusion Matrix\nThe confusion matrix provides a breakdown of the model’s predictions:\n\n\n\n\n\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\n1 (True Positive)\n1 (False Negative)\n\n\nActual Negative\n0 (False Positive)\n2 (True Negative)\n\n\n\n\n\n\n1.9.2.4 Key Observations\n\nStrengths:\n\nThe model demonstrates excellent precision and specificity, correctly predicting all negative cases without any false positives.\nIt has an overall accuracy of 75%, showing its general capability to classify XOR gate outputs.\n\nLimitations:\n\nThe model struggles with Recall, as it misclassifies 1 out of 2 actual positives (false negative).\nThe F1 Score of 0.67 indicates a moderate balance between Precision and Recall, but it reflects room for improvement in handling positive cases.\n\n\n\n\n\n1.9.2.5 Skill Assessment of the MLP\nThe XOR gate problem is a classic non-linear function. While MLPs are theoretically capable of solving such problems, the performance here suggests potential limitations: - Model Architecture: The MLP may need more layers or neurons to fully capture XOR’s non-linear behavior. - Training Process: Adjustments to hyperparameters like learning rate, epochs, or weight initialization might enhance performance.\n\n\n\n1.9.2.6 Relation to Hypothesis Testing\nThe results can be related to statistical hypothesis testing concepts: - Type I Error (False Positive): No such errors occurred in this case. - Type II Error (False Negative): The model failed to identify 1 actual positive case, corresponding to a Type II Error.\n\nThis MLP exhibits reasonable performance with an accuracy of 75% but requires further refinement to improve its ability to generalize the XOR gate’s non-linear nature, particularly in predicting positive outputs. Improvements in architecture and training can likely address these limitations.\n\n\n\n\nRosenblatt, Frank. 1957. The Perceptron, a Perceiving and Recognizing Automaton Project Para. Cornell Aeronautical Laboratory.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of Artificial Neural Network</span>"
    ]
  },
  {
    "objectID": "benchmarkDL.html",
    "href": "benchmarkDL.html",
    "title": "2  Benchmarking Deep Learning with Libraries",
    "section": "",
    "text": "2.1 Transitioning from Custom MLP Architecture to Keras for Generalized Applications\nThe custom Multi-Layer Perceptron (MLP) class architecture implemented earlier provides a solid foundational understanding of how neural networks function, including layer definitions, activation mechanisms, forward and backward propagation, and performance evaluation. However, when scaling these implementations to generalizable and complex applications, certain limitations of such custom approaches become evident. This section introduces Keras, a widely adopted library for building and benchmarking Artificial Neural Networks (ANNs), highlighting its relevance as a practical alternative to custom implementations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Benchmarking Deep Learning with Libraries</span>"
    ]
  },
  {
    "objectID": "benchmarkDL.html#limitations-of-the-custom-mlp-class-architecture",
    "href": "benchmarkDL.html#limitations-of-the-custom-mlp-class-architecture",
    "title": "2  Benchmarking Deep Learning with Libraries",
    "section": "2.2 Limitations of the Custom MLP Class Architecture",
    "text": "2.2 Limitations of the Custom MLP Class Architecture\nWhile the MLP class offers flexibility and transparency for learning purposes, its limitations include:\n\n2.2.1 Scalability\n\nThe manual initialization of weights, biases, and layer-specific computations can become cumbersome as the network depth and size increase.\nHandling large datasets or multiple training tasks requires additional optimization techniques that are non-trivial to implement manually.\n\n\n\n2.2.2 Optimization Challenges\n\nImplementing advanced optimizers like RMSprop, Adam, or adaptive gradient methods demands significant coding effort.\nFeatures like learning rate scheduling and early stopping require extensive additional logic.\n\n\n\n2.2.3 Performance Bottlenecks\n\nThe current design lacks GPU acceleration, limiting its applicability to computationally intensive tasks.\nDebugging and profiling performance manually can be error-prone and time-consuming.\n\n\n\n2.2.4 Generalization Issues\n\nWhile sufficient for specific tasks like XOR gate simulations, the architecture lacks modularity for handling diverse, general-purpose applications such as image recognition or text classification.\nIntegration with modern research architectures, such as convolutional or recurrent networks, is challenging.\n\n\n\n2.2.5 Limited Ecosystem Support\nCustom implementations do not leverage pre-trained models, a key requirement for applications in transfer learning and fine-tuning.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Benchmarking Deep Learning with Libraries</span>"
    ]
  },
  {
    "objectID": "benchmarkDL.html#keras-a-practical-alternative-for-generalized-applications",
    "href": "benchmarkDL.html#keras-a-practical-alternative-for-generalized-applications",
    "title": "2  Benchmarking Deep Learning with Libraries",
    "section": "2.3 Keras: A Practical Alternative for Generalized Applications",
    "text": "2.3 Keras: A Practical Alternative for Generalized Applications\nKeras is a neural network Application Programming Interface (API) for Python that is tightly integrated with TensorFlow, which is used to build machine learning models. Keras’ models offer a simple, user-friendly way to define a neural network, which will then be built for you by TensorFlow.\n\n2.3.1 Why Keras?\nKeras addresses the limitations of from the scratch architecture by providing a high-level, modular, and extensible framework built on top of TensorFlow. Here’s how Keras improves upon the custom MLP class:\nEase of Use:\n\nPre-built layers and optimizers simplify network creation without compromising flexibility.\nAPIs for common tasks, such as dataset preprocessing, model saving, and loading, minimize boilerplate code.\n\nScalability and Performance:\n\nSupport for GPUs and TPUs ensures computational efficiency, especially for large datasets and deep architectures.\nBuilt-in profiling tools facilitate real-time performance monitoring and debugging.\n\nModularity for Advanced Architectures:\n\nKeras supports a wide range of network types, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Transformers, which are impractical to implement from scratch.\nThe library integrates seamlessly with TensorFlow’s ecosystem, allowing access to tools like TensorBoard for visualization.\n\nPre-trained Models and Ecosystem:\n\nKeras offers access to a library of state-of-the-art pre-trained models, enabling rapid prototyping and transfer learning.\nThe ecosystem includes community support and extensive documentation, enhancing usability and troubleshooting.\n\n\n\n\n\n\n\nWhat’s the Difference Between Tensorflow and Keras?\n\n\n\nTensorFlow is an open-source set of libraries for creating and working with neural networks, such as those used in Machine Learning (ML) and Deep Learning projects.\nKeras, on the other hand, is a high-level API that runs on top of TensorFlow. Keras simplifies the implementation of complex neural networks with its easy to use framework.\n\n\n\nTensorflow vs keras\n\n\n\n\n\n\n\n\n\n\nWhen to Use Keras vs TensorFlow\n\n\n\nTensorFlow provides a comprehensive machine learning platform that offers both high level and low level capabilities for building and deploying machine learning models. However, it does have a steep learning curve. It’s best used when you have a need for:\n\nDeep learning research\nComplex neural networks\nWorking with large datasets\nHigh performance models\n\nKeras, on the other hand, is perfect for those that do not have a strong background in Deep Learning, but still want to work with neural networks. Using Keras, you can build a neural network model quickly and easily using minimal code, allowing for rapid prototyping. For example:\n# Import the Keras libraries required in this example: \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n# Create a Sequential model: \nmodel = Sequential()\n# Add layers with the add() method: \nmodel.add(Dense(32, input_dim=784)) \nmodel.add(Activation('relu'))\nKeras is less error prone than TensorFlow, and models are more likely to be accurate with Keras than with TensorFlow. This is because Keras operates within the limitations of its framework, which include:\n\nComputation speed: Keras sacrifices speed for user-friendliness.\nLow-level Errors: sometimes you’ll get TensorFlow backend error messages that Keras was not designed to handle.\nAlgorithm Support: Keras is not well suited for working with certain basic machine learning algorithms and models like clustering and Principal Component Analysis (PCM).\nDynamic Charts: Keras has no support for dynamic chart creation.\n\n\n\n\n\n2.3.2 Keras Model Overview\nModels are the central entities in Keras, enabling the definition of TensorFlow neural networks by specifying attributes, functions, and layers. Keras provides multiple APIs for designing neural networks, catering to varying levels of complexity and use cases:\n\nSequential API:\n\nAllows building models layer by layer, suitable for most straightforward problems.\nProvides a simple list-based structure but is restricted to single-input, single-output stacks of layers.\n\nFunctional API:\n\nA comprehensive and flexible API supporting arbitrary model architectures.\nIdeal for creating complex models with multiple inputs, outputs, or shared layers.\n\nModel Subclassing:\n\nEnables implementing models from scratch by subclassing the base Model class.\nPrimarily used for research or highly specialized applications, though rarely needed for typical use cases.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Benchmarking Deep Learning with Libraries</span>"
    ]
  },
  {
    "objectID": "benchmarkDL.html#transition-example-simulating-xor-gate-with-keras",
    "href": "benchmarkDL.html#transition-example-simulating-xor-gate-with-keras",
    "title": "2  Benchmarking Deep Learning with Libraries",
    "section": "2.4 Transition Example: Simulating XOR Gate with Keras",
    "text": "2.4 Transition Example: Simulating XOR Gate with Keras\nRevisiting the XOR gate example, here’s how it can be implemented using Keras:\n\nModel definition\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Dense(4, input_dim=2, activation='sigmoid'))\nmodel.add(Dense(16, activation='sigmoid'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nC:\\Users\\SIJUKSWAMY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                   │ (None, 4)              │            12 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 16)             │            80 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 1)              │            17 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 109 (436.00 B)\n\n\n\n Trainable params: 109 (436.00 B)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nModel compilation:\n\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n\nLoading data:\n\n\nimport numpy as np\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny=np.array([[0], [1], [1], [0]])\n\n\nModel training:\n\n\nmodel.fit(X, y, epochs=100, batch_size=2)\n\nEpoch 1/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 1s 1s/step - accuracy: 0.5000 - loss: 0.7152\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 1s 26ms/step - accuracy: 0.5000 - loss: 0.7158\nEpoch 2/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - accuracy: 0.5000 - loss: 0.7138\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.5000 - loss: 0.7137\nEpoch 3/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.5000 - loss: 0.7102\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - accuracy: 0.5000 - loss: 0.7110\nEpoch 4/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.5000 - loss: 0.7080\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.5000 - loss: 0.7088\nEpoch 5/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.7083\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5000 - loss: 0.7075\nEpoch 6/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 44ms/step - accuracy: 0.5000 - loss: 0.7064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.5000 - loss: 0.7057\nEpoch 7/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - accuracy: 0.5000 - loss: 0.7022\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.7032\nEpoch 8/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.0000e+00 - loss: 0.8403\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.3333 - loss: 0.7499    \nEpoch 9/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.0000e+00 - loss: 0.8320\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step - accuracy: 0.3333 - loss: 0.7460    \nEpoch 10/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 50ms/step - accuracy: 0.5000 - loss: 0.7017\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 51ms/step - accuracy: 0.5000 - loss: 0.7011\nEpoch 11/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 110ms/step - accuracy: 0.5000 - loss: 0.6997\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - accuracy: 0.5000 - loss: 0.7001 \nEpoch 12/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step - accuracy: 0.5000 - loss: 0.6980\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - accuracy: 0.5000 - loss: 0.6993\nEpoch 13/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - accuracy: 0.5000 - loss: 0.7005\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6999\nEpoch 14/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - accuracy: 1.0000 - loss: 0.5871\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.6667 - loss: 0.6618\nEpoch 15/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5000 - loss: 0.6969\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - accuracy: 0.5000 - loss: 0.6983\nEpoch 16/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 1.0000 - loss: 0.5916\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.6667 - loss: 0.6629\nEpoch 17/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - accuracy: 0.5000 - loss: 0.6964\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6978\nEpoch 18/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6960\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6974\nEpoch 19/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6989\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.5000 - loss: 0.6982\nEpoch 20/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.0000e+00 - loss: 0.7941\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - accuracy: 0.3333 - loss: 0.7305    \nEpoch 21/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - accuracy: 0.5000 - loss: 0.6983\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.5000 - loss: 0.6976\nEpoch 22/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6978\nEpoch 23/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - accuracy: 0.0000e+00 - loss: 0.7848\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.3333 - loss: 0.7269    \nEpoch 24/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - accuracy: 0.5000 - loss: 0.6989\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step - accuracy: 0.5000 - loss: 0.6973\nEpoch 25/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - accuracy: 0.5000 - loss: 0.6974\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6967\nEpoch 26/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.0000e+00 - loss: 0.7770\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - accuracy: 0.3333 - loss: 0.7238    \nEpoch 27/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6949\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6956\nEpoch 28/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6982\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - accuracy: 0.5000 - loss: 0.6966\nEpoch 29/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - accuracy: 0.5000 - loss: 0.6968\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6961\nEpoch 30/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - accuracy: 0.5000 - loss: 0.6952\nEpoch 31/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5000 - loss: 0.6979\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.5000 - loss: 0.6963\nEpoch 32/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 1.0000 - loss: 0.6262\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.6667 - loss: 0.6724\nEpoch 33/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6929\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - accuracy: 0.5000 - loss: 0.6945\nEpoch 34/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - accuracy: 0.5000 - loss: 0.6941\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6948\nEpoch 35/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.5000 - loss: 0.6962\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.5000 - loss: 0.6954\nEpoch 36/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 1.0000 - loss: 0.6320\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.6667 - loss: 0.6741\nEpoch 37/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - accuracy: 0.5000 - loss: 0.6973\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step - accuracy: 0.5000 - loss: 0.6957\nEpoch 38/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 53ms/step - accuracy: 1.0000 - loss: 0.6344\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - accuracy: 0.6667 - loss: 0.6747\nEpoch 39/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - accuracy: 1.0000 - loss: 0.6351\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.6667 - loss: 0.6749\nEpoch 40/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.5000 - loss: 0.6923\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6939\nEpoch 41/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - accuracy: 0.5000 - loss: 0.6958\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.5000 - loss: 0.6950\nEpoch 42/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.5000 - loss: 0.6920\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.5000 - loss: 0.6937\nEpoch 43/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.0000e+00 - loss: 0.7479\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.3333 - loss: 0.7128    \nEpoch 44/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6955\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.5000 - loss: 0.6946\nEpoch 45/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.5000 - loss: 0.6954\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - accuracy: 0.5000 - loss: 0.6946\nEpoch 46/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 39ms/step - accuracy: 0.0000e+00 - loss: 0.7424\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 48ms/step - accuracy: 0.3333 - loss: 0.7108    \nEpoch 47/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.5000 - loss: 0.6965\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - accuracy: 0.5000 - loss: 0.6948\nEpoch 48/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 38ms/step - accuracy: 0.0000e+00 - loss: 0.7391\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - accuracy: 0.3333 - loss: 0.7095    \nEpoch 49/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 0.5000 - loss: 0.6951\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6943\nEpoch 50/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6926\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.5000 - loss: 0.6934\nEpoch 51/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5000 - loss: 0.6963\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - accuracy: 0.5000 - loss: 0.6947\nEpoch 52/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step - accuracy: 0.5000 - loss: 0.6950\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6942\nEpoch 53/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6950\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - accuracy: 0.5000 - loss: 0.6942\nEpoch 54/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - accuracy: 0.0000e+00 - loss: 0.7326\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 37ms/step - accuracy: 0.3333 - loss: 0.7072    \nEpoch 55/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - accuracy: 0.0000e+00 - loss: 0.7307\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - accuracy: 0.3333 - loss: 0.7064    \nEpoch 56/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - accuracy: 0.5000 - loss: 0.6923\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.5000 - loss: 0.6931\nEpoch 57/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.0000e+00 - loss: 0.7285\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.3333 - loss: 0.7056    \nEpoch 58/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - accuracy: 0.5000 - loss: 0.6922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - accuracy: 0.5000 - loss: 0.6931\nEpoch 59/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - accuracy: 1.0000 - loss: 0.6605\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.6667 - loss: 0.6828\nEpoch 60/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - accuracy: 0.5000 - loss: 0.6909\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step - accuracy: 0.5000 - loss: 0.6926\nEpoch 61/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 1.0000 - loss: 0.6607\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step - accuracy: 0.6667 - loss: 0.6828\nEpoch 62/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.5000 - loss: 0.6922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5000 - loss: 0.6931\nEpoch 63/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - accuracy: 0.0000e+00 - loss: 0.7257\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.3333 - loss: 0.7047    \nEpoch 64/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - accuracy: 0.5000 - loss: 0.6947\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.5000 - loss: 0.6938\nEpoch 65/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - accuracy: 0.5000 - loss: 0.6960\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6943\nEpoch 66/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6947\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5000 - loss: 0.6938\nEpoch 67/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - accuracy: 0.5000 - loss: 0.6907\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6925\nEpoch 68/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - accuracy: 1.0000 - loss: 0.6654\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.6667 - loss: 0.6843\nEpoch 69/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 1.0000 - loss: 0.6650\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step - accuracy: 0.6667 - loss: 0.6841\nEpoch 70/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - accuracy: 0.5000 - loss: 0.6959\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6942\nEpoch 71/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - accuracy: 0.5000 - loss: 0.6920\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.5000 - loss: 0.6929\nEpoch 72/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6907\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6924\nEpoch 73/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - accuracy: 0.5000 - loss: 0.6946\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.5000 - loss: 0.6937\nEpoch 74/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.5000 - loss: 0.6919\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - accuracy: 0.5000 - loss: 0.6928\nEpoch 75/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - accuracy: 0.5000 - loss: 0.6959\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step - accuracy: 0.5000 - loss: 0.6941\nEpoch 76/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - accuracy: 0.5000 - loss: 0.6946\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6937\nEpoch 77/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 1.0000 - loss: 0.6702\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.6667 - loss: 0.6858\nEpoch 78/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - accuracy: 0.5000 - loss: 0.6958\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6941\nEpoch 79/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 1.0000 - loss: 0.6707\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - accuracy: 0.6667 - loss: 0.6859\nEpoch 80/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - accuracy: 1.0000 - loss: 0.6705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.6667 - loss: 0.6858\nEpoch 81/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6958\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - accuracy: 0.5000 - loss: 0.6941\nEpoch 82/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.5000 - loss: 0.6918\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6927\nEpoch 83/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 49ms/step - accuracy: 0.5000 - loss: 0.6958\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 38ms/step - accuracy: 0.5000 - loss: 0.6940\nEpoch 84/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 129ms/step - accuracy: 0.5000 - loss: 0.6905\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - accuracy: 0.5000 - loss: 0.6923 \nEpoch 85/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.5000 - loss: 0.6905\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6922\nEpoch 86/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.5000 - loss: 0.6945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 45ms/step - accuracy: 0.5000 - loss: 0.6936\nEpoch 87/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6917\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 50ms/step - accuracy: 0.5000 - loss: 0.6926\nEpoch 88/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - accuracy: 0.0000e+00 - loss: 0.7106\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - accuracy: 0.3333 - loss: 0.6994    \nEpoch 89/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 48ms/step - accuracy: 0.5000 - loss: 0.6957\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - accuracy: 0.5000 - loss: 0.6939\nEpoch 90/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 45ms/step - accuracy: 0.5000 - loss: 0.6945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 39ms/step - accuracy: 0.5000 - loss: 0.6935\nEpoch 91/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 39ms/step - accuracy: 0.5000 - loss: 0.6904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.5000 - loss: 0.6922\nEpoch 92/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.5000 - loss: 0.6945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6935\nEpoch 93/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 59ms/step - accuracy: 0.5000 - loss: 0.6916\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.5000 - loss: 0.6926\nEpoch 94/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 1.0000 - loss: 0.6787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.6667 - loss: 0.6886\nEpoch 95/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 120ms/step - accuracy: 0.5000 - loss: 0.6904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 39ms/step - accuracy: 0.5000 - loss: 0.6921 \nEpoch 96/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - accuracy: 0.5000 - loss: 0.6945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 50ms/step - accuracy: 0.5000 - loss: 0.6935\nEpoch 97/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - accuracy: 0.5000 - loss: 0.6957\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - accuracy: 0.5000 - loss: 0.6939\nEpoch 98/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 1.0000 - loss: 0.6796\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.6667 - loss: 0.6888\nEpoch 99/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - accuracy: 0.5000 - loss: 0.6957\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step - accuracy: 0.5000 - loss: 0.6939\nEpoch 100/100\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - accuracy: 0.5000 - loss: 0.6945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - accuracy: 0.5000 - loss: 0.6935\n\n\n&lt;keras.src.callbacks.history.History at 0x25cec8ae960&gt;\n\n\n\nModel evaluation:\n\n\nloss, accuracy = model.evaluate(X, y)\nprint(f\"Accuracy: {accuracy}\")\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 182ms/step - accuracy: 0.5000 - loss: 0.6930\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 199ms/step - accuracy: 0.5000 - loss: 0.6930\nAccuracy: 0.5\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy using Keras, the manual implementation steps (e.g., weight initialization, forward/backward propagation) are abstracted, focusing instead on defining and optimizing the architecture. In this approach, the key words are: compile– fit–evaluate–predict.\n\n\n\n2.4.1 Performance evaluation of the keras XOR gate model\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n# Predict the outputs\ny_pred = (model.predict(X) &gt; 0.5).astype(int)\n\n# Calculate performance metrics\nconf_matrix = confusion_matrix(y, y_pred)\naccuracy = accuracy_score(y, y_pred)\nprecision = precision_score(y, y_pred)\nrecall = recall_score(y, y_pred)\nf1 = f1_score(y, y_pred)\n\n# Display results\nprint(\"Predictions:\")\nfor inp, pred in zip(X, y_pred):\n    print(f\"Input: {inp}, Prediction: {pred}\")\n\nprint(\"\\nPerformance Metrics:\")\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall (Sensitivity): {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(conf_matrix)\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step\nPredictions:\nInput: [0 0], Prediction: [1]\nInput: [0 1], Prediction: [1]\nInput: [1 0], Prediction: [1]\nInput: [1 1], Prediction: [1]\n\nPerformance Metrics:\nAccuracy: 0.50\nPrecision: 0.50\nRecall (Sensitivity): 1.00\nF1 Score: 0.67\n\nConfusion Matrix:\n[[0 2]\n [0 2]]\n\n\n\nTask: Simulate the OR and AND gates using the same approach and analyse the skill of the model using the performance metrices.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Benchmarking Deep Learning with Libraries</span>"
    ]
  },
  {
    "objectID": "benchmarkDL.html#transition-example",
    "href": "benchmarkDL.html#transition-example",
    "title": "2  Benchmarking Deep Learning with Libraries",
    "section": "2.4 Transition Example",
    "text": "2.4 Transition Example\n\n2.4.1 Simulating XOR Gate with Keras\nRevisiting the XOR gate example, here’s how it can be implemented using Keras:\n\nModel definition\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Dense(4, input_dim=2, activation='sigmoid')) #first hidden layer with 4 neurons\nmodel.add(Dense(16, activation='sigmoid')) # second hidden layer with 16 neurons\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nC:\\Users\\SIJUKSWAMY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                   │ (None, 4)              │            12 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 16)             │            80 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 1)              │            17 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 109 (436.00 B)\n\n\n\n Trainable params: 109 (436.00 B)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nModel compilation:\n\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n\nLoading data:\n\n\nimport numpy as np\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny=np.array([[0], [1], [1], [0]])\n\n\nModel training:\n\n\nmodel.fit(X, y, epochs=100, batch_size=2,verbose=0)\n\n&lt;keras.src.callbacks.history.History at 0x2c4df93b5c0&gt;\n\n\n\nModel evaluation:\n\n\nloss, accuracy = model.evaluate(X, y)\nprint(f\"Accuracy: {accuracy}\")\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 166ms/step - accuracy: 0.5000 - loss: 0.7061\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 182ms/step - accuracy: 0.5000 - loss: 0.7061\nAccuracy: 0.5\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy using Keras, the manual implementation steps (e.g., weight initialization, forward/backward propagation) are abstracted, focusing instead on defining and optimizing the architecture. In this approach, the key words are: compile– fit–evaluate–predict.\n\n\n\n\n2.4.2 Performance evaluation of the keras XOR gate model\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n# Predict the outputs\ny_pred = (model.predict(X) &gt; 0.5).astype(int)\n\n# Calculate performance metrics\nconf_matrix = confusion_matrix(y, y_pred)\naccuracy = accuracy_score(y, y_pred)\nprecision = precision_score(y, y_pred)\nrecall = recall_score(y, y_pred)\nf1 = f1_score(y, y_pred)\n\n# Display results\nprint(\"Predictions:\")\nfor inp, pred in zip(X, y_pred):\n    print(f\"Input: {inp}, Prediction: {pred}\")\n\nprint(\"\\nPerformance Metrics:\")\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall (Sensitivity): {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\n\nprint(\"\\nConfusion Matrix:\")\nprint(conf_matrix)\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 224ms/step\nPredictions:\nInput: [0 0], Prediction: [1]\nInput: [0 1], Prediction: [1]\nInput: [1 0], Prediction: [1]\nInput: [1 1], Prediction: [1]\n\nPerformance Metrics:\nAccuracy: 0.50\nPrecision: 0.50\nRecall (Sensitivity): 1.00\nF1 Score: 0.67\n\nConfusion Matrix:\n[[0 2]\n [0 2]]\n\n\n\n\n2.4.3 Task works\n\nTask 1: Simulate the OR and AND gates using the same approach and analyse the skill of the model using the performance metrices.\n\n\nTask 2: Implement the regression task to predict the price of a pizza based on its radius using an MLP model in Keras\n\nSolution:\n\nLoading libraries:\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import load_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,r2_score\n\nCreating synthetic data\n\n# Generate synthetic data: radius (X) and price (y)\nnp.random.seed(42)\nradii = np.random.uniform(5, 20, 25)  # Random radii between 5 and 20 cm\nprices = radii * 10 + np.random.normal(0, 5, 25)  # Price proportional to radius with some noise\n\nX = radii.reshape(-1, 1)  # Feature: radius\ny = prices.reshape(-1, 1)  # Target: price\n\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24003)\n\nCreating a model\n\n# Build the MLP model\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=1, activation='relu')) # Hidden layer with 16 neurons\nmodel.add(Dense(8, activation='relu'))# Hidden layer with 8 neurons\nmodel.add(Dense(1)) # Output layer for regression\n\nC:\\Users\\SIJUKSWAMY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nCompiling the model\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\nTrain the model\n\n# Train the model and capture training history\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, verbose=0)\n\nPlotting Model performance while training\n\n# Plot training and validation loss\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss During Training')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nEvaluate and save the model\n\n# Evaluate the model\ntrain_predictions = model.predict(X_train)\ntest_predictions = model.predict(X_test)\n\n# Compute R-squared values\ntrain_r2 = r2_score(y_train, train_predictions)\ntest_r2 = r2_score(y_test, test_predictions)\ny_pred = model.predict(X_test)\nprint(f\"Training R-squared: {train_r2:.2f}\")\nprint(f\"Testing R-squared: {test_r2:.2f}\")\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error on Test Data: {mse:.2f}\")\n\nprint(f\"Training R-squared: {train_r2:.2f}\")\nprint(f\"Testing R-squared: {test_r2:.2f}\")\n# Save the model\nmodel.save(\"pizza_price_model.h5\")\nprint(\"Model saved as 'pizza_price_model.h5'.\")\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 51ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 38ms/step\n\n\nWARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n\n\nTraining R-squared: 0.97\nTesting R-squared: 0.98\nMean Squared Error on Test Data: 51.62\nTraining R-squared: 0.97\nTesting R-squared: 0.98\nModel saved as 'pizza_price_model.h5'.\n\n\nUsing the saved model for prediction\n\n# Load the saved model for future use\nloaded_model = load_model(\"pizza_price_model.h5\")\nprint(\"\\nLoaded the saved model and testing it...\")\n\ntest_radius = np.array([[12]])  # Example: predict price for a 12 cm pizza\npredicted_price = loaded_model.predict(test_radius)\nprint(f\"Predicted price for a pizza with radius {test_radius[0][0]} cm: ${predicted_price[0][0]:.2f}\")\n\nWARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n\n\n\nLoaded the saved model and testing it...\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 56ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step\nPredicted price for a pizza with radius 12 cm: $118.42",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Benchmarking Deep Learning with Libraries</span>"
    ]
  },
  {
    "objectID": "microprojectsMLP.html",
    "href": "microprojectsMLP.html",
    "title": "3  Micro Projects using ANN",
    "section": "",
    "text": "3.1 Exploring the Iris Dataset with an MLP\nThe Iris dataset is a classic dataset in machine learning, widely used for classification problems. It consists of 150 samples from three species of Iris flowers: Iris-setosa, Iris-versicolor, and Iris-virginica. For each sample, four features are provided:\nThe goal is to classify the species of a given flower based on its features. The dataset is simple and small, making it an excellent starting point for exploring machine learning algorithms such as the Multilayer Perceptron (MLP).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Micro Projects using ANN</span>"
    ]
  },
  {
    "objectID": "microprojectsMLP.html#exploring-the-iris-dataset-with-an-mlp",
    "href": "microprojectsMLP.html#exploring-the-iris-dataset-with-an-mlp",
    "title": "3  Micro Projects using ANN",
    "section": "",
    "text": "Sepal Length (cm)\n\nSepal Width (cm)\n\nPetal Length (cm)\n\nPetal Width (cm)\n\n\n\n3.1.1 Why Use MLP for Iris Dataset?\nAn MLP is a feedforward artificial neural network that can learn complex patterns in data. Using an MLP for the Iris dataset provides an opportunity to:\n\nUnderstand how neural networks can classify multi-class data.\nExplore the strengths and limitations of MLPs in small datasets.\nExperiment with performance measures, such as accuracy, precision, recall, and confusion matrices, in a classification problem.\n\n\n\n3.1.2 Problem Setting\nIn this task: 1. Input Features: The four numerical features (sepal length, sepal width, petal length, petal width).\n\nOutput Labels: The flower species, encoded as integers:\n\nIris-setosa → 0\n\nIris-versicolor → 1\n\nIris-virginica → 2\n\nObjective: Train an MLP to classify the flower species based on input features.\n\n\n\n3.1.3 Outline of Exploration\nThis exploration will include:\n\nData Preprocessing: Normalizing feature values and encoding labels.\nModel Building: Designing an MLP using Keras for multi-class classification.\nModel Training and Evaluation: Measuring the model’s performance using metrics like accuracy and confusion matrices.\nVisualization: Interpreting results through performance plots.\n\nThis foundational task demonstrates the power of neural networks in multi-class classification while highlighting the practical workflow of MLP implementation.\n\nSolution:\n\nBelow is the step-by-step code to solve the Iris dataset classification problem using an MLP model:\n\nStep 1: Load and Explore the Iris Dataset\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data  # Input features: Sepal Length, Sepal Width, Petal Length, Petal Width\ny = iris.target  # Target labels: 0 (Setosa), 1 (Versicolor), 2 (Virginica)\n\n# Dataset overview\nprint(f\"Features:\\n{iris.feature_names}\")\nprint(f\"Classes:\\n{iris.target_names}\")\nprint(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n\nFeatures:\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\nClasses:\n['setosa' 'versicolor' 'virginica']\nShape of X: (150, 4), Shape of y: (150,)\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Convert dataset to a DataFrame\niris_df = pd.DataFrame(data=np.column_stack((X, y)), columns=iris.feature_names + [\"target\"])\niris_df['target'] = iris_df['target'].astype(int)\niris_df['species'] = iris_df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n# Correlation matrix\ncorrelation_matrix = iris_df.iloc[:, :-2].corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", cbar=True)\nplt.title('Correlation Matrix of Features')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pair plot to visualize distributions over classes\nsns.pairplot(iris_df, hue='species', diag_kind='kde', palette='Set2')\nplt.suptitle('Pair Plot of Features Colored by Target Class', y=1.02)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Group by target class and calculate statistics\ngrouped_stats = iris_df.groupby('species').agg(['mean', 'std', 'min', 'max'])\nprint(\"Summary Statistics of Features by Class:\")\nprint(grouped_stats)\n\nSummary Statistics of Features by Class:\n           sepal length (cm)                     sepal width (cm)            \\\n                        mean       std  min  max             mean       std   \nspecies                                                                       \nsetosa                 5.006  0.352490  4.3  5.8            3.428  0.379064   \nversicolor             5.936  0.516171  4.9  7.0            2.770  0.313798   \nvirginica              6.588  0.635880  4.9  7.9            2.974  0.322497   \n\n                     petal length (cm)                     petal width (cm)  \\\n            min  max              mean       std  min  max             mean   \nspecies                                                                       \nsetosa      2.3  4.4             1.462  0.173664  1.0  1.9            0.246   \nversicolor  2.0  3.4             4.260  0.469911  3.0  5.1            1.326   \nvirginica   2.2  3.8             5.552  0.551895  4.5  6.9            2.026   \n\n                               target               \n                 std  min  max   mean  std min max  \nspecies                                             \nsetosa      0.105386  0.1  0.6    0.0  0.0   0   0  \nversicolor  0.197753  1.0  1.8    1.0  0.0   1   1  \nvirginica   0.274650  1.4  2.5    2.0  0.0   2   2  \n\n\n\n# Boxplots for each feature by target class\nplt.figure(figsize=(12, 8))\nfor i, feature in enumerate(iris.feature_names):\n    plt.subplot(2, 2, i+1)\n    sns.boxplot(x='species', y=feature, data=iris_df, palette='Set2')\n    plt.title(f'Distribution of {feature} by Target Class')\n    plt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\nC:\\Users\\SIJUKSWAMY\\AppData\\Local\\Temp\\ipykernel_11892\\451241665.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(x='species', y=feature, data=iris_df, palette='Set2')\nC:\\Users\\SIJUKSWAMY\\AppData\\Local\\Temp\\ipykernel_11892\\451241665.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(x='species', y=feature, data=iris_df, palette='Set2')\nC:\\Users\\SIJUKSWAMY\\AppData\\Local\\Temp\\ipykernel_11892\\451241665.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(x='species', y=feature, data=iris_df, palette='Set2')\nC:\\Users\\SIJUKSWAMY\\AppData\\Local\\Temp\\ipykernel_11892\\451241665.py:5: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(x='species', y=feature, data=iris_df, palette='Set2')\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import f_oneway\n\n# ANOVA for features with target class\nanova_results = {}\nfor feature in iris.feature_names:\n    groups = [iris_df[iris_df['species'] == species][feature] for species in iris.target_names]\n    f_stat, p_val = f_oneway(*groups)\n    anova_results[feature] = {'F-statistic': f_stat, 'p-value': p_val}\n\n# Display results\nanova_results_df = pd.DataFrame(anova_results).T\nprint(\"ANOVA Results:\")\nprint(anova_results_df)\n\nANOVA Results:\n                   F-statistic       p-value\nsepal length (cm)   119.264502  1.669669e-31\nsepal width (cm)     49.160040  4.492017e-17\npetal length (cm)  1180.161182  2.856777e-91\npetal width (cm)    960.007147  4.169446e-85\n\n\nANOVA tests whether the mean of each feature significantly differs among the target classes. Features with low p-values (&lt; 0.05) are strongly correlated with the target class.\n\nStep 2: Preprocess the Data\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the input features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# One-hot encode the target labels\nencoder = OneHotEncoder(sparse_output=False)  \ny_train = encoder.fit_transform(y_train.reshape(-1, 1))\ny_test = encoder.transform(y_test.reshape(-1, 1))\n\n\nStep 3: Build the MLP Model\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Build the MLP model\nmodel = Sequential([\n    Dense(8, activation='relu', input_shape=(X_train.shape[1],)),  # Hidden layer 1\n    Dense(8, activation='relu'),  # Hidden layer 2\n    Dense(y_train.shape[1], activation='softmax')  # Output layer for multi-class classification\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Summary of the model\nmodel.summary()\n\nC:\\Users\\SIJUKSWAMY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nModel: \"sequential_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_3 (Dense)                 │ (None, 8)              │            40 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 8)              │            72 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (Dense)                 │ (None, 3)              │            27 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 139 (556.00 B)\n\n\n\n Trainable params: 139 (556.00 B)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nStep 4: Train the Model\n\n\n# Train the model\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n\nTest Accuracy: 0.97\n\n\n\nStep 5: Evaluate the Model\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Make predictions\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_test_classes = np.argmax(y_test, axis=1)\n\n# Classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test_classes, y_pred_classes, target_names=iris.target_names))\n\n# Confusion matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test_classes, y_pred_classes))\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 109ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 124ms/step\nClassification Report:\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        10\n  versicolor       1.00      0.89      0.94         9\n   virginica       0.92      1.00      0.96        11\n\n    accuracy                           0.97        30\n   macro avg       0.97      0.96      0.97        30\nweighted avg       0.97      0.97      0.97        30\n\nConfusion Matrix:\n[[10  0  0]\n [ 0  8  1]\n [ 0  0 11]]\n\n\n\nStep 6: Visualize Training Progress\n\n\nimport matplotlib.pyplot as plt\n\n# Plot training and validation accuracy\nplt.figure(figsize=(12, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Accuracy During Training')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Loss plot\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss During Training')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nStep 7: Save and Reload the Model for Future Predictions\n\n\n# Save the model\nmodel.save('iris_mlp_model.h5')\n\n# Reload the model\nloaded_model = tf.keras.models.load_model('iris_mlp_model.h5')\n\n# Predict on new data\nnew_sample = np.array([[5.1, 3.5, 1.4, 0.2]])  # Example input\nnew_sample = scaler.transform(new_sample)\npredicted_class = np.argmax(loaded_model.predict(new_sample), axis=1)\nprint(f\"Predicted Class: {iris.target_names[predicted_class[0]]}\")\n\nWARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \nWARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n\n\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 50ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step\nPredicted Class: setosa",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Micro Projects using ANN</span>"
    ]
  },
  {
    "objectID": "intro.html#the-need-for-an-appropriate-learning-algorithm-in-multi-layer-networks",
    "href": "intro.html#the-need-for-an-appropriate-learning-algorithm-in-multi-layer-networks",
    "title": "1  Basics of Artificial Neural Network",
    "section": "1.7 The Need for an Appropriate Learning Algorithm in Multi-Layer Networks",
    "text": "1.7 The Need for an Appropriate Learning Algorithm in Multi-Layer Networks\nIn machine learning, training a multi-layer neural network (also known as a deep neural network) presents a significant challenge compared to simpler models like the Perceptron. This complexity arises from the structure of the network and the way information flows through multiple layers. The primary challenge is that the weight updates at each layer must be carefully adjusted to ensure the network learns effectively.\n\n1.7.1 Challenge of Multi-Layer Networks\nA multi-layer neural network consists of several layers:\n\nInput Layer: Receives the raw data.\nHidden Layers: Transform the input into meaningful representations.\nOutput Layer: Produces the final predictions or classifications.\n\nAs we move deeper into the network, each layer influences the output indirectly through many intermediate transformations. This creates a situation where updating the weights at deeper layers is not straightforward because the error at the output layer is spread across multiple layers.\n\n\n1.7.2 Why a Learning Algorithm is Needed?\nFor multi-layer networks, we require a learning algorithm to adjust the weights effectively. In simpler models like the Perceptron, weights are updated based on the error between the predicted and actual outputs. However, for multi-layer networks, the error must be propagated back through the layers to determine how much each weight in each layer contributed to the final error.\nWithout a structured learning approach, the network would fail to update its weights in a coherent way, preventing it from learning the underlying patterns in the data. This is where Backpropagation comes into play.\n\n\n1.7.3 Backpropagation: The Core Learning Algorithm\nBackpropagation is the most widely used algorithm for training multi-layer networks. It is based on the principle of gradient descent, a method used to minimize the loss function by adjusting the weights. Here’s how backpropagation works:\n\nForward Pass: The input data is passed through the network, and activations for each layer are computed.\nLoss Calculation: The error between the predicted and actual output is computed using a loss function.\nBackward Pass: The error is propagated back through the network, and gradients of the loss function with respect to each weight are computed.\nWeight Update: The weights are updated using the computed gradients.\n\n\n1.7.3.1 Key Features of Backpropagation:\n\nError Propagation: Backpropagation calculates how much each weight in the network contributed to the error at the output, and this error is propagated backward through the layers.\nGradient Calculation: By using the chain rule of calculus, backpropagation calculates the gradients of the loss function with respect to each weight in the network.\nWeight Updates: These gradients are then used in gradient descent to adjust the weights in such a way that the loss function is minimized.\n\n\n\n\n\n\n\nGradient Descent: Not Enough by Itself\n\n\n\nWhile gradient descent is a general optimization algorithm, it cannot directly address the challenge of updating weights in deep neural networks. In shallow networks (with fewer layers), gradient descent can work directly on the loss function. However, in deep networks, the error needs to be distributed back through the multiple layers. Backpropagation is the algorithm that enables this efficient error propagation, making the use of gradient descent feasible for deep networks.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of Artificial Neural Network</span>"
    ]
  },
  {
    "objectID": "intro.html#mathematical-background-of-backpropagation",
    "href": "intro.html#mathematical-background-of-backpropagation",
    "title": "1  Basics of Artificial Neural Network",
    "section": "1.8 Mathematical Background of Backpropagation",
    "text": "1.8 Mathematical Background of Backpropagation\nBackpropagation is the core learning algorithm for training multi-layer neural networks. It involves adjusting the weights of the network by computing the gradients of a loss function with respect to each weight. This process allows the network to learn how to minimize the loss and improve its predictions.\n\n1.8.1 Loss Function\nThe loss function quantifies the difference between the network’s predictions and the true labels. For a simple regression task, the loss function is often the mean squared error (MSE):\n\\[\nL = \\frac{1}{2} (y - \\hat{y})^2\n\\]\nwhere: - \\(y\\) is the true label, - \\(\\hat{y}\\) is the predicted output of the network.\nThe factor \\(\\frac{1}{2}\\) is included to simplify the calculation of the derivative later.\n\n\n1.8.2 Forward Pass\nDuring the forward pass, the input \\(x\\) is passed through each layer of the network. Each layer computes a weighted sum of its inputs followed by a non-linear activation function \\(\\sigma\\). The output of a neuron in layer \\(l\\) is calculated as:\n\\[\na_i^l = \\sigma \\left( \\sum_j w_{ij}^{l-1} a_j^{l-1} + b_i^l \\right)\n\\]\nwhere: - \\(w_{ij}^{l-1}\\) is the weight connecting neuron \\(j\\) in layer \\(l-1\\) to neuron \\(i\\) in layer \\(l\\), - \\(a_j^{l-1}\\) is the activation of neuron \\(j\\) in the previous layer, - \\(b_i^l\\) is the bias of neuron \\(i\\) in layer \\(l\\), - \\(\\sigma\\) is the activation function (e.g., sigmoid, ReLU, etc.).\nThe output of the network is:\n\\[\n\\hat{y} = a^L\n\\]\nwhere \\(L\\) is the last layer.\n\n\n1.8.3 Backward Pass: Computing Gradients\nThe main objective of backpropagation is to compute the gradients of the loss function with respect to the weights and biases of the network. This is achieved through the chain rule of calculus, which allows the error to be propagated backward through the network.\n\n1.8.3.1 Gradient of the Loss with respect to Output Layer\nThe first step is to calculate the gradient of the loss function with respect to the predicted output \\(\\hat{y}\\):\n\\[\n\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y\n\\]\nFor the output layer \\(L\\), the gradient of the loss with respect to the activation \\(a_i^L\\) is:\n\\[\n\\delta_i^L = \\frac{\\partial L}{\\partial a_i^L} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a_i^L}\n\\]\nFor a typical activation function \\(\\sigma\\), we have:\n\\[\n\\frac{\\partial \\hat{y}}{\\partial a_i^L} = \\sigma'(a_i^L)\n\\]\nwhere \\(\\sigma'(a_i^L)\\) is the derivative of the activation function at \\(a_i^L\\).\nThus, the error term for the output layer is:\n\\[\n\\delta_i^L = (\\hat{y} - y) \\cdot \\sigma'(a_i^L)\n\\]\n\n\n1.8.3.2 Gradient for Hidden Layers\nFor each hidden layer \\(l\\), the error term for neuron \\(i\\) in layer \\(l\\) is:\n\\[\n\\delta_i^l = \\left( \\sum_j w_{ij}^l \\delta_j^{l+1} \\right) \\cdot \\sigma'(a_i^l)\n\\]\nwhere:\n\n\\(\\delta_j^{l+1}\\) is the error term for neuron \\(j\\) in the next layer \\(l+1\\),\n\\(w_{ij}^l\\) is the weight connecting neuron \\(j\\) in layer \\(l+1\\) to neuron \\(i\\) in layer \\(l\\),\n\\(\\sigma'(a_i^l)\\) is the derivative of the activation function for neuron \\(i\\) in layer \\(l\\).\n\n\n\n1.8.3.3 Gradients of the Weights and Biases\nAfter calculating the error terms, we compute the gradients of the weights and biases. The gradient of the loss with respect to the weight \\(w_{ij}^l\\) is:\n\\[\n\\frac{\\partial L}{\\partial w_{ij}^l} = a_j^{l-1} \\cdot \\delta_i^l\n\\]\nAnd the gradient of the loss with respect to the bias \\(b_i^l\\) is:\n\\[\n\\frac{\\partial L}{\\partial b_i^l} = \\delta_i^l\n\\]\n\n\n\n1.8.4 Weight Update Rule (Gradient Descent)\nOnce the gradients are computed, we update the weights and biases using gradient descent. The update rule for the weights is:\n\\[\nw_{ij}^l \\leftarrow w_{ij}^l - \\eta \\cdot \\frac{\\partial L}{\\partial w_{ij}^l}\n\\]\nAnd for the biases:\n\\[\nb_i^l \\leftarrow b_i^l - \\eta \\cdot \\frac{\\partial L}{\\partial b_i^l}\n\\]\nwhere \\(\\eta\\) is the learning rate, controlling the step size for weight updates.\n\n\n\n\n\n\nSummary of Backpropagation\n\n\n\nThe backpropagation algorithm involves the following steps:\n\nForward pass: Compute the activations for each layer.\nCompute loss: Calculate the loss function based on the predicted and true values.\nBackward pass:\n\nCompute error terms for the output layer.\nPropagate error backward through the network using the chain rule.\n\nCompute gradients: Compute the gradients of the loss with respect to weights and biases.\nUpdate weights: Use gradient descent to adjust the weights and biases.\n\n\n\n\n\n1.8.5 Why MLP Can Solve XOR?\nThe MLP can learn the XOR function by introducing hidden layers that allow it to combine the inputs in non-linear ways. A network with just one hidden layer is sufficient to approximate the XOR gate. The hidden layer transforms the inputs into a higher-dimensional space where the problem becomes linearly separable, allowing the output layer to make correct predictions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of Artificial Neural Network</span>"
    ]
  }
]